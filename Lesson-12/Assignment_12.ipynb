{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习上课内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.回答以下理论问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 请写一下TF-IDF的计算公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TF(Term frequency):在一个文档中，一个词出现的次数: $tf_{t,d}=count(t,d)$,也可以对结果取对数, $tf_{t,d}=log_{10}(count(t,d)+1)$\n",
    "其中，t=某个词在该文档中出现对频次，d=文档的总词数\n",
    ">\n",
    ">IDF(Inverse document frenquency):总文档数除以包含某个词的文档数，再取对数: $idf_t=log_{10}(\\frac{N}{df_t})$\n",
    "其中，N=总文档数，$df_t$=包含t的文档数\n",
    ">\n",
    ">TF-IDF = $TF * IDF$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. LDA算法的基本假设是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">LDA的2个基本假设：\n",
    ">\n",
    ">1. Per-document topic distributions: The topics of each document obey a probability distribution(每个文本主题服从某个概率分布)\n",
    ">2. Per-topic word distributions: The words of each topic obey a probability distribution (每个主题下的词服从一个概率分布)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 在TextRank算法中构建图的权重是如何得到的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">一般是通过计算2个词的词向量之间的余弦相似度来得到的: $W_{ij}=similarity\\;cosine(w_i, w_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 什么是命名实体识别？ 有什么应用场景？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">命名实体识别指识别文本中具有特定意义的实体，如人名、机构名、地名等专有名词和有意义的时间等；\n",
    ">\n",
    ">命名实体识别是关系抽取、事件抽取、知识图谱、信息检索、语义网络、问答系统、句法分析、机器翻译等任务的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.NLP主要有哪几类任务 ？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">NLP任务:\n",
    ">\n",
    ">* 分类任务(Classification tasks)\n",
    ">  *  情感分析\n",
    ">  *  文本分类\n",
    ">  *  实体识别\n",
    ">* 生成任务(Generation tasks)\n",
    ">  *  机器翻译\n",
    ">  *  对话系统(QA)\n",
    ">  *  文本生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 手动实现TextRank算法 (在新闻数据中随机提取100条新闻训练词向量和做做法测试）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 提示：\n",
    " 1. 确定窗口，建立图链接。   \n",
    " 2. 通过词向量相似度确定图上边的权重\n",
    " 3. 根据公式实现算法迭代(d=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re, os, sys, logging\n",
    "import jieba\n",
    "import random\n",
    "from optparse import OptionParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取新闻语料\n",
    "news = pd.read_csv('sqlResult_1558435.csv', encoding='gb18030')['content'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取中文停顿词函数\n",
    "def get_stopwords():\n",
    "    stopwords = [line.strip() for line in open('chinese_stopwords.txt',encoding='UTF-8').readlines()]\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = get_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文分词函数，用正则去除多余的符号\n",
    "def cut_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub('\\\\\\\\n|[\\n\\u3000\\r]', '', text)\n",
    "    relu = '！？｡。，＂＃＄％＆＇（）＊＋－／：；＜＝＞＠［＼］＾＿｀\\\n",
    "    ｛｜｝～｟｠｢｣､、〃《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–\\\n",
    "    —‘\\'‛“”„‟…‧﹏' + '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~·ʔ•'\n",
    "    text = re.sub('[{}]'.format(relu), '', text)\n",
    "    #text = re.sub('\\\\\\\\n|[\\n\\u3000\\r]', '', text)\n",
    "    seg_list = jieba.cut(text)            \n",
    "    sentence_segment=[] \n",
    "    for word in seg_list:\n",
    "        if word not in stopwords:\n",
    "            sentence_segment.append(word.strip())\n",
    "    #sentence_segment.append(word.strip())        \n",
    "    # 把已去掉停用词的sentence_segment，用' '.join()拼接起来\n",
    "    seg_res = ' '.join(sentence_segment)\n",
    "    return seg_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_content = news[:1000].apply(cut_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'夏普 智能手机 市场 排得 号 没落 并于 2013 年 退出 中国 市场 月份 官方 回归 中国 预示 很快 夏普 新机 中国 登场 第一款 夏普 手机 登陆 中国 手机 近日 一款 型号 FS8016 夏普 神秘 新机 悄然 GeekBench 跑 分库 相关 信息 这款 机子 旗舰 定位 搭载 高通 骁龙 660 处理器 配备 4GB 内存 骁龙 660 高通 最受 瞩目 芯片 采用 14 纳米 工艺 八个 Kryo260 核心 设计 集成 Adreno512GPU X12LTE 调制解调器 市面上 一款 机子 采用 骁龙 660 处理器 上市 销售 OPPOR11 骁龙 660 旗舰 芯片 核新能 去年 骁龙 820 强 单核 改进 放在 手机 高端 机 OPPO 高通 签署 排他性 协议 独占 两三个 月 时间 夏普 测试 新机 独占 时期 一过 夏普 发布 骁龙 660 新品 曝光 渲染 图 夏普 新机 核心 竞争 优势 屏 2013 年 推出 全球 首款 屏 手机 EDGEST302SH 夏普 手机 推出 多达 28 款 屏 手机 月份 媒体 沟通 会上 惠普 罗忠生 我敢 打赌 12 月 在座 手机 换掉 屏 时代 到来 怀揣 手机 传统 手机'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_content[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank关键字提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank():\n",
    "    \n",
    "    def __init__(self, content, wv):\n",
    "        self.content = content\n",
    "        self.wv = wv\n",
    "        self.wordmap = defaultdict(set)\n",
    "        \n",
    "    # 添加关系\n",
    "    def add_to_wordmap(self, words):\n",
    "        for i in range(len(words) - 1):\n",
    "            for j in range(len(words[i:]) - 1):\n",
    "                # 因为要用词向量计算权重，因此如果某个词没有词向量，则不建立联系\n",
    "                if words[i] != words[j] and words[i] in self.wv and words[j] in self.wv:\n",
    "                    self.wordmap[words[i]].add(words[j])\n",
    "                    self.wordmap[words[j]].add(words[i])\n",
    "    \n",
    "    # 根据窗口大小构建每个节点的相邻节点,返回边的集合 \n",
    "    def createNodes(self, window=3):\n",
    "        word_list = self.content.split()\n",
    "        if len(word_list) < window:\n",
    "            self.add_to_wordmap(word_list)\n",
    "        else:\n",
    "            for i in range(len(word_list) - window + 1):\n",
    "                self.add_to_wordmap(word_list[i:i+window])\n",
    "    \n",
    "    def cosine(self, word1, word2):\n",
    "        wv1 = self.wv[word1]\n",
    "        wv2 = self.wv[word2]\n",
    "        return np.dot(wv1.T, wv2)[0][0] / (np.linalg.norm(wv1) * np.linalg.norm(wv2))\n",
    "    \n",
    "    # 根据边的相连关系，构建矩阵 \n",
    "    def createMatrix(self):\n",
    "        self.words = list(self.wordmap.keys())\n",
    "        self.matrix = np.zeros([len(self.words), len(self.words)])\n",
    "        # 记录词的index\n",
    "        self.word_index = {}\n",
    "        \n",
    "        for i, word in enumerate(self.words):\n",
    "            self.word_index[word] = i\n",
    "        \n",
    "        for word_i in self.words:\n",
    "            for word_j in self.wordmap[word_i]:\n",
    "                weight = self.cosine(word_i, word_j) / sum([self.cosine(word_j, word_k) for word_k in self.wordmap[word_j]])\n",
    "                self.matrix[self.word_index[word_i], self.word_index[word_j]] = weight\n",
    "    \n",
    "    #根据textrank公式计算权重 \n",
    "    def calculate(self, num_iters = 1000, d=0.85):\n",
    "        self.WS = np.ones([len(self.words), 1]) / len(self.words)\n",
    "        for i in range(num_iters):\n",
    "            self.WS = (1 - d) + d * np.dot(self.matrix, self.WS)\n",
    "    \n",
    "    # 输出词和相应的权重\n",
    "    def get_result(self):\n",
    "        ws_dict = {}\n",
    "        for word in self.words:\n",
    "            ws_dict[word] = self.WS[self.word_index[word]][0]\n",
    "        return ws_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 尝试使用TfidfVectorizer获取向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认pattern是只匹配长度大于等于2的词\n",
    "new_content_vec = TfidfVectorizer(max_features=10000, token_pattern='(?u)\\\\b\\\\w+\\\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_content_vec.fit_transform(new_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取TfidfVectorizer向量里面的词和向量对应的dict\n",
    "def get_wordvec_dict(X):\n",
    "    wordvec_dict = {}\n",
    "    for word in new_content_vec.vocabulary_.keys():\n",
    "        wordvec_dict[word] = X[:, new_content_vec.vocabulary_[word]].toarray()\n",
    "    return wordvec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eg：{某个词：向量}\n",
    "wordvec_dict = get_wordvec_dict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword(content, wordvec_dict, topn=10):\n",
    "    tr = TextRank(content, wordvec_dict)\n",
    "    tr.createNodes(window=3)\n",
    "    tr.createMatrix()\n",
    "    tr.calculate(num_iters=1000, d=0.85)\n",
    "    ws = tr.get_result()\n",
    "    for item in sorted(ws.items(), key=lambda x: x[1], reverse=True)[:topn]:\n",
    "        print('word:{0:{2}<10}, weight:{1:.4f}'.format(item[0], item[1], chr(12288)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:发展　　　　　　　　, weight:5.2888\n",
      "word:经济　　　　　　　　, weight:3.5340\n",
      "word:新　　　　　　　　　, weight:3.2161\n",
      "word:年　　　　　　　　　, weight:3.0234\n",
      "word:顺义　　　　　　　　, weight:2.7919\n",
      "word:企业　　　　　　　　, weight:2.4069\n",
      "word:厂房　　　　　　　　, weight:2.2665\n",
      "word:产业　　　　　　　　, weight:2.0502\n",
      "word:北京　　　　　　　　, weight:1.8391\n",
      "word:中　　　　　　　　　, weight:1.8035\n"
     ]
    }
   ],
   "source": [
    "get_keyword(random.choice(new_content),wordvec_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### 尝试使用gensim里的FastText获取关键字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把已分词的文件导出为txt文件，按正常来说，训练词向量，不需要去停留词的\n",
    "pd.DataFrame(new_content).to_csv('news.txt',header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-09 12:52:46,611 - INFO - running ipykernel_launcher.py\n",
      "2020-04-09 12:52:46,612 - INFO - resetting layer weights\n",
      "2020-04-09 12:52:54,520 - INFO - collecting all words and their counts\n",
      "2020-04-09 12:52:54,522 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-04-09 12:52:54,587 - INFO - collected 46083 word types from a corpus of 282096 raw words and 1000 sentences\n",
      "2020-04-09 12:52:54,588 - INFO - Loading a fresh vocabulary\n",
      "2020-04-09 12:52:54,669 - INFO - effective_min_count=1 retains 46083 unique words (100% of original 46083, drops 0)\n",
      "2020-04-09 12:52:54,669 - INFO - effective_min_count=1 leaves 282096 word corpus (100% of original 282096, drops 0)\n",
      "2020-04-09 12:52:54,764 - INFO - deleting the raw counts dictionary of 46083 items\n",
      "2020-04-09 12:52:54,765 - INFO - sample=0.001 downsamples 7 most-common words\n",
      "2020-04-09 12:52:54,766 - INFO - downsampling leaves estimated 279082 word corpus (98.9% of prior 282096)\n",
      "2020-04-09 12:52:54,985 - INFO - estimated required memory for 46083 words, 207968 buckets and 200 dimensions: 267423260 bytes\n",
      "2020-04-09 12:52:54,991 - INFO - resetting layer weights\n",
      "2020-04-09 12:53:05,872 - INFO - training model with 6 workers on 46083 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-04-09 12:53:06,525 - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-09 12:53:06,600 - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-09 12:53:06,601 - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-09 12:53:06,634 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-09 12:53:06,638 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-09 12:53:06,640 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-09 12:53:06,640 - INFO - EPOCH - 1 : training on 282096 raw words (279076 effective words) took 0.8s, 364867 effective words/s\n",
      "2020-04-09 12:53:07,209 - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-09 12:53:07,253 - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-09 12:53:07,299 - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-09 12:53:07,301 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-09 12:53:07,316 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-09 12:53:07,317 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-09 12:53:07,318 - INFO - EPOCH - 2 : training on 282096 raw words (279133 effective words) took 0.7s, 413321 effective words/s\n",
      "2020-04-09 12:53:07,866 - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-09 12:53:07,940 - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-09 12:53:07,946 - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-09 12:53:07,976 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-09 12:53:07,978 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-09 12:53:07,983 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-09 12:53:07,984 - INFO - EPOCH - 3 : training on 282096 raw words (279172 effective words) took 0.7s, 420166 effective words/s\n",
      "2020-04-09 12:53:08,529 - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-09 12:53:08,596 - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-09 12:53:08,618 - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-09 12:53:08,641 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-09 12:53:08,642 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-09 12:53:08,642 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-09 12:53:08,643 - INFO - EPOCH - 4 : training on 282096 raw words (279137 effective words) took 0.7s, 425471 effective words/s\n",
      "2020-04-09 12:53:09,203 - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-09 12:53:09,245 - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-09 12:53:09,293 - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-09 12:53:09,295 - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-09 12:53:09,307 - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-09 12:53:09,310 - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-09 12:53:09,311 - INFO - EPOCH - 5 : training on 282096 raw words (279184 effective words) took 0.7s, 418544 effective words/s\n",
      "2020-04-09 12:53:09,311 - INFO - training on a 1410480 raw words (1395702 effective words) took 3.4s, 405901 effective words/s\n",
      "2020-04-09 12:53:09,926 - INFO - saving FastText object under news.FastText_min_count1.model, separately None\n",
      "2020-04-09 12:53:09,927 - INFO - storing np array 'vectors_ngrams' to news.FastText_min_count1.model.wv.vectors_ngrams.npy\n",
      "2020-04-09 12:53:11,641 - INFO - not storing attribute vectors_norm\n",
      "2020-04-09 12:53:11,641 - INFO - not storing attribute vectors_vocab_norm\n",
      "2020-04-09 12:53:11,642 - INFO - not storing attribute vectors_ngrams_norm\n",
      "2020-04-09 12:53:11,642 - INFO - not storing attribute buckets_word\n",
      "2020-04-09 12:53:11,643 - INFO - storing np array 'vectors_ngrams_lockf' to news.FastText_min_count1.model.trainables.vectors_ngrams_lockf.npy\n",
      "2020-04-09 12:53:16,957 - INFO - saved news.FastText_min_count1.model\n",
      "2020-04-09 12:53:16,958 - INFO - storing 46083x200 projection weights into news.FastText_min_count1.vectors\n",
      "2020-04-09 12:53:22,269 - INFO - FastText model training finished\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import logging\n",
    "import multiprocessing\n",
    "from optparse import OptionParser\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "# FastText训练词向量模型函数\n",
    "def FastText_train(infile,outmodel,outvector,size,window,min_count,sg):\n",
    "    '''train the word vectors by FastText'''\n",
    "    \n",
    "    # 训练模型\n",
    "    model = FastText(LineSentence(infile),size=size,window=window,min_count=min_count,sg=sg,workers=multiprocessing.cpu_count())  \n",
    "\n",
    "    # 保存模型\n",
    "    model.save(outmodel)\n",
    "    model.wv.save_word2vec_format(outvector,binary=False)\n",
    "\n",
    "\n",
    "# FastText程序主函数\n",
    "def FastText_train_main():\n",
    "    infile = 'news.txt'\n",
    "    outmodel = 'news.FastText_min_count1.model'\n",
    "    outvec = 'news.FastText_min_count1.vectors'\n",
    "    vec_size = 200\n",
    "    window = 5\n",
    "    min_count = 1\n",
    "    sg = 1\n",
    "\n",
    "    try:       \n",
    "        FastText_train(infile, outmodel, outvec, vec_size, window, min_count,sg)\n",
    "        logger.info('FastText model training finished')\n",
    "    except Exception as err:\n",
    "        logger.info(err)    \n",
    "    \n",
    "       \n",
    "if __name__ == '__main__':\n",
    "    # 返回当前运行的py文件名称\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    # logging.basicConfig函数中，可以指定日志的输出格式format，这个参数可以输出很多有用的信息\n",
    "    # %(asctime)s: 打印日志的时间     %(levelname)s: 打印日志级别名称      %(message)s: 打印日志信息\n",
    "    logging.basicConfig(level = logging.INFO, format = '%(asctime)s - %(levelname)s - %(message)s')\n",
    "    # logging.getLogger(name)方法进行初始化，name可以不填。通常logger的名字我们对应模块名\n",
    "    logger = logging.getLogger(program)  # logging.getLogger(logger_name)\n",
    "    # logger.info打印程序运行是的正常的信息，用于替代print输出\n",
    "    logger.info('running ' + program)    \n",
    "    # 运行程序主函数\n",
    "    FastText_train_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-09 12:53:40,185 - INFO - loading FastText object from news.FastText_min_count1.model\n",
      "2020-04-09 12:53:40,875 - INFO - loading wv recursively from news.FastText_min_count1.model.wv.* with mmap=None\n",
      "2020-04-09 12:53:40,875 - INFO - loading vectors_ngrams from news.FastText_min_count1.model.wv.vectors_ngrams.npy with mmap=None\n",
      "2020-04-09 12:53:41,426 - INFO - setting ignored attribute vectors_norm to None\n",
      "2020-04-09 12:53:41,427 - INFO - setting ignored attribute vectors_vocab_norm to None\n",
      "2020-04-09 12:53:41,427 - INFO - setting ignored attribute vectors_ngrams_norm to None\n",
      "2020-04-09 12:53:41,427 - INFO - setting ignored attribute buckets_word to None\n",
      "2020-04-09 12:53:41,428 - INFO - loading vocabulary recursively from news.FastText_min_count1.model.vocabulary.* with mmap=None\n",
      "2020-04-09 12:53:41,429 - INFO - loading trainables recursively from news.FastText_min_count1.model.trainables.* with mmap=None\n",
      "2020-04-09 12:53:41,429 - INFO - loading vectors_ngrams_lockf from news.FastText_min_count1.model.trainables.vectors_ngrams_lockf.npy with mmap=None\n",
      "2020-04-09 12:53:41,979 - INFO - loaded news.FastText_min_count1.model\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "FASTTEXT_MODEL_DIR = 'news.FastText_min_count1.model'\n",
    "model = FastText.load(FASTTEXT_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_dict(model):\n",
    "    word2vec_dict = {}\n",
    "    for word in model.wv.index2word:\n",
    "        if len(word) >= 2:\n",
    "            word2vec_dict[word] = model.wv[word].reshape(-1,1)\n",
    "    return word2vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_dict = get_word2vec_dict(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 基于FastText和TfidfVectorizer训练的词向量获取关键字的运行时间对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:手机　　　　　　　　, weight:3.9151\n",
      "word:夏普　　　　　　　　, weight:3.9136\n",
      "word:骁龙　　　　　　　　, weight:2.7582\n",
      "word:新机　　　　　　　　, weight:2.5062\n",
      "word:中国　　　　　　　　, weight:2.3136\n",
      "word:高通　　　　　　　　, weight:2.1919\n",
      "word:660　　　　　　　, weight:1.7684\n",
      "word:核心　　　　　　　　, weight:1.6195\n",
      "word:独占　　　　　　　　, weight:1.6180\n",
      "word:机子　　　　　　　　, weight:1.5316\n",
      "Wall time: 48.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 基于FastText训练的词向量获取关键字\n",
    "get_keyword(new_content[6],word2vec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:手机　　　　　　　　, weight:3.7568\n",
      "word:骁龙　　　　　　　　, weight:3.4484\n",
      "word:新机　　　　　　　　, weight:3.2995\n",
      "word:660　　　　　　　, weight:3.0032\n",
      "word:夏普　　　　　　　　, weight:2.4547\n",
      "word:中国　　　　　　　　, weight:2.3452\n",
      "word:机子　　　　　　　　, weight:2.1771\n",
      "word:高通　　　　　　　　, weight:2.0832\n",
      "word:旗舰　　　　　　　　, weight:2.0483\n",
      "word:2013　　　　　　, weight:1.9524\n",
      "Wall time: 16.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 基于TfidfVectorizer训练的词向量获取关键字\n",
    "get_keyword(new_content[6],wordvec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------   分割线   ------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择1 使用词向量和k-means的方法寻找关键词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：\n",
    "1. 使用3.1训练好的词向量\n",
    "2. 可使用sklearn等机器学习库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入KMeans\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class keyword_kmeans():\n",
    "    \n",
    "    def __init__(self, train_wv, wv_dict, n_clusters=3, max_iter=500):\n",
    "        self.centers = KMeans(n_clusters=n_clusters, max_iter=max_iter).fit(train_wv).cluster_centers_\n",
    "        self.wv_dict = wv_dict\n",
    "        \n",
    "    def distance(self, wordvec):\n",
    "        dist_l = []\n",
    "        for i, center in enumerate(self.centers):\n",
    "            dist = np.dot(center, wordvec)[0] / (np.linalg.norm(center) * np.linalg.norm(wordvec))\n",
    "            dist_l.append((i, dist))\n",
    "        return max(dist_l, key=lambda x: x[1])\n",
    "    \n",
    "    def calculate(self, content, topn=10):\n",
    "        words = list(set(content.split()))\n",
    "        self.keywords = sorted([(word, self.distance(self.wv_dict[word])) for word in words if word in self.wv_dict and len(word) >=2], \n",
    "                               key=lambda x:x[1][1], reverse=True)[:topn]\n",
    "    \n",
    "    def get_result(self, print_out=True):\n",
    "        if print_out:\n",
    "            for word, (center, dist) in self.keywords:\n",
    "                print('word:{0:{3}<4}, center:{1:<2}, distance:{2:.4f}'.format(word, center, dist, chr(12288)))\n",
    "        else:\n",
    "            return self.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmeans_keywords(content, train_wv, wv_dict, n_clusters):\n",
    "    keywords = keyword_kmeans(train_wv, wv_dict, n_clusters)\n",
    "    keywords.calculate(random.choice(new_content))\n",
    "    return keywords.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:超过　　, center:1 , distance:0.4027\n",
      "word:中国　　, center:0 , distance:0.3846\n",
      "word:公司　　, center:1 , distance:0.3617\n",
      "word:10　　, center:0 , distance:0.3613\n",
      "word:23　　, center:2 , distance:0.3464\n",
      "word:未来　　, center:2 , distance:0.3316\n",
      "word:北京　　, center:2 , distance:0.3195\n",
      "word:显示　　, center:2 , distance:0.2947\n",
      "word:合作　　, center:2 , distance:0.2846\n",
      "word:国内　　, center:0 , distance:0.2719\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "get_kmeans_keywords(new_content[6], X.T, wordvec_dict, n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------   分割线   ------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选做 2.  提取新闻人物里的对话。(使用以上提取小数据即可）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：    \n",
    "1.寻找预料里具有表示说的意思。    \n",
    "2.使用语法分析提取句子结构。    \n",
    "3.检测谓语是否有表示说的意思。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from pyltp import SentenceSplitter, NamedEntityRecognizer, Postagger, Parser, Segmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speak_words = ['表示', '说', '回复', '指出', '认为', '坦言', '告诉', '强调', '称', '直言', '普遍认为', '介绍', '透露', '重申', '呼吁', '说道', '感叹',\n",
    "               '地说', '写道',\n",
    "               '中称', '证实', '还称', '猜测', '暗示', '感慨', '热议', '敦促', '指责', '声称', '主张', '反对', '批评', '表态', '中说', '承认', '却说',\n",
    "               '感触',\n",
    "               '提到', '所说', '引述', '质疑', '抨击', '回应', '分析说', '发现', '表示', '表态', '推测', '推断', '判决', '判定', '要求']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 从词向量模型里获取与“说”相关的词，推荐使用腾讯的词向量，但占用内存比较大\n",
    "def get_related_words(initial_words, model):\n",
    "    \"\"\"\n",
    "    @initial_words are initial words we already know\n",
    "    @model is the word2vec model\n",
    "    \"\"\"\n",
    "    unseen = initial_words\n",
    "    seen = defaultdict(int)\n",
    "    max_size = 500\n",
    "    while unseen and len(seen) < max_size:\n",
    "        if len(seen) % 100 == 0:\n",
    "            print('seen length : {}'.format(len(seen)))\n",
    "        node = unseen.pop(0)\n",
    "        new_expanding = [w for w, s in model.wv.most_similar(node, topn=5) if s>0]\n",
    "        unseen += new_expanding\n",
    "        seen[node] += 1\n",
    "    return seen\n",
    "\n",
    "# 获取相关字的函数\n",
    "def get_words_said(model_path):\n",
    "    # 加载词向量模型，可以选用项目1训练好的文件,也可以选用其他预训练的模型\n",
    "    model = KeyedVectors.load_word2vec_format(model_path, binary=False)\n",
    "    model.init_sims(replace=True)\n",
    "    related_words = get_related_words(['说', '表示', '认为'], model)\n",
    "    related_words = sorted(related_words.items(), key=lambda x: x[1], reverse=True)\n",
    "    said = [i[0] for i in related_words if i[1] >= 1]\n",
    "    return said\n",
    "\n",
    "\n",
    "def save_said(path):\n",
    "    said = get_words_said(path)\n",
    "    string = '|'.join(said)\n",
    "    try:\n",
    "        with open(\"similar_said.txt\", 'w') as f:\n",
    "            f.write(string)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_said(filename):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            string = f.readlines()\n",
    "            string = string[0].split('|')\n",
    "            return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen length : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Jerry\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen length : 100\n",
      "seen length : 100\n",
      "seen length : 100\n",
      "seen length : 100\n",
      "seen length : 100\n",
      "seen length : 100\n",
      "seen length : 200\n",
      "seen length : 300\n",
      "seen length : 400\n",
      "seen length : 400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 腾讯词向量文件路径\n",
    "path = 'H:/1-开课吧/Tencent_AILab_ChineseEmbedding.txt'\n",
    "# 获得与“说”相关的字，并保存\n",
    "result = save_said(path)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词\n",
    "def cut(string):\n",
    "    return ' '.join(jieba.cut(string))\n",
    "\n",
    "# 处理文本\n",
    "def token(string):\n",
    "    string = re.findall('[\\d|\\w|\\u3002 |\\uff1f |\\uff01 |\\uff0c |\\u3001 |\\uff1b |\\uff1a |\\u201c |\\u201d |\\u2018 |\\u2019 |\\uff08 |\\uff09 |\\u300a |\\u300b |\\u3008 |\\u3009 |\\u3010 |\\u3011 |\\u300e |\\u300f |\\u300c |\\u300d |\\ufe43 |\\ufe44 |\\u3014 |\\u3015 |\\u2026 |\\u2014 |\\uff5e |\\ufe4f |\\uffe5]+', string)\n",
    "    return ' '.join(string)\n",
    "\n",
    "# 处理文本和分词\n",
    "def deal(string):\n",
    "    string = token(string)\n",
    "    return cut(string)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用SentenceSplitter分句\n",
    "def split_sentences(string):\n",
    "    sents = SentenceSplitter.split(string)\n",
    "    sentences = [s for s in sents if len(s) != 0]\n",
    "    return sentences\n",
    "\n",
    "# 处理句子文本和分词\n",
    "def split_words(sentences):\n",
    "    sents = [deal(s) for s in sentences]\n",
    "    return sents\n",
    "\n",
    "#  得到词性标注\n",
    "def get_word_pos(ltp_model_path, sents):\n",
    "    model_path = ltp_model_path\n",
    "    pos_model_path = os.path.join(model_path, 'pos.model')\n",
    "    from pyltp import Postagger\n",
    "    postagger = Postagger()\n",
    "    postagger.load(pos_model_path)\n",
    "    postags = [postagger.postag(words.split()) for words in sents]\n",
    "    postags = [list(w) for w in postags]\n",
    "\n",
    "    postagger.release()\n",
    "    return postags\n",
    "\n",
    "# 句法分析\n",
    "def dependency_parsing(ltp_model_path, sents, postags, said):\n",
    "    # ltp模型目录的路径\n",
    "    LTP_DATA_DIR = ltp_model_path \n",
    "    # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "    par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  \n",
    "    # 初始化实例\n",
    "    parser = Parser()  \n",
    "    # 加载依存句法分析模型\n",
    "    parser.load(par_model_path)      \n",
    "        \n",
    "    # 命名实体识别模型路径，模型名称为`ner.model`\n",
    "    ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model') \n",
    "    # 初始化实例\n",
    "    recognizer = NamedEntityRecognizer() \n",
    "    # 加载命名实体识别模型\n",
    "    recognizer.load(ner_model_path)  \n",
    "\n",
    "    # 定义最后获取的内容空列表\n",
    "    contents = []\n",
    "    for index in range(len(sents)):\n",
    "        wo = sents[index].split()\n",
    "        po = postags[index]\n",
    "        # 命名实体识别\n",
    "        netags = recognizer.recognize(wo, po)  \n",
    "        netags = list(netags)\n",
    "        if ('S-Ns' not in netags) and ('S-Ni' not in netags) and ('S-Nh' not in netags):\n",
    "            continue\n",
    "        # 句法分析\n",
    "        arcs = parser.parse(wo, po)  \n",
    "        arcs = [(arc.head, arc.relation) for arc in arcs]\n",
    "\n",
    "        arcs = [(i, arc) for i, arc in enumerate(arcs) if arc[1] == 'SBV']\n",
    "        for arc in arcs:\n",
    "            verb = arc[1][0]\n",
    "            subject = arc[0]\n",
    "            if wo[verb - 1] not in said:\n",
    "                continue\n",
    "\n",
    "            contents.append((wo[subject], wo[verb - 1], ''.join(wo[verb:])))\n",
    "    return contents\n",
    "\n",
    "\n",
    "def get_speech_extraction(string):\n",
    "    # 加载保存与“说”相关的txt文件\n",
    "    said = load_said(\"similar_said.txt\")\n",
    "    speak = ['表示', '说', '回复', '指出', '认为', '坦言', '告诉', '强调', \n",
    "                   '称', '直言', '普遍认为', '介绍', '透露', '重申', '呼吁', '说道',\n",
    "                   '感叹', '地说', '写道', '中称', '证实', '还称', '猜测', '暗示', \n",
    "                   '感慨', '热议', '敦促', '指责', '声称', '主张', '反对', '批评',\n",
    "                   '表态', '中说', '承认', '却说', '感触', '提到', '所说', '引述',\n",
    "                   '质疑', '抨击', '回应', '分析说', '发现', '表示', '表态', '推测', \n",
    "                   '推断', '判决', '判定', '要求', '宣布']\n",
    "    speak_words = list(set(said + speak))\n",
    "    # ltp 模型文件路径\n",
    "    ltp_model_path = '../ltp_model/'\n",
    "    # 分句\n",
    "    sentences = split_sentences(string)\n",
    "    # 处理句子文本和分词\n",
    "    sents = split_words(sentences)\n",
    "    # 得到词性标注\n",
    "    postags = get_word_pos(ltp_model_path, sents)\n",
    "    # 句法分析后获得的内容\n",
    "    contents = dependency_parsing(ltp_model_path, sents, postags, speak_words)\n",
    "    contents_dict = {}\n",
    "    for indx, ones in enumerate(contents):\n",
    "        contents_dict[str(indx)] = [ones[0], ones[1], ones[2][1:]]\n",
    "    \n",
    "    speech_contents = pd.DataFrame(contents_dict,index=['Person','Speak','Contents']).T\n",
    "    \n",
    "    return speech_contents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_contents = \"\"\"\n",
    "习近平指出，当前，墨西哥等拉美和加勒比国家疫情不断发展，我代表中国政府和中国人民，向墨西哥政府和人民表示诚挚慰问和坚定支持。相信在总统先生领导下，墨西哥能够尽快战胜疫情。中方愿继续向墨方提供力所能及的支持，包括协助墨方在中国采购急需防疫物资。双方还可以继续以专家视频会议等方式交流防控和诊疗经验。 \n",
    "习近平强调，中方始终秉持人类命运共同体理念，积极开展抗疫国际合作，坚定支持世卫组织工作。在二十国集团领导人应对新冠肺炎特别峰会上，我就抗疫国际合作和稳定世界经济提出了4点倡议。中方愿同包括墨西哥在内的各成员国加强沟通协调，落实峰会共识。相信通过这次抗疫合作，中墨两国人民友谊将不断深化，双边关系的战略性将不断提高。中方愿同墨方加强在联合国等多边框架内沟通协调，共同推动构建人类命运共同体。\n",
    "洛佩斯表示，墨方当前正处于抗击新冠肺炎疫情的困难时刻，中方为墨方抗疫提供了宝贵支持和帮助，这对墨方非常重要。墨西哥同中国一直保持着兄弟般的友好关系，墨方将继续致力于积极发展对华关系，同中方加强抗疫和公共卫生等各领域交流合作。\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Person</th>\n",
       "      <th>Speak</th>\n",
       "      <th>Contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>习近平</td>\n",
       "      <td>指出</td>\n",
       "      <td>当前，墨西哥等拉美和加勒比国家疫情不断发展，我代表中国政府和中国人民，向墨西哥政府和人民表示...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>习近平</td>\n",
       "      <td>强调</td>\n",
       "      <td>中方始终秉持人类命运共同体理念，积极开展抗疫国际合作，坚定支持世卫组织工作。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>洛佩斯</td>\n",
       "      <td>表示</td>\n",
       "      <td>墨方当前正处于抗击新冠肺炎疫情的困难时刻，中方为墨方抗疫提供了宝贵支持和帮助，这对墨方非常重要。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Person Speak                                           Contents\n",
       "0    习近平    指出  当前，墨西哥等拉美和加勒比国家疫情不断发展，我代表中国政府和中国人民，向墨西哥政府和人民表示...\n",
       "1    习近平    强调             中方始终秉持人类命运共同体理念，积极开展抗疫国际合作，坚定支持世卫组织工作。\n",
       "2    洛佩斯    表示   墨方当前正处于抗击新冠肺炎疫情的困难时刻，中方为墨方抗疫提供了宝贵支持和帮助，这对墨方非常重要。"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_speech_extraction(news_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------   分割线   ------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择3. ： 电影评论分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个作业中你要完成一个电影评论分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.数据获取。（采用爬虫技术爬取相关网页上的电影评论数据，例如猫眼电影评论，豆瓣电影评论）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.把所获得数据分解为训练集，验证集和测试集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.选用相应算法构建模型，并测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 数据集地址：https://pan.baidu.com/s/1c0yn3TlkzHYTdEBz3T5arA#list/path=%2F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户数据：738701\n",
      "评分数目：2125056\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv('H:/3-NLP数据集/1-电影评论/ratings.csv')\n",
    "\n",
    "print('用户数据：%d' % ratings.userId.unique().shape[0])\n",
    "print('评分数目：%d' % ratings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正向（5星）数目：638106\n",
      "负向（1星）数目：190927\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>comment</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>698338</th>\n",
       "      <td>13682</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1485619200</td>\n",
       "      <td>别总在每一部电影要期待惊天动地的爱情。我认为魔幻，色彩，想象，故事都是不错的。那么差评的人...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719834</th>\n",
       "      <td>371166</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1485532800</td>\n",
       "      <td>这么好看的电影评分这么低真的科学么？</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019907</th>\n",
       "      <td>56671</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>1457193600</td>\n",
       "      <td>美帝就是厉害，不得不服。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294431</th>\n",
       "      <td>200233</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1436544000</td>\n",
       "      <td>除了有点老套简单的剧情，其他全部很好。国漫电影良心之作。日漫痕迹基本没有了，特效和画风简直...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33384</th>\n",
       "      <td>31692</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1462118400</td>\n",
       "      <td>终于想起来了 我从加州回来之后去看的 叶骏</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800461</th>\n",
       "      <td>670744</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1474905600</td>\n",
       "      <td>只是一部有点温暖的商业片，分数太高了！不能人云亦云</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117956</th>\n",
       "      <td>35546</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1468080000</td>\n",
       "      <td>不完美，但却恰到好处。这部电影不会火，因为太有意境和情怀。现在大众的艺术欣赏水平更喜欢搞笑...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721578</th>\n",
       "      <td>176004</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>1473868800</td>\n",
       "      <td>僵尸跑起来真带劲</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301967</th>\n",
       "      <td>5193</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1436889600</td>\n",
       "      <td>其实剧本写得一般，节奏没掌握好，有一些笑点很没必要，但大圣的音乐一响起来我就泪目了。因为他...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380384</th>\n",
       "      <td>63701</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1461427200</td>\n",
       "      <td>家里看的，所以没有感受到大家说的十分震撼。但还是随大流给好评。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         userId  movieId  rating   timestamp  \\\n",
       "698338    13682       10       5  1485619200   \n",
       "719834   371166       10       5  1485532800   \n",
       "2019907   56671       27       5  1457193600   \n",
       "294431   200233        5       5  1436544000   \n",
       "33384     31692        0       5  1462118400   \n",
       "1800461  670744       24       1  1474905600   \n",
       "117956    35546        1       5  1468080000   \n",
       "1721578  176004       24       5  1473868800   \n",
       "301967     5193        5       5  1436889600   \n",
       "380384    63701        5       5  1461427200   \n",
       "\n",
       "                                                   comment  like  \n",
       "698338    别总在每一部电影要期待惊天动地的爱情。我认为魔幻，色彩，想象，故事都是不错的。那么差评的人...     7  \n",
       "719834                                  这么好看的电影评分这么低真的科学么？    11  \n",
       "2019907                                       美帝就是厉害，不得不服。     0  \n",
       "294431    除了有点老套简单的剧情，其他全部很好。国漫电影良心之作。日漫痕迹基本没有了，特效和画风简直...     1  \n",
       "33384                                终于想起来了 我从加州回来之后去看的 叶骏     0  \n",
       "1800461                          只是一部有点温暖的商业片，分数太高了！不能人云亦云     0  \n",
       "117956    不完美，但却恰到好处。这部电影不会火，因为太有意境和情怀。现在大众的艺术欣赏水平更喜欢搞笑...     3  \n",
       "1721578                                           僵尸跑起来真带劲     0  \n",
       "301967    其实剧本写得一般，节奏没掌握好，有一些笑点很没必要，但大圣的音乐一响起来我就泪目了。因为他...     0  \n",
       "380384                     家里看的，所以没有感受到大家说的十分震撼。但还是随大流给好评。     0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_with_opinions = ratings[(ratings.rating==1) | (ratings.rating==5)]\n",
    "\n",
    "print('正向（5星）数目：%d' % (ratings_with_opinions[ratings_with_opinions.rating==5].shape[0]))\n",
    "print('负向（1星）数目：%d' % (ratings_with_opinions[ratings_with_opinions.rating==1].shape[0]))\n",
    "\n",
    "ratings_with_opinions.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### 机器学习方法 做短文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split                #划分训练/测试集\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb    \n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import roc_auc_score,precision_score,recall_score,f1_score,roc_curve,auc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取中文停顿词函数\n",
    "def getStopwords():\n",
    "    stopwords = [line.strip() for line in open('chinese_stopwords.txt',encoding='UTF-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "# 获取中文停顿词\n",
    "stopwords = getStopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取影评数据函数\n",
    "def read_data():\n",
    "    # 用pd.read_csv方法读取数据\n",
    "    data = pd.read_csv('H:/3-NLP数据集/1-电影评论/ratings.csv')\n",
    "    data = data.loc[:, ['comment', 'rating']]\n",
    "    df = pd.DataFrame()\n",
    "    data['rating'][(data['rating']<=3)] = 0\n",
    "    data['rating'][(data['rating']> 3)] = 1\n",
    "    data1 = data[(data['rating']==0) & (data['comment'].str.len()>10)][:400000]\n",
    "    data2 = data[(data['rating']==1) & (data['comment'].str.len()>10)][:400000]    \n",
    "    df = pd.concat([data1,data2])\n",
    "    X = df['comment']\n",
    "    y = df['rating']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('H:/3-NLP数据集/1-电影评论/ratings.csv')\n",
    "data = data.loc[:, ['comment', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "data1 = data[(data['rating']==1) & (data['comment'].str.len()>10)][:100000]\n",
    "data2 = data[(data['rating']==2) & (data['comment'].str.len()>10)][:100000]\n",
    "data3 = data[(data['rating']==3) & (data['comment'].str.len()>10)][:100000]\n",
    "data4 = data[(data['rating']==4) & (data['comment'].str.len()>10)][:100000]\n",
    "data5 = data[(data['rating']==5) & (data['comment'].str.len()>10)][:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([data1,data2,data3,data4,data5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Jerry\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "D:\\Users\\Jerry\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "data['rating'][(data['rating']<=3)] = 0\n",
    "data['rating'][(data['rating']> 3)] = 1\n",
    "data1 = data[(data['rating']==0) & (data['comment'].str.len()>10)][:400000]\n",
    "data2 = data[(data['rating']==1) & (data['comment'].str.len()>10)][:400000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([data1,data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(df['rating']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Jerry\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "D:\\Users\\Jerry\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "X, y = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文分词函数，用正则去除多余的符号\n",
    "def cut_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub('\\\\\\\\n|[\\n\\u3000\\r]', ' ', text)\n",
    "    relu = '！？｡。，＂＃＄％＆＇（）＊＋－／：；＜＝＞＠［＼］＾＿｀\\\n",
    "    ｛｜｝～｟｠｢｣､、〃《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–\\\n",
    "    —‘\\'‛“”„‟…‧﹏' + '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~·ʔ•'\n",
    "    text = re.sub('[{}]'.format(relu), ' ', text)\n",
    "    seg_list = jieba.cut(text)            \n",
    "    sentence_segment=[] \n",
    "    for word in seg_list:\n",
    "        if word not in stopwords:\n",
    "            sentence_segment.append(word.strip())\n",
    "    #sentence_segment.append(word)        \n",
    "    # 把已去掉停用词的sentence_segment，用' '.join()拼接起来\n",
    "    seg_res = ' '.join(sentence_segment)\n",
    "    return seg_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义分类评估指标，actual为真实的类别，predict为预测的类别，predict_prod为预测类别的概率\n",
    "def metrics_result(actual, predict, predict_prod):  \n",
    "    precision_scores = precision_score(actual, predict,average='weighted')\n",
    "    recall_scores = recall_score(actual, predict,average='weighted')\n",
    "    f1_scores = f1_score(actual, predict,average='weighted')\n",
    "    #fpr, tpr, threshold = roc_curve(actual,predict_prod)\n",
    "    #auc_scores = auc(fpr, tpr)\n",
    "    print('精度:{0:.3f}'.format(precision_scores))\n",
    "    print('召回:{0:0.3f}'.format(recall_scores))  \n",
    "    print('f1-score:{0:.3f}'.format(f1_scores))\n",
    "    #print('AUC:{0:.3f}'.format(auc_scores))\n",
    "    return precision_scores, recall_scores, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义roc_curve_plot函数，用于绘制roc_curve图\n",
    "def roc_curve_plot(y_test,y_pred):\n",
    "    fpr, tpr, threshold = roc_curve(y_test,y_pred, pos_label=None, sample_weight=None,drop_intermediate=True)\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.plot(fpr, tpr)\n",
    "    ax.plot([-0.005,1.01],[-0.005,1.01], ls='--')\n",
    "    plt.xlim([-0.005,1.01])\n",
    "    plt.ylim([-0.005,1.01])\n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show() \n",
    "    return fpr,tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 运行分词函数（分词时间约12分钟）\n",
    "yingping = X.apply(cut_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存分词结果\n",
    "yingping.to_csv('yingping800000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取分词csv文件\n",
    "yingping_X = pd.read_csv('yingping800000.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>奥创 整容 韩国</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>失望  剧本 敷衍了事  主线 剧情 突破 理解  人物 缺乏 动机  正邪 之间  妇联...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015 年度 失望 作品  面面俱到  实则 画蛇添足  主题深刻  实则 老调重弹  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>打到 尾  真的 无聊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>剧情 第一集 好玩  全靠 密集 笑点 提神  僧多粥少 后果 每部 寡姐 换 队友 谈恋...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0                                          奥创 整容 韩国 \n",
       "1   失望  剧本 敷衍了事  主线 剧情 突破 理解  人物 缺乏 动机  正邪 之间  妇联...\n",
       "2   2015 年度 失望 作品  面面俱到  实则 画蛇添足  主题深刻  实则 老调重弹  ...\n",
       "4                                       打到 尾  真的 无聊 \n",
       "5   剧情 第一集 好玩  全靠 密集 笑点 提神  僧多粥少 后果 每部 寡姐 换 队友 谈恋..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yingping_X.columns=['comment']\n",
    "yingping_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer 是 CountVectorizer + TfidfTransformer的组合，输出的各个文本各个词的TF-IDF值\n",
    "# min_df=5, max_features=10000\n",
    "tfidf_vec = TfidfVectorizer(min_df=5, max_features=50000) \n",
    "tfidf_matrix = tfidf_vec.fit_transform(yingping_X['comment'].astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800000, 50000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<800000x50000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7744232 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50981     0\n",
       "111809    0\n",
       "31475     0\n",
       "232354    0\n",
       "110580    0\n",
       "         ..\n",
       "81408     0\n",
       "147715    0\n",
       "105947    0\n",
       "119236    0\n",
       "110281    0\n",
       "Name: rating, Length: 1000, dtype: int32"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集8:2，由于数据集不均衡，所以使用stratify=y_c，让划分的数据集比例保持原来的数据分布\n",
    "X_train,X_test,y_train,y_test = train_test_split(tfidf_matrix, y, test_size = 0.2, random_state = 1)#,stratify = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM原生接口，使用贝叶斯调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义LightGBM调参函数\n",
    "def LGB_CV(num_leaves, \n",
    "           learning_rate, \n",
    "           feature_fraction,\n",
    "           lambda_l1, \n",
    "           lambda_l2, \n",
    "           max_depth, \n",
    "           bagging_fraction, \n",
    "           bagging_freq):\n",
    "    \n",
    "    # LightGBM希望接下来的三个参数是整数。所以我们把它们设为整数\n",
    "    num_leaves = int(num_leaves)\n",
    "    max_depth = int(max_depth)\n",
    "    bagging_freq = int(bagging_freq)\n",
    "    \n",
    "    assert type(num_leaves) == int\n",
    "    assert type(max_depth) == int\n",
    "    assert type(bagging_freq) == int\n",
    "    # 定义接收的参数\n",
    "    param = {\n",
    "        'num_leaves': num_leaves,\n",
    "        'learning_rate': learning_rate,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'bagging_freq': bagging_freq,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'max_depth': max_depth,\n",
    "        'save_binary': True, \n",
    "        'seed': 1337,\n",
    "        'feature_fraction_seed': 1337,\n",
    "        'bagging_seed': 1337,\n",
    "        'drop_seed': 1337,\n",
    "        'data_random_seed': 1337,\n",
    "        'objective': 'binary',\n",
    "        #'objective': 'multiclass',      \n",
    "        #'num_class': 5,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        #'metric': {'multi_logloss','multi_error'},\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': False }    \n",
    "        \n",
    "    xg_train = lgb.Dataset(X_train, label=y_train)\n",
    "    xg_valid = lgb.Dataset(X_test, label=y_test)   \n",
    "\n",
    "    num_round = 20000    \n",
    "    lgb_cv_result = lgb.cv(param,\n",
    "                       xg_train, \n",
    "                       num_round,\n",
    "                       nfold = 5,\n",
    "                       stratified=False,\n",
    "                       #metrics= {'multi_logloss','multi_error'},\n",
    "                       metrics='auc',\n",
    "                       verbose_eval = 2000,\n",
    "                       early_stopping_rounds=100,\n",
    "                       seed=0\n",
    "                              )\n",
    "    \n",
    "    return max(lgb_cv_result['auc-mean'])\n",
    "    #return -min(lgb_cv_result['multi_logloss-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | baggin... | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "[2000]\tcv_agg's auc: 0.838625 + 0.000404985\n",
      "[4000]\tcv_agg's auc: 0.848602 + 0.000394512\n",
      "[6000]\tcv_agg's auc: 0.852671 + 0.000370002\n",
      "[8000]\tcv_agg's auc: 0.854493 + 0.000379889\n",
      "[10000]\tcv_agg's auc: 0.855358 + 0.000480742\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8556  \u001b[0m | \u001b[0m 0.4096  \u001b[0m | \u001b[0m 2.428   \u001b[0m | \u001b[0m 0.7834  \u001b[0m | \u001b[0m 4.593   \u001b[0m | \u001b[0m 3.21    \u001b[0m | \u001b[0m 0.05425 \u001b[0m | \u001b[0m 6.143   \u001b[0m | \u001b[0m 49.28   \u001b[0m |\n",
      "[2000]\tcv_agg's auc: 0.81912 + 0.000696155\n",
      "[4000]\tcv_agg's auc: 0.834848 + 0.000597933\n",
      "[6000]\tcv_agg's auc: 0.842479 + 0.000487412\n",
      "[8000]\tcv_agg's auc: 0.847153 + 0.000490604\n",
      "[10000]\tcv_agg's auc: 0.850382 + 0.000454935\n",
      "[12000]\tcv_agg's auc: 0.85279 + 0.000449294\n",
      "[14000]\tcv_agg's auc: 0.854645 + 0.000405783\n",
      "[16000]\tcv_agg's auc: 0.856132 + 0.000404968\n",
      "[18000]\tcv_agg's auc: 0.857339 + 0.000439156\n",
      "[20000]\tcv_agg's auc: 0.858327 + 0.000459122\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8583  \u001b[0m | \u001b[95m 0.6607  \u001b[0m | \u001b[95m 7.603   \u001b[0m | \u001b[95m 0.8842  \u001b[0m | \u001b[95m 0.7037  \u001b[0m | \u001b[95m 7.811   \u001b[0m | \u001b[95m 0.01058 \u001b[0m | \u001b[95m 11.7    \u001b[0m | \u001b[95m 44.18   \u001b[0m |\n",
      "=========================================================================================================================\n",
      "Wall time: 3h 40min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#参数范围设定\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (20, 50), \n",
    "    'learning_rate': (0.005, 0.1),   \n",
    "    'feature_fraction': (0.7, 1),\n",
    "    'lambda_l1': (0, 10.0), \n",
    "    'lambda_l2': (0, 10.0), \n",
    "    'max_depth':(3,15),\n",
    "    'bagging_fraction':(0.2,1),\n",
    "    'bagging_freq':(1,10),\n",
    "}\n",
    "\n",
    "#优化器\n",
    "LGB_BO = BayesianOptimization(LGB_CV, bounds_LGB, random_state = 1337)\n",
    "\n",
    "# 初始随机尝试\n",
    "init_points = 1\n",
    "# 优化尝试\n",
    "n_iter = 1     \n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore') # 忽视一些警告，当然可以注释掉\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.8583269637725411,\n",
       " 'params': {'bagging_fraction': 0.6606550771419661,\n",
       "  'bagging_freq': 7.603232043137429,\n",
       "  'feature_fraction': 0.8841714542757871,\n",
       "  'lambda_l1': 0.7036583820464692,\n",
       "  'lambda_l2': 7.81096941818678,\n",
       "  'learning_rate': 0.010576267427300978,\n",
       "  'max_depth': 11.702923443684123,\n",
       "  'num_leaves': 44.18273598394461}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_leaves': int(LGB_BO.max['params']['num_leaves']), \n",
    "    'learning_rate': LGB_BO.max['params']['learning_rate'],   \n",
    "    'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n",
    "    'lambda_l1': LGB_BO.max['params']['lambda_l1'], \n",
    "    'lambda_l2': LGB_BO.max['params']['lambda_l2'], \n",
    "    'max_depth':int(LGB_BO.max['params']['max_depth']),\n",
    "    'bagging_fraction':LGB_BO.max['params']['bagging_fraction'],\n",
    "    'bagging_freq':int(LGB_BO.max['params']['bagging_freq']),\n",
    "    'save_binary': True, \n",
    "    'seed': 1337,\n",
    "    'feature_fraction_seed': 1337,\n",
    "    'bagging_seed': 1337,\n",
    "    'drop_seed': 1337,\n",
    "    'data_random_seed': 1337,\n",
    "    'objective': 'binary',\n",
    "    #'objective': 'multiclass',      \n",
    "    #'num_class': 5,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': 1,\n",
    "    #'metric': {'multi_logloss','multi_error'},\n",
    "    'metric': 'auc',\n",
    "    'is_unbalance': True,\n",
    "    'boost_from_average': False \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttrain's auc: 0.717438\ttest's auc: 0.716337\n",
      "[200]\ttrain's auc: 0.740312\ttest's auc: 0.73834\n",
      "[300]\ttrain's auc: 0.755824\ttest's auc: 0.75328\n",
      "[400]\ttrain's auc: 0.769914\ttest's auc: 0.76726\n",
      "[500]\ttrain's auc: 0.778629\ttest's auc: 0.775251\n",
      "[600]\ttrain's auc: 0.784901\ttest's auc: 0.781165\n",
      "[700]\ttrain's auc: 0.790997\ttest's auc: 0.78722\n",
      "[800]\ttrain's auc: 0.796114\ttest's auc: 0.79187\n",
      "[900]\ttrain's auc: 0.800349\ttest's auc: 0.795712\n",
      "[1000]\ttrain's auc: 0.804326\ttest's auc: 0.799425\n",
      "[1100]\ttrain's auc: 0.807425\ttest's auc: 0.802345\n",
      "[1200]\ttrain's auc: 0.810433\ttest's auc: 0.805113\n",
      "[1300]\ttrain's auc: 0.812873\ttest's auc: 0.807533\n",
      "[1400]\ttrain's auc: 0.815462\ttest's auc: 0.809936\n",
      "[1500]\ttrain's auc: 0.817844\ttest's auc: 0.812094\n",
      "[1600]\ttrain's auc: 0.819995\ttest's auc: 0.81415\n",
      "[1700]\ttrain's auc: 0.821925\ttest's auc: 0.815887\n",
      "[1800]\ttrain's auc: 0.823612\ttest's auc: 0.817395\n",
      "[1900]\ttrain's auc: 0.82529\ttest's auc: 0.818801\n",
      "[2000]\ttrain's auc: 0.826744\ttest's auc: 0.820081\n",
      "[2100]\ttrain's auc: 0.828221\ttest's auc: 0.82141\n",
      "[2200]\ttrain's auc: 0.829649\ttest's auc: 0.822639\n",
      "[2300]\ttrain's auc: 0.83094\ttest's auc: 0.82381\n",
      "[2400]\ttrain's auc: 0.832175\ttest's auc: 0.824906\n",
      "[2500]\ttrain's auc: 0.833271\ttest's auc: 0.82585\n",
      "[2600]\ttrain's auc: 0.834377\ttest's auc: 0.826798\n",
      "[2700]\ttrain's auc: 0.835436\ttest's auc: 0.827665\n",
      "[2800]\ttrain's auc: 0.836349\ttest's auc: 0.828424\n",
      "[2900]\ttrain's auc: 0.837381\ttest's auc: 0.829295\n",
      "[3000]\ttrain's auc: 0.838327\ttest's auc: 0.830082\n",
      "[3100]\ttrain's auc: 0.83928\ttest's auc: 0.830882\n",
      "[3200]\ttrain's auc: 0.84015\ttest's auc: 0.831569\n",
      "[3300]\ttrain's auc: 0.841008\ttest's auc: 0.83234\n",
      "[3400]\ttrain's auc: 0.841814\ttest's auc: 0.833014\n",
      "[3500]\ttrain's auc: 0.842562\ttest's auc: 0.833674\n",
      "[3600]\ttrain's auc: 0.843245\ttest's auc: 0.834192\n",
      "[3700]\ttrain's auc: 0.843961\ttest's auc: 0.834762\n",
      "[3800]\ttrain's auc: 0.844618\ttest's auc: 0.83532\n",
      "[3900]\ttrain's auc: 0.845307\ttest's auc: 0.835903\n",
      "[4000]\ttrain's auc: 0.846011\ttest's auc: 0.83648\n",
      "[4100]\ttrain's auc: 0.846645\ttest's auc: 0.836944\n",
      "[4200]\ttrain's auc: 0.847247\ttest's auc: 0.83746\n",
      "[4300]\ttrain's auc: 0.847832\ttest's auc: 0.837983\n",
      "[4400]\ttrain's auc: 0.84839\ttest's auc: 0.838434\n",
      "[4500]\ttrain's auc: 0.848988\ttest's auc: 0.838885\n",
      "[4600]\ttrain's auc: 0.849542\ttest's auc: 0.839317\n",
      "[4700]\ttrain's auc: 0.850097\ttest's auc: 0.839747\n",
      "[4800]\ttrain's auc: 0.850586\ttest's auc: 0.840182\n",
      "[4900]\ttrain's auc: 0.851104\ttest's auc: 0.840567\n",
      "[5000]\ttrain's auc: 0.851629\ttest's auc: 0.84102\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttrain's auc: 0.851629\ttest's auc: 0.84102\n"
     ]
    }
   ],
   "source": [
    "lgb_dtrain = lgb.Dataset(X_train, y_train)\n",
    "lgb_dtest = lgb.Dataset(X_test, y_test,reference=lgb_dtrain)\n",
    "# 显示训练过程中需要监控的数据\n",
    "#lgb_watchlist=[(lgb_dtrain,'train'),(lgb_dtest,'test')]\n",
    "lgb_watchlist=[lgb_dtest,lgb_dtrain]\n",
    "# 记录训练过程中监控的数据，需要定义一个空的dict\n",
    "lgb_progress = dict()\n",
    "# 用搜索得到的最佳参数训练新模型\n",
    "lgb_model = lgb.train(params, \n",
    "                      lgb_dtrain, \n",
    "                      num_boost_round = 5000,\n",
    "                      early_stopping_rounds = 100,\n",
    "                      valid_sets = [lgb_dtest,lgb_dtrain],\n",
    "                      valid_names=['test', 'train'],\n",
    "                      #eval_metric = 'auc',\n",
    "                      evals_result =lgb_progress ,\n",
    "                      verbose_eval = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用best_iteration来做预测\n",
    "lgb_y_pred = lgb_model.predict(X_test, ntree_limit = lgb_model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于预测值为0-1的浮点数，概率值，需要转成1,2,3,4,5\n",
    "#lgb_y_predictions = np.argmax(lgb_y_pred, axis=1)\n",
    "# 由于预测值为0-1的浮点数，概率值，需要转成0或1\n",
    "lgb_y_predictions = [int(round(value)) for value in lgb_y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制在训练过程中的AUC变化\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(lgb_progress['train']['auc'],label='Train',color='r')\n",
    "plt.plot(lgb_progress['test']['auc'],label='Test',color='g')\n",
    "plt.xlabel('num_boost_round')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('LightGBM AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精度:0.754\n",
      "召回:0.754\n",
      "f1-score:0.754\n"
     ]
    }
   ],
   "source": [
    "lgb_ms = metrics_result(y_test, lgb_y_predictions, lgb_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确度： 0.7538125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"准确度：\",accuracy_score(y_test,lgb_y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 传统机器学习方法做短文本分类，效果很一般，二分类准确率75%，5分类准确率大约50%\n",
    ">\n",
    "> ### 下面尝试一下使用fasttext来做短文本分类，二分类准确率大约81%，5分类准确率大约56%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext,re,jieba\n",
    "from fasttext import train_supervised\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('H:/3-NLP数据集/1-电影评论/ratings.csv')\n",
    "data = data.loc[:, ['rating', 'comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "data['rating'][(data['rating']<=3)] = 0\n",
    "data['rating'][(data['rating']> 3)] = 1\n",
    "data1 = data[(data['rating']==0) & (data['comment'].str.len()>12)][:200000]\n",
    "data2 = data[(data['rating']==1) & (data['comment'].str.len()>12)][:200000]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([data1,data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return re.findall(r'[\\d|\\w]+',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].apply(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文分词函数，用正则去除多余的符号,不去掉停用词\n",
    "def cut_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub('\\\\\\\\n|[\\n\\u3000\\r]', ' ', text)\n",
    "    relu = '！？｡。，＂＃＄％＆＇（）＊＋－／：；＜＝＞＠［＼］＾＿｀\\\n",
    "    ｛｜｝～｟｠｢｣､、〃《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–\\\n",
    "    —‘\\'‛“”„‟…‧﹏' + '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~·ʔ•'\n",
    "    text = re.sub('[{}]'.format(relu), ' ', text)\n",
    "    seg_list = jieba.cut(text)            \n",
    "    sentence_segment=[] \n",
    "    for word in seg_list:\n",
    "#         if word not in stopwords:\n",
    "#             sentence_segment.append(word.strip())\n",
    "        sentence_segment.append(word)        \n",
    "    # 把已去掉停用词的sentence_segment，用' '.join()拼接起来\n",
    "    seg_res = ' '.join(sentence_segment)\n",
    "    return seg_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['comment'] = df['comment'].apply(cut_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(text):\n",
    "    new_text = '__label__' + str(text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'] = df['rating'].apply(add_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('fasttext.txt',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shuffle(df,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124209</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>画面 确实 美         其实 细想 后         椿 是 个 令人感动...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7915</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>漫画 电影 的 一大 特点 在于 以 正义 之名 乱来         哈哈哈   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459473</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>四分 电影         五星 鼓励         第一次 在 多伦多 看到 爽...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137923</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>4         很 难得 出场 英雄 这么 多         竟然 没有 一个...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127002</th>\n",
       "      <td>__label__0</td>\n",
       "      <td>真 没有 尴尬         剧情 发展 很 理所当然         是 动画 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rating                                            comment\n",
       "124209  __label__1      画面 确实 美         其实 细想 后         椿 是 个 令人感动...\n",
       "7915    __label__0      漫画 电影 的 一大 特点 在于 以 正义 之名 乱来         哈哈哈   ...\n",
       "459473  __label__1      四分 电影         五星 鼓励         第一次 在 多伦多 看到 爽...\n",
       "137923  __label__1      4         很 难得 出场 英雄 这么 多         竟然 没有 一个...\n",
       "127002  __label__0      真 没有 尴尬         剧情 发展 很 理所当然         是 动画 ..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:350000].to_csv('train.txt',index=None,header=None)\n",
    "df[350000:].to_csv('test.txt',index=None,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(N, p, r):\n",
    "    print(\"测试数据量：\\t\" + str(N))\n",
    "    print(\"Precision@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"Recall@{}\\t{:.3f}\".format(1, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据量：\t50000\n",
      "Precision@1\t0.813\n",
      "Recall@1\t0.813\n",
      "测试数据量：\t50000\n",
      "Precision@1\t0.816\n",
      "Recall@1\t0.816\n",
      "测试数据量：\t50000\n",
      "Precision@1\t0.799\n",
      "Recall@1\t0.799\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "train_data = \"train.txt\" # 训练集\n",
    "valid_data = \"test.txt\" # 验证集\n",
    "\n",
    "# train_supervised使用与fastText cli相同的参数和默认值\n",
    "model = train_supervised(\n",
    "    input=train_data, epoch=10, lr=0.02, wordNgrams=3, verbose=0, minCount=5, dim=100,\n",
    "    loss=\"softmax\",bucket= 2000000\n",
    ")\n",
    "print_results(*model.test(valid_data))\n",
    "\n",
    "model = train_supervised(\n",
    "    input=train_data, epoch=10, lr=0.02, wordNgrams=3, verbose=0, minCount=5, dim=100,\n",
    "    loss=\"hs\",bucket= 2000000\n",
    ")\n",
    "print_results(*model.test(valid_data))\n",
    "model.save_model(\"cooking.bin\")\n",
    "\n",
    "# 压缩模型\n",
    "model.quantize(input=train_data, qnorm=True, retrain=True, cutoff=100000)\n",
    "print_results(*model.test(valid_data))\n",
    "model.save_model(\"cooking.ftz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 下面尝试一下使用albert来做短文本分类，由于没有GPU,只能下载albert_small_zh_google较少的模型来跑一下\n",
    ">\n",
    "> ### 由于时间问题，只用了10万条评论数据训练了一轮，ACC只有82%左右，在数据量增大和多跑几轮的话，效果提升会很大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,re\n",
    "import numpy as np\n",
    "from bert4keras.backend import keras, set_gelu\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.optimizers import Adam, extend_with_piecewise_linear_lr\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import open\n",
    "from keras.layers import Lambda, Dense\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'H:\\1-开课吧\\03-人工智能与自然语言处理\\第二章 人工智能引论与思维方法\\第12节 自然语言理解的关键问题\\GPT2-Chinese\\albert_small_zh_google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切换gelu版本\n",
    "set_gelu('tanh') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'albert_config_small_google.json'\n",
    "checkpoint_path = 'albert_model.ckpt'\n",
    "dict_path = 'vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立分词器\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "max_len = 128\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def load_data(filename):\n",
    "    D = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for l in f:\n",
    "            text, label = l.strip().split('\\t')\n",
    "            D.append((text, int(label)))\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把中文多余符号去掉\n",
    "def token(string):\n",
    "    string = re.sub('\\\\\\\\n|[\\n\\u3000\\r]', ' ', string)\n",
    "    relu = '！？｡。，＂＃＄％＆＇（）＊＋－／：；＜＝＞＠［＼］＾＿｀\\\n",
    "    ｛｜｝～｟｠｢｣､、〃《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–\\\n",
    "    —‘\\'‛“”„‟…‧﹏' + '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~·ʔ•'\n",
    "    string = re.sub('[{}]'.format(relu), ' ', string)   \n",
    "    return re.findall(r'[\\d|\\w]+',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv('H:/3-NLP数据集/1-电影评论/ratings.csv')\n",
    "data = data.loc[:, ['rating', 'comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用apply对comment列去掉多余符号\n",
    "data['comment'] = data['comment'].apply(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评分小于等于3的，分类为0，否则为1\n",
    "data['rating'][(data['rating']<=3)] = 0\n",
    "data['rating'][(data['rating']> 3)] = 1\n",
    "# 筛选评分分类为0和1，且comment字符串长度大于8的数据，取5万条数据\n",
    "data1 = data[(data['rating']==0) & (data['comment'].str.len()>8)][:50000]\n",
    "data2 = data[(data['rating']==1) & (data['comment'].str.len()>8)][:50000]    \n",
    "# 合拼数据\n",
    "df = pd.concat([data1,data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把数据打乱\n",
    "df = shuffle(df,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把数据导出为txt文本，index和header为空，分隔符为4个空格，即\"\\t\"\n",
    "df[['comment','rating']][:90000].to_csv('albert_train.txt',index=None,header=None,sep='\\t')\n",
    "df[['comment','rating']][90000:95000].to_csv('albert_valid.txt',header=None,index=None,sep='\\t')\n",
    "df[['comment','rating']][95000:].to_csv('albert_test.txt',header=None,index=None,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "train_data = load_data('albert_train.txt')\n",
    "valid_data = load_data('albert_valid.txt')\n",
    "test_data = load_data('albert_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据生成器\n",
    "\n",
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        for is_end, (text, label) in self.sample(random):\n",
    "            token_ids, segment_ids = tokenizer.encode(text, max_length=max_len)\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            batch_labels.append([label])\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                batch_labels = sequence_padding(batch_labels)\n",
    "                yield [batch_token_ids, batch_segment_ids], batch_labels\n",
    "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练模型\n",
    "bert = build_transformer_model(\n",
    "    config_path=config_path,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    model='albert',\n",
    "    return_keras_model=False,\n",
    "\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 128)    2704384     Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Mapping (Dense)       (None, None, 384)    49536       Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-MultiHeadSelfAttent (None, None, 384)    591360      Embedding-Mapping[0][0]          \n",
      "                                                                 Embedding-Mapping[0][0]          \n",
      "                                                                 Embedding-Mapping[0][0]          \n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-MultiHeadSelfAttent (None, None, 384)    0           Embedding-Mapping[0][0]          \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-MultiHeadSelfAttent (None, None, 384)    768         Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-FeedForward (FeedFo (None, None, 384)    1181568     Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-FeedForward-Add (Ad (None, None, 384)    0           Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[0][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[1][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[2][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[3][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[4][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[5][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-FeedForward-Norm (L (None, None, 384)    768         Transformer-FeedForward-Add[0][0]\n",
      "                                                                 Transformer-FeedForward-Add[1][0]\n",
      "                                                                 Transformer-FeedForward-Add[2][0]\n",
      "                                                                 Transformer-FeedForward-Add[3][0]\n",
      "                                                                 Transformer-FeedForward-Add[4][0]\n",
      "                                                                 Transformer-FeedForward-Add[5][0]\n",
      "__________________________________________________________________________________________________\n",
      "CLS-token (Lambda)              (None, 384)          0           Transformer-FeedForward-Norm[5][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 2)            770         CLS-token[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,595,202\n",
      "Trainable params: 4,595,202\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 建立模型，加载权重\n",
    "output = Lambda(lambda x: x[:, 0], name='CLS-token')(bert.model.output)\n",
    "output = Dense(\n",
    "    units=num_classes,\n",
    "    activation='softmax',\n",
    "    kernel_initializer=bert.initializer\n",
    ")(output)\n",
    "\n",
    "model = keras.models.Model(bert.model.input, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 派生为带分段线性学习率的优化器。\n",
    "# 其中name参数可选，但最好填入，以区分不同的派生优化器。\n",
    "AdamLR = extend_with_piecewise_linear_lr(Adam, name='AdamLR')\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    # optimizer=Adam(1e-5),  # 用足够小的学习率\n",
    "    optimizer=AdamLR(learning_rate=1e-4, lr_schedule={\n",
    "        1000: 1,\n",
    "        2000: 0.1\n",
    "    }),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# 转换数据集\n",
    "train_generator = data_generator(train_data, batch_size)\n",
    "valid_generator = data_generator(valid_data, batch_size)\n",
    "test_generator = data_generator(test_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2813/2813 [==============================] - 5383s 2s/step - loss: 0.4622 - accuracy: 0.7690\n",
      "val_acc: 0.81620, best_val_acc: 0.81620, test_acc: 0.81720\n",
      "\n",
      "final test acc: 0.817200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 评估函数\n",
    "def evaluate(data):\n",
    "    total, right = 0., 0.\n",
    "    for x_true, y_true in data:\n",
    "        y_pred = model.predict(x_true).argmax(axis=1)\n",
    "        y_true = y_true[:, 0]\n",
    "        total += len(y_true)\n",
    "        right += (y_true == y_pred).sum()\n",
    "    return right / total\n",
    "\n",
    "\n",
    "# 评估类\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.best_val_acc = 0.\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_acc = evaluate(valid_generator)\n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            model.save_weights('best_model.weights')\n",
    "        test_acc = evaluate(test_generator)\n",
    "        print(\n",
    "            u'val_acc: %.5f, best_val_acc: %.5f, test_acc: %.5f\\n' %\n",
    "            (val_acc, self.best_val_acc, test_acc)\n",
    "        )\n",
    "\n",
    "\n",
    "evaluator = Evaluator()\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator.forfit(),\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=1,\n",
    "    callbacks=[evaluator]\n",
    ")\n",
    "\n",
    "model.load_weights('best_model.weights')\n",
    "print(u'final test acc: %05f\\n' % (evaluate(test_generator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择4：文章自动续写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个作业中你要完成一个文章自动续写的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.数据获取。（根据你的兴趣采用爬虫技术爬去相关网站上的文本数据内容：比如故事网站，小说网站等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.选取模型，并训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.展示一些你模型的输出例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r'H:\\1-开课吧\\03-人工智能与自然语言处理\\第二章 人工智能引论与思维方法\\第12节 自然语言理解的关键问题\\GPT2-Chinese\\chinese_L-12_H-768_A-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from bert4keras.backend import keras, K\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer, load_vocab\n",
    "from bert4keras.optimizers import Adam\n",
    "from bert4keras.snippets import sequence_padding, open\n",
    "from bert4keras.snippets import DataGenerator, AutoRegressiveDecoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 256\n",
    "batch_size = 16\n",
    "steps_per_epoch = 1000\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert配置\n",
    "config_path = 'bert_config.json'\n",
    "checkpoint_path = 'bert_model.ckpt'\n",
    "dict_path = 'vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels = []\n",
    "\n",
    "for txt in glob.glob(r'data\\train.txt'):\n",
    "    txt = open(txt, encoding='utf-8').read()\n",
    "    txt = txt.replace('\\r', '').replace('\\n', '')\n",
    "    txt = txt.replace(u'整理制作，并提供下载', '')\n",
    "    txt = re.sub(u'www.*?com', '', txt)\n",
    "    txt = txt.replace(u'\\u3000', ' ')\n",
    "    sents = []\n",
    "    for t in txt.split('  '):\n",
    "        #print(t)\n",
    "        for s in re.findall(u'.*?。', t):\n",
    "            if len(s) <= maxlen - 2:\n",
    "                sents.append(s)\n",
    "    novels.append(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载并精简词表，建立分词器\n",
    "token_dict, keep_tokens = load_vocab(\n",
    "    dict_path=dict_path,\n",
    "    simplified=True,\n",
    "    startswith=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\n",
    ")\n",
    "tokenizer = Tokenizer(token_dict, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "构建语料中: 100%|████████████████████████████████████████████████████████████| 91726/91726 [00:00<00:00, 176190.25it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "pbar = tqdm(desc=u'构建语料中', total=sum(len(n) for n in novels))\n",
    "\n",
    "for novel in novels:\n",
    "    s = u''\n",
    "    for i in range(len(novel)):\n",
    "        for j in range(len(novel) - i):\n",
    "            if len(s) + len(novel[i + j]) > maxlen - 2:\n",
    "                data.append(s)\n",
    "                s = u''\n",
    "                break\n",
    "            else:\n",
    "                s += novel[i + j]\n",
    "        pbar.update(1)\n",
    "        if i + j >= len(novel):\n",
    "            break\n",
    "    if s:\n",
    "        data.append(s)\n",
    "\n",
    "pbar.close()\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids = [], []\n",
    "        for is_end, text in self.sample(random):\n",
    "            token_ids, segment_ids = tokenizer.encode(text)\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                yield [batch_token_ids, batch_segment_ids], None\n",
    "                batch_token_ids, batch_segment_ids = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 768)    10432512    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Attention-LM-Mask (Lambda)      (1, 1, None, None)   0           Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "                                                                 Attention-LM-Mask[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          \n",
      "                                                                 Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
      "                                                                 Transformer-0-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Attention-LM-Mask[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
      "                                                                 Transformer-1-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Attention-LM-Mask[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
      "                                                                 Transformer-2-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Attention-LM-Mask[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
      "                                                                 Transformer-3-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]\n",
      "                                                                 Transformer-3-FeedForward-Norm[0]\n",
      "                                                                 Transformer-3-FeedForward-Norm[0]\n",
      "                                                                 Attention-LM-Mask[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]\n",
      "                                                                 Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
      "                                                                 Transformer-4-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]\n",
      "                                                                 Transformer-4-FeedForward-Norm[0]\n",
      "                                                                 Transformer-4-FeedForward-Norm[0]\n",
      "                                                                 Attention-LM-Mask[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]\n",
      "                                                                 Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
      "                                                                 Transformer-5-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]\n",
      "                                                                 Transformer-5-FeedForward-Norm[0]\n",
      "                                                                 Transformer-5-FeedForward-Norm[0]\n",
      "                                                                 Attention-LM-Mask[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]\n",
      "                                                                 Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
      "                                                                 Transformer-6-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]\n",
      "                                                                 Transformer-6-FeedForward-Norm[0]\n",
      "                                                                 Transformer-6-FeedForward-Norm[0]\n",
      "                                                                 Attention-LM-Mask[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]\n",
      "                                                                 Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
      "                                                                 Transformer-7-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]\n",
      "                                                                 Transformer-7-FeedForward-Norm[0]\n",
      "                                                                 Transformer-7-FeedForward-Norm[0]\n",
      "                                                                 Attention-LM-Mask[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]\n",
      "                                                                 Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
      "                                                                 Transformer-8-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]\n",
      "                                                                 Transformer-8-FeedForward-Norm[0]\n",
      "                                                                 Transformer-8-FeedForward-Norm[0]\n",
      "                                                                 Attention-LM-Mask[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]\n",
      "                                                                 Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
      "                                                                 Transformer-9-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]\n",
      "                                                                 Transformer-9-FeedForward-Norm[0]\n",
      "                                                                 Transformer-9-FeedForward-Norm[0]\n",
      "                                                                 Attention-LM-Mask[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]\n",
      "                                                                 Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
      "                                                                 Transformer-10-FeedForward-Dropou\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0\n",
      "                                                                 Transformer-10-FeedForward-Norm[0\n",
      "                                                                 Transformer-10-FeedForward-Norm[0\n",
      "                                                                 Attention-LM-Mask[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0\n",
      "                                                                 Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
      "                                                                 Transformer-11-FeedForward-Dropou\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Dense (Dense)               (None, None, 768)    590592      Transformer-11-FeedForward-Norm[0\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Norm (LayerNormalization)   (None, None, 768)    1536        MLM-Dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Proba (EmbeddingDense)      (None, None, 13584)  13584       MLM-Norm[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 96,488,976\n",
      "Trainable params: 96,488,976\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建transformer模型\n",
    "model = build_transformer_model(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    "    application='lm',\n",
    "    keep_tokens=keep_tokens  # 只保留keep_tokens中的字，精简原字表\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉熵作为loss，并mask掉输入部分的预测\n",
    "y_true = model.input[0][:, 1:]     # 目标tokens\n",
    "y_mask = model.get_layer('Embedding-Token').output_mask[:, 1:]  # 目标mask\n",
    "y_mask = K.cast(y_mask, K.floatx())  # 转为浮点型\n",
    "y_pred = model.output[:, :-1]  # 预测tokens，预测与目标错开一位\n",
    "cross_entropy = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "cross_entropy = K.sum(cross_entropy * y_mask) / K.sum(y_mask)\n",
    "\n",
    "model.add_loss(cross_entropy)\n",
    "model.compile(optimizer=Adam(1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryCompletion(AutoRegressiveDecoder):\n",
    "    \"\"\"基于随机采样的故事续写\n",
    "    \"\"\"\n",
    "    @AutoRegressiveDecoder.set_rtype('probas')\n",
    "    def predict(self, inputs, output_ids, step):\n",
    "        token_ids = inputs[0]\n",
    "        token_ids = np.concatenate([token_ids, output_ids], 1)\n",
    "        segment_ids = np.zeros_like(token_ids)\n",
    "        return model.predict([token_ids, segment_ids])[:, -1]\n",
    "\n",
    "    def generate(self, text, n=1, topk=5):\n",
    "        token_ids, _ = tokenizer.encode(text)\n",
    "        results = self.random_sample([token_ids[:-1]], n, topk)  # 基于随机采样\n",
    "        return [text + tokenizer.decode(ids) for ids in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_completion = StoryCompletion(\n",
    "    start_id=None, end_id=tokenizer._token_end_id, maxlen=maxlen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text():\n",
    "    text1 = u'萧炎对这威胁之话还是感到陌生，若非他灵魂力量同样不弱的话，恐怕早就忍不住的出手了。'\n",
    "    text2 = u'萧炎嘴角掀起一抹微笑，或许，这也会是一种其他的开始。'\n",
    "    text3 = u'半年时间，眨眼便过。'\n",
    "    for text in [text1, text2, text3]:\n",
    "        t = story_completion.generate(text)\n",
    "        print(u'输入: %s' % text)\n",
    "        print(u'结果: %s\\n' % ('\\n'.join(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.lowest = 1e10\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 保存最优\n",
    "        if logs['loss'] <= self.lowest:\n",
    "            self.lowest = logs['loss']\n",
    "            model.save_weights('./best_model.weights')\n",
    "        # 展示训练效果\n",
    "        generate_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练模型，由于本地没有GPU资源，所以在kaggle上训练了3轮，把模型下载到本地加载\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    evaluator = Evaluate()\n",
    "    train_generator = data_generator(data, batch_size)\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_generator.forfit(),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        callbacks=[evaluator]\n",
    "    )\n",
    "\n",
    "else:\n",
    "\n",
    "    model.load_weights('./best_model.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练好的模型权重文件\n",
    "model.load_weights(r'best_model.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text():\n",
    "    text1 = u'萧炎对这威胁之话还是感到陌生，若非他灵魂力量同样不弱的话，恐怕早就忍不住的出手了。'\n",
    "    text2 = u'萧炎嘴角掀起一抹微笑，或许，这也会是一种其他的开始。'\n",
    "    text3 = u'半年时间，眨眼便过。'\n",
    "    for text in [text1, text2, text3]:\n",
    "        t = story_completion.generate(text)\n",
    "        print(u'【输入的文章开头】: %s' % text)\n",
    "        print(u'【文章续写的结果】: %s\\n' % ('\\n'.join(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【输入的文章开头】: 萧炎对这威胁之话还是感到陌生，若非他灵魂力量同样不弱的话，恐怕早就忍不住的出手了。\n",
      "【文章续写的结果】: 萧炎对这威胁之话还是感到陌生，若非他灵魂力量同样不弱的话，恐怕早就忍不住的出手了。在萧炎心神一动，其身形便是化为了一道道影子，对着萧炎狠狠的狠狠砸了过去。这家伙，这是我的对手，我们还真是不知道，你是谁的？一旁的萧厉，也是一愣，眼中掠过一抹惊愕，旋即目光转向萧厉，低声道：萧家，这次，我不是想要将你给解救出来。萧炎笑着道。\n",
      "\n",
      "【输入的文章开头】: 萧炎嘴角掀起一抹微笑，或许，这也会是一种其他的开始。\n",
      "【文章续写的结果】: 萧炎嘴角掀起一抹微笑，或许，这也会是一种其他的开始。在萧炎面前，一名斗尊阶别的斗者，正是一脸阴寒，面庞上的狰狞与阴狠，令得他有着一股不可察觉的感觉。你也是斗皇强者。萧炎微微摇了摇头，道。呵呵，这家伙倒是没什么特别的本事啊，他还真有这个性子，不过他也不会在这种事上出什么问题，我还是不敢随意的放过去。萧炎摇了摇头，笑道。\n",
      "\n",
      "【输入的文章开头】: 半年时间，眨眼便过。\n",
      "【文章续写的结果】: 半年时间，眨眼便过。在那些年，他们都是拥有着无比庞大的财富以及财气，而这种财富，则正在逐渐地减弱，当他们在心中，却是忽然发现，自己的身体，竟然已经被他们所遗留下来的所有能量所焚烧，因此，当他们的灵魂，都是在此刻，变得极为的虚弱了起来在那一瞬间，萧炎体内的那股灵魂力量，也是开始缓缓的释放而出，然后在那无数道目光的注视下，迅速的消失。\n",
      "\n",
      "Wall time: 58.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
