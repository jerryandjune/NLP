{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 今天是2020年1月05日，今天世界上又多了一名AI工程师 :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 作业截止时间\n",
    "此次作业截止时间为 2020.01.12日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：每道题是否回答完整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 智能客服、无人驾驶、舆情系统、人脸识别、信用风险评分、智能写作、智能改题、欺诈分析及检测、对话系统等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Github可以用来托管项目代码，也可以收藏并fork别人的优质项目；Jupyter是一个浏览器类型的IDE，其交互能力与实验能力极强，所以我们用它做演示；Pycharm通常用于大型工程，拥有显示变量，断点运行，debug等优质功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    现实世界的变化受着众多因素的影响，包括确定的和随机的。如果从建模的背景、目的和手段看，主要因素是确定的，随机因素可以忽略，或者随机因素的影响可以简单地以平均值的作用出现，那么就能够建立确定性模型。如果随机因素对研究对象的影响必须考虑，就应建立随机模型。\n",
    "    用随即变量和概率分布描述随机因素的影响，建立的随机模型--即概率模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:在自然语言处理中，概率模型极为重要，在中文分词、词性标注、命名实体识别等诸多场景中有着广泛的应用;另外微软的小冰，苹果的Siri，小米的小爱同学都是应用场景的产物。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "1、为什么使用概率？\n",
    "\n",
    "       在人工智能许多应用场景中，很多数据都具有不确定性和随机性。事实上，不管是文字翻译、语音识别还是其他的智能行为，计算机通常都必须处理不确定的变量，这些不确定性通常有3种可能的来源：      \n",
    "    a) 被建模系统内在的随机性。例如在一个纸牌游戏中，我们假设纸牌被真正混洗成了随机顺序；    \n",
    "    b) 不完全观测。即使是确定的系统，当我们不能完全观测到这个系统的全部变量时，这个系统也呈现随机性。例如在一个游戏节目中，参与者被要求在三个门之间选择，其中两个门背后没有东西，另一个门后放置着一个礼盒。对局外人来说，参与者的每个选择的结果是确定的，但对参与者来说，结果却是不确定的。        \n",
    "    c)  不完全建模。当我们使用一些必须舍弃某些观测信息的模型时，舍弃的这些信息会导致模型的预测出现不确定性。\n",
    "    \n",
    "2、基于解析和模式匹配的编程难点是什么?\n",
    "    解析和模式匹配的难点在于，当遇到大规模数据时，处理的数据量太大，要耗费大量的时间和电脑内存。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:语言模型是一个单词序列上的概率分布，对于一个给定长度为m的序列，它可以为整个序列产生一个概率 P(w_1,w_2,…,w_m) 。其实就是想办法找到一个概率分布，它可以表示任意一个句子或序列出现的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:目前在自然语言处理相关应用非常广泛，如语音识别(speech recognition) , 机器翻译(machine translation), 词性标注(part-of-speech tagging), 句法分析(parsing)等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "N-gram语言模型(Language Model)是自然语言处理（NLP）中一个非常重要的概念，当N=1的时候，为一元模型（Unigram model），p(wi)若语言中有20000个词，则需要估计20000个参数；就是把句子拆分成一个一个的分词，然后计算这些分词同时出现的概率的模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:1-gram的缺点就是忽略了词跟词之间的联系，很多时候两个词之间并不是互相独立的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:N-gram语言模型(Language Model)是自然语言处理（NLP）中一个非常重要的概念，当N=2的时候，为二元模型（bigram model），p(wi|wi-1)若语言中有20000个词，则需要估计计20000^2个参数；就是把句子拆成两个两个的词，这里的两个是一种平滑序列，例如：‘我是一名小学生’，拆成‘我是 是一名 一名小学生’，然后计算它们同时出现的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1569578233461&di=4adfa7597fb380e7cc0e67190bbd7605&imgtype=0&src=http%3A%2F%2Fs1.sinaimg.cn%2Flarge%2F006eYYfyzy76cmpG3Yb1f)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "homework = '''\n",
    "homework = 称呼 问候 老师 时间 科目 作业 动作\n",
    "称呼 = 小朋友们， | 同学们， | 小可爱们， \n",
    "问候 = 正常 | 不正常 \n",
    "正常 = 你们好， | 大家好，\n",
    "不正常 = 你们都给我听着， | 你们都给我记住，\n",
    "老师 = 我是 科目 的 姓 老师，\n",
    "科目 = 语文 | 数学 | 英语 | 物理 | 化学 \n",
    "姓 = 梁 | 黄 | 李 | 郭 | 陈 \n",
    "时间 = 今天的 | 明天的 | 一周的 | 未来一个月\n",
    "作业 = 作业， | 功课，\n",
    "动作 = 请 称呼 结束语| 麻烦 称呼 结束语\n",
    "结束语 = 完成哦！ | 一定要做完哦！\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否提出了和课程上区别较大的语法结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "work = \"\"\"\n",
    "work = 介绍1 介绍2 介绍3 \n",
    "介绍1 = 招呼 头衔 \n",
    "招呼 = 先生你好 | 女士你好 | 你好\n",
    "头衔 = 自我介绍 公司 部门\n",
    "自我介绍 = 我这边是 | 这里是 | 我来自 | 我是\n",
    "公司 = 中国银行 | 广发银行 | 建设银行 | 人人贷 | 皮皮贷\n",
    "部门 = 财务部， | 催收部， | 业务部， | 放贷部，\n",
    "介绍2 = 证据 欠款  \n",
    "证据 = 资料显示 | 证据表明 | 我们查到 | 近年来数据显示\n",
    "欠款 = 说明 金额 天数 \n",
    "说明 = 你欠了我们 | 你的欠款是 | 你需要还给我们\n",
    "金额 = 十万， | 一百万， | 两个香蕉， | 两毛钱， |半毛钱， \n",
    "天数 = 已经很久了。 | 都过去一年了。 | 逾期时间一个月。\n",
    "介绍3 = 威逼 | 利诱\n",
    "威逼 = 诘问 手段 暴力\n",
    "诘问 = 你咋回事？ | 你还能不能还钱了？ | 你有钱花没钱还？\n",
    "手段 = 待会给你送一个苹果？ |  等会给你爱人打电话！  |  我知道你在那上班！| 我知道你住哪里！ \n",
    "暴力 =  就问你怕不怕！ | 赶紧给我还钱！ | 就问你敢不敢不还！ \n",
    "利诱 = 时间 奖励 温柔\n",
    "时间 = 现在还钱 | 即刻还款 | 马上结清款项\n",
    "奖励 = 抽奖 其他 \n",
    "抽奖 = 有机会抽奖获得 | 可以抽取奖励 | 可以领取奖励\n",
    "其他 = 住院一个月。 | 终身爱奇艺高级会员哦。 | 一套辟邪剑谱。\n",
    "温柔 = 你看怎么样? | 考虑一下呗~ | 这一波怎么说?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：是否和上一个语法区别比较大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def create_grammar(grammar_str, split = '=', line_split = '\\n'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(line_split):\n",
    "        if not line.strip(): continue\n",
    "        exp, stmt = line.split(split)\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split(\"|\")]\n",
    "    return grammar\n",
    "\n",
    "choice = random.choice\n",
    "\n",
    "\n",
    "def generate(gram, target):\n",
    "    if target not in gram: return target # means target is a terminal expression\n",
    "    \n",
    "    expaned = [generate(gram, t) for t in choice(gram[target])]\n",
    "    return ''.join([e if e != '/n' else '\\n' for e in expaned if e != 'null'])\n",
    "\n",
    "def generate_n(gram,target,n=1):\n",
    "    sentence = []\n",
    "    for i in range(n):\n",
    "        if target not in gram:\n",
    "            return target\n",
    "        expanded = [generate(gram,t) for t in choice(gram[target])]\n",
    "        #print(''.join([e for e in expanded]))\n",
    "        sentence.append(''.join([e if e != '/n' else '\\n' for e in expanded if e != 'null']))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'homework': [['称呼', '问候', '老师', '时间', '科目', '作业', '动作']],\n",
       " '称呼': [['小朋友们，'], ['同学们，'], ['小可爱们，']],\n",
       " '问候': [['正常'], ['不正常']],\n",
       " '正常': [['你们好，'], ['大家好，']],\n",
       " '不正常': [['你们都给我听着，'], ['你们都给我记住，']],\n",
       " '老师': [['我是', '科目', '的', '姓', '老师，']],\n",
       " '科目': [['语文'], ['数学'], ['英语'], ['物理'], ['化学']],\n",
       " '姓': [['梁'], ['黄'], ['李'], ['郭'], ['陈']],\n",
       " '时间': [['今天的'], ['明天的'], ['一周的'], ['未来一个月']],\n",
       " '作业': [['作业，'], ['功课，']],\n",
       " '动作': [['请', '称呼', '结束语'], ['麻烦', '称呼', '结束语']],\n",
       " '结束语': [['完成哦！'], ['一定要做完哦！']]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homework_gram = create_grammar(homework)\n",
    "homework_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['同学们，你们都给我听着，我是物理的陈老师，今天的物理作业，请小可爱们，一定要做完哦！',\n",
       " '小朋友们，你们都给我记住，我是化学的郭老师，明天的语文作业，麻烦同学们，完成哦！',\n",
       " '小朋友们，你们好，我是数学的李老师，未来一个月英语作业，请同学们，一定要做完哦！',\n",
       " '小朋友们，你们都给我记住，我是英语的郭老师，一周的化学作业，麻烦小可爱们，一定要做完哦！',\n",
       " '小可爱们，你们好，我是化学的郭老师，今天的数学作业，请同学们，完成哦！']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(homework_gram,'homework',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work': [['介绍1', '介绍2', '介绍3']],\n",
       " '介绍1': [['招呼', '头衔']],\n",
       " '招呼': [['先生你好'], ['女士你好'], ['你好']],\n",
       " '头衔': [['自我介绍', '公司', '部门']],\n",
       " '自我介绍': [['我这边是'], ['这里是'], ['我来自'], ['我是']],\n",
       " '公司': [['中国银行'], ['广发银行'], ['建设银行'], ['人人贷'], ['皮皮贷']],\n",
       " '部门': [['财务部，'], ['催收部，'], ['业务部，'], ['放贷部，']],\n",
       " '介绍2': [['证据', '欠款']],\n",
       " '证据': [['资料显示'], ['证据表明'], ['我们查到'], ['近年来数据显示']],\n",
       " '欠款': [['说明', '金额', '天数']],\n",
       " '说明': [['你欠了我们'], ['你的欠款是'], ['你需要还给我们']],\n",
       " '金额': [['十万，'], ['一百万，'], ['两个香蕉，'], ['两毛钱，'], ['半毛钱，']],\n",
       " '天数': [['已经很久了。'], ['都过去一年了。'], ['逾期时间一个月。']],\n",
       " '介绍3': [['威逼'], ['利诱']],\n",
       " '威逼': [['诘问', '手段', '暴力']],\n",
       " '诘问': [['你咋回事？'], ['你还能不能还钱了？'], ['你有钱花没钱还？']],\n",
       " '手段': [['待会给你送一个苹果？'], ['等会给你爱人打电话！'], ['我知道你在那上班！'], ['我知道你住哪里！']],\n",
       " '暴力': [['就问你怕不怕！'], ['赶紧给我还钱！'], ['就问你敢不敢不还！']],\n",
       " '利诱': [['时间', '奖励', '温柔']],\n",
       " '时间': [['现在还钱'], ['即刻还款'], ['马上结清款项']],\n",
       " '奖励': [['抽奖', '其他']],\n",
       " '抽奖': [['有机会抽奖获得'], ['可以抽取奖励'], ['可以领取奖励']],\n",
       " '其他': [['住院一个月。'], ['终身爱奇艺高级会员哦。'], ['一套辟邪剑谱。']],\n",
       " '温柔': [['你看怎么样?'], ['考虑一下呗~'], ['这一波怎么说?']]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_gram = create_grammar(work)\n",
    "work_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['先生你好我是皮皮贷放贷部，近年来数据显示你欠了我们两个香蕉，都过去一年了。马上结清款项可以抽取奖励一套辟邪剑谱。这一波怎么说?',\n",
       " '先生你好我这边是建设银行催收部，近年来数据显示你的欠款是两毛钱，都过去一年了。你有钱花没钱还？等会给你爱人打电话！就问你敢不敢不还！',\n",
       " '你好我这边是人人贷催收部，我们查到你需要还给我们半毛钱，已经很久了。现在还钱可以领取奖励终身爱奇艺高级会员哦。你看怎么样?',\n",
       " '你好我是建设银行财务部，我们查到你的欠款是十万，已经很久了。你还能不能还钱了？待会给你送一个苹果？就问你怕不怕！',\n",
       " '女士你好我来自皮皮贷催收部，近年来数据显示你需要还给我们两毛钱，都过去一年了。马上结清款项可以抽取奖励一套辟邪剑谱。这一波怎么说?',\n",
       " '你好这里是中国银行财务部，资料显示你需要还给我们一百万，逾期时间一个月。你咋回事？我知道你在那上班！赶紧给我还钱！',\n",
       " '先生你好我是中国银行放贷部，近年来数据显示你的欠款是两个香蕉，都过去一年了。即刻还款有机会抽奖获得终身爱奇艺高级会员哦。你看怎么样?',\n",
       " '先生你好这里是建设银行催收部，近年来数据显示你的欠款是两个香蕉，逾期时间一个月。你还能不能还钱了？我知道你在那上班！就问你怕不怕！',\n",
       " '女士你好这里是皮皮贷财务部，近年来数据显示你的欠款是半毛钱，已经很久了。你还能不能还钱了？我知道你住哪里！就问你敢不敢不还！',\n",
       " '你好这里是中国银行业务部，近年来数据显示你欠了我们两毛钱，已经很久了。马上结清款项可以领取奖励住院一个月。这一波怎么说?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(work_gram,'work',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**; 运行代码，观察是否能够生成多个句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点** 1. 是否使用了新的数据集； 2. csv(txt)数据是否正确解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'C:\\Users\\shab\\Desktop\\Assignment-01\\movie_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = pd.read_csv(filename, engine='python',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = content['comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'犯我中华者虽远必诛，是有多无脑才信这句话。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    # we will learn the regular expression next course.\n",
    "    return re.findall('\\w+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_clean = [''.join(token(str(a)))for a in comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string): \n",
    "    return jieba.cut(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = [cut(i) for i in comment_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comment_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\shab\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.621 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "TOKEN = []\n",
    "for i in range(len(comment_clean)):  \n",
    "    TOKEN += cut(comment_clean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4490313"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['到', '了', '脑残', '的', '地步', '看', '了', '恶心']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN[2:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 328262),\n",
       " ('了', 102420),\n",
       " ('是', 73106),\n",
       " ('我', 50338),\n",
       " ('都', 36255),\n",
       " ('很', 34712),\n",
       " ('看', 34022),\n",
       " ('电影', 33675),\n",
       " ('也', 32065),\n",
       " ('和', 31290),\n",
       " ('在', 31245),\n",
       " ('不', 28435),\n",
       " ('有', 27939),\n",
       " ('就', 25685),\n",
       " ('人', 23909),\n",
       " ('好', 22858),\n",
       " ('啊', 20803),\n",
       " ('这', 17484),\n",
       " ('还', 17449),\n",
       " ('一个', 17343),\n",
       " ('你', 17282),\n",
       " ('还是', 16425),\n",
       " ('但', 15578),\n",
       " ('故事', 15010),\n",
       " ('没有', 14343),\n",
       " ('就是', 14007),\n",
       " ('喜欢', 13566),\n",
       " ('让', 13304),\n",
       " ('太', 12676),\n",
       " ('又', 11566),\n",
       " ('剧情', 11359),\n",
       " ('没', 10858),\n",
       " ('说', 10764),\n",
       " ('吧', 10747),\n",
       " ('他', 10675),\n",
       " ('不错', 10416),\n",
       " ('得', 10349),\n",
       " ('到', 10341),\n",
       " ('给', 10300),\n",
       " ('这个', 10058),\n",
       " ('上', 10054),\n",
       " ('被', 9939),\n",
       " ('对', 9824),\n",
       " ('最后', 9694),\n",
       " ('一部', 9693),\n",
       " ('片子', 9590),\n",
       " ('什么', 9571),\n",
       " ('能', 9532),\n",
       " ('与', 9168),\n",
       " ('多', 8977)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count = Counter(TOKEN)\n",
    "words_count.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吴京', '意淫', '到', '了', '脑残', '的', '地步', '看', '了', '恶心']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN = [str(t) for t in TOKEN]\n",
    "TOKEN[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = Counter(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4490313"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吴京意淫', '意淫到', '到了', '了脑残', '脑残的', '的地步', '地步看', '看了', '了恶心', '恶心想']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN[:-2]))]\n",
    "TOKEN_2_GRAM[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count_2 = Counter(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762733"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求某个词在语料中出现的概率\n",
    "def prob_1(word): \n",
    "    return words_count[word] / len(TOKEN)\n",
    "\n",
    "# 求某两个相连的词在语料中出现的概率\n",
    "def prob_2(word1, word2):\n",
    "    if word1 + word2 in words_count_2: \n",
    "        return words_count_2[word1+word2] / len(TOKEN_2_GRAM)\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1379365482702645e-05"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2('我们', '在')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probablity(sentence):\n",
    "    #首先把句子分词\n",
    "    words = list(cut(sentence))\n",
    "    #句子概率初始化\n",
    "    sentence_pro = 1\n",
    "    \n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i+1]\n",
    "        #两个词连在一起的概率\n",
    "        probability = prob_2(word, next_)\n",
    "        #一个句子里每两个词连在一起的概率全部相乘，就是这个句子的概率\n",
    "        sentence_pro *= probability\n",
    "    sentence_pro *= prob_1(words[-1])\n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1684389246338515e-21"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probablity('今天是个好日子')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_comments = \"\"\"\n",
    "movie_comments = 明星 动作 形容\n",
    "明星 = 张杰 | 娜姐 | 小白 | 周杰伦\n",
    "动作 = 演的 | 唱的 | 跪的 | 演绎\n",
    "形容 = 好 | 一级棒 | 太棒了 | 恶心的 | 不好说啊\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_comments': [['明星', '动作', '形容']],\n",
       " '明星': [['张杰'], ['娜姐'], ['小白'], ['周杰伦']],\n",
       " '动作': [['演的'], ['唱的'], ['跪的'], ['演绎']],\n",
       " '形容': [['好'], ['一级棒'], ['太棒了'], ['恶心的'], ['不好说啊']]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_comments_grammer = create_grammar(movie_comments)\n",
    "movie_comments_grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 周杰伦演的恶心的 \n",
      " Prob: 1.2565507110569379e-21\n",
      "sentence: 娜姐跪的一级棒 \n",
      " Prob: 3.7920529274069726e-29\n",
      "sentence: 小白唱的不好说啊 \n",
      " Prob: 8.576994248718087e-30\n",
      "sentence: 周杰伦演绎好 \n",
      " Prob: 5.049388265773984e-16\n",
      "sentence: 娜姐演的一级棒 \n",
      " Prob: 1.8960264637034863e-29\n",
      "sentence: 小白唱的太棒了 \n",
      " Prob: 3.195297473019166e-21\n",
      "sentence: 张杰演的太棒了 \n",
      " Prob: 1.1570870476911422e-16\n",
      "sentence: 周杰伦跪的不好说啊 \n",
      " Prob: 1.3833861691480786e-31\n",
      "sentence: 娜姐演绎太棒了 \n",
      " Prob: 4.2855075840412675e-18\n",
      "sentence: 娜姐唱的恶心的 \n",
      " Prob: 1.1715209636921826e-22\n"
     ]
    }
   ],
   "source": [
    "for sen in [generate_n(gram=movie_comments_grammer,target='movie_comments',n=1) for i in range(10)]:\n",
    "    print('sentence: {} \\n Prob: {}'.format(sen[0],get_probablity(sen[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(gram,target,n): \n",
    "    '''\n",
    "    gram: 语法规则定义\n",
    "    target：生成的目标\n",
    "    n: 生成的句子数量\n",
    "    '''\n",
    "    sentence = [generate(gram=gram,target=target) for i in range(n)]\n",
    "    sen_pro = [get_probablity(i) for i in sentence]\n",
    "    dic = {k:v for k,v in zip(sentence,sen_pro)}\n",
    "    best_sentence = sorted(dic.items(),key=lambda x:x[1], reverse=True)\n",
    "    print(best_sentence)\n",
    "    return best_sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('张杰演的好', 3.7441213990714096e-13), ('小白演绎好', 5.049388265773984e-16), ('张杰演的太棒了', 1.1570870476911422e-16), ('小白演的太棒了', 1.1570870476911422e-16), ('娜姐演绎太棒了', 4.2855075840412675e-18), ('张杰演的恶心的', 4.242333443546458e-18), ('周杰伦跪的好', 1.667644579215742e-19), ('周杰伦演绎恶心的', 1.1465766063639074e-19), ('小白演绎恶心的', 1.1465766063639074e-19), ('周杰伦唱的太棒了', 3.195297473019166e-21), ('小白跪的太棒了', 5.1537056016438157e-23), ('周杰伦跪的太棒了', 5.1537056016438157e-23), ('小白演绎不好说啊', 3.931540590236379e-27), ('小白唱的一级棒', 2.3510728149923232e-27), ('张杰跪的一级棒', 3.7920529274069726e-29), ('周杰伦跪的一级棒', 3.7920529274069726e-29), ('娜姐演的一级棒', 1.8960264637034863e-29), ('娜姐唱的不好说啊', 8.576994248718087e-30), ('小白跪的不好说啊', 1.3833861691480786e-31)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('张杰演的好', 3.7441213990714096e-13)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_best(gram = movie_comments_grammer ,target = 'movie_comments',n = 20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否使用 lambda 语法进行排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 数据来自影评，定义的语法规则偏离这些数据的，对句子合理性的判断有所影响，而且数据量不够大，需要增加语料数据量或者其他不同情境的语料数据；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**评阅点**: 是否提出了比较实际的问题，例如OOV问题，例如数据量，例如变成 3-gram问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
