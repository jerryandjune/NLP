{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入线性回归模型\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import operator\n",
    "# 用icecream进行调试可以打印出变量名\n",
    "from icecream import ic  \n",
    "# 解决 matplotlib 中文乱码\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算x的函数\n",
    "def f(x):\n",
    "    return 15.5 * x + np.random.randint(-50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_Reg(x, plot = True):\n",
    "    # 设定随机种子\n",
    "    np.random.seed(42)\n",
    "    # 产生数据X,Y\n",
    "    X = (np.random.random(100)*10).reshape(-1, 1)\n",
    "    Y = np.array([f(i) for i in X])\n",
    "    # 实例化算法模型，并拟合数据\n",
    "    model = linear_model.LinearRegression().fit(X,Y)\n",
    "    # 基于训练好的模型对象实现预测功能\n",
    "    y_pred = model.predict(x)\n",
    "    # 是否展示可视化\n",
    "    if plot:        \n",
    "        # 根据回归系数和截距计算X对应的y值\n",
    "        y = model.coef_ * X + model.intercept_\n",
    "        # 设置画布大小\n",
    "        plt.figure(figsize=(10,6))\n",
    "        # 绘制原始数据X,Y散点图\n",
    "        plt.scatter(X, Y ,color='g',alpha=0.4)\n",
    "        # 绘制原始数据X,y的线性图\n",
    "        plt.plot(X, y, color='b',alpha=0.7)\n",
    "        # 绘制预测数据x,与对应的y_pred的散点图\n",
    "        plt.scatter(x,y_pred, color='r',marker='^',s = 80,alpha=0.8)\n",
    "        plt.xlabel('X轴',fontsize=12)\n",
    "        plt.ylabel('Y轴',fontsize=12)\n",
    "        # 把x，和y_pred 作为注释添加到图中\n",
    "        for i,j in zip(x,y_pred):\n",
    "            plt.text(i[0]-0.6,j[0]+14,'（%.2f，%.2f）'%(i[0],j[0]), color = 'black', alpha=0.9,bbox=dict(boxstyle='round,pad=0.5', fc='r', ec='k', lw=1, alpha=0.1))\n",
    "    print('y_pred:',y_pred.reshape(1,-1))\n",
    "    print('线性模型的回归系数:',model.coef_[0][0])\n",
    "    print('线性模型的截距:',model.intercept_[0])\n",
    "    return y_pred, model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: [[121.49180661 144.30902429  59.40413943]]\n",
      "线性模型的回归系数: 15.521916793556553\n",
      "线性模型的截距: 1.1969514571311208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[121.49180661],\n",
       "        [144.30902429],\n",
       "        [ 59.40413943]]), array([[15.52191679]]), array([1.19695146]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "linear_Reg([[7.75],[9.22],[3.75]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)\n",
    "# 产生数据X,Y\n",
    "X = (np.random.random(100)*10).reshape(-1, 1)\n",
    "Y = np.array([f(i) for i in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1,x2):\n",
    "    return cosine(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_model(X,Y):\n",
    "    return [(Xi,yi) for Xi,yi in zip(X,Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, k = 5):\n",
    "    most_similars = sorted(knn_model(X,Y),key = lambda xi:distance(xi[0],x))[:k]\n",
    "    y_hats = [_y for x,_y in most_similars]\n",
    "    print(most_similars)\n",
    "    return np.mean(y_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9.86276829],\n",
       "        [8.73391946],\n",
       "        [5.09745525],\n",
       "        [2.71835714],\n",
       "        [3.36918728],\n",
       "        [2.16954265],\n",
       "        [2.76477143],\n",
       "        [3.43315593],\n",
       "        [8.62158935],\n",
       "        [1.5669967 ],\n",
       "        [1.40887245],\n",
       "        [7.57080281],\n",
       "        [7.36324918],\n",
       "        [3.55663092],\n",
       "        [3.41093017],\n",
       "        [6.66803051],\n",
       "        [2.17100639],\n",
       "        [5.61426982],\n",
       "        [1.24178778],\n",
       "        [3.19736484],\n",
       "        [9.53213873],\n",
       "        [1.37356791],\n",
       "        [5.69413097],\n",
       "        [9.75665483],\n",
       "        [5.03367061],\n",
       "        [6.67664206],\n",
       "        [0.34191498],\n",
       "        [4.56119366],\n",
       "        [1.55851363],\n",
       "        [4.76048966],\n",
       "        [1.69702438],\n",
       "        [8.96258337],\n",
       "        [3.73393758],\n",
       "        [3.7969293 ],\n",
       "        [8.58316588],\n",
       "        [6.46061055],\n",
       "        [5.83461698],\n",
       "        [6.68350027],\n",
       "        [1.7779262 ],\n",
       "        [8.49248017],\n",
       "        [4.42372582],\n",
       "        [8.3146771 ],\n",
       "        [7.63920716],\n",
       "        [9.19690977],\n",
       "        [0.70573427],\n",
       "        [1.56165416],\n",
       "        [6.36894232],\n",
       "        [5.55695687],\n",
       "        [1.91928713],\n",
       "        [4.25655983],\n",
       "        [5.13420208],\n",
       "        [2.69377586],\n",
       "        [5.99020019],\n",
       "        [2.20173945],\n",
       "        [3.00862494],\n",
       "        [0.48394484],\n",
       "        [5.64320059],\n",
       "        [9.36032205],\n",
       "        [8.03027642],\n",
       "        [6.97305149],\n",
       "        [4.61384276],\n",
       "        [6.62434611],\n",
       "        [7.49665638],\n",
       "        [5.31454892],\n",
       "        [4.82810126],\n",
       "        [0.29274233],\n",
       "        [8.94283104],\n",
       "        [8.31532084],\n",
       "        [7.37843461],\n",
       "        [8.98497298],\n",
       "        [2.30734141],\n",
       "        [1.7996051 ],\n",
       "        [6.14158122],\n",
       "        [0.16669482],\n",
       "        [2.99322246],\n",
       "        [6.68540878],\n",
       "        [9.52552436],\n",
       "        [8.16207012],\n",
       "        [0.77862236],\n",
       "        [1.34612929],\n",
       "        [5.76548117],\n",
       "        [1.77278624],\n",
       "        [9.944587  ],\n",
       "        [1.17374868],\n",
       "        [8.40844833],\n",
       "        [0.1402673 ],\n",
       "        [8.71687484],\n",
       "        [9.02406745],\n",
       "        [4.49204469],\n",
       "        [6.18181976],\n",
       "        [9.78978133],\n",
       "        [3.97278484],\n",
       "        [4.47230834],\n",
       "        [2.3325998 ],\n",
       "        [9.3172938 ],\n",
       "        [2.72802469],\n",
       "        [9.32327793],\n",
       "        [4.13230497],\n",
       "        [1.60678424],\n",
       "        [4.68802528]]), array([[197.87290847],\n",
       "        [175.3757516 ],\n",
       "        [125.01055637],\n",
       "        [ 55.13453571],\n",
       "        [ 34.2224028 ],\n",
       "        [ 37.62791108],\n",
       "        [  0.85395719],\n",
       "        [ 85.21391688],\n",
       "        [162.634635  ],\n",
       "        [  7.28844885],\n",
       "        [ 11.83752295],\n",
       "        [111.3474435 ],\n",
       "        [ 65.13036231],\n",
       "        [103.1277792 ],\n",
       "        [ 73.86941756],\n",
       "        [ 72.35447284],\n",
       "        [ -3.34940099],\n",
       "        [ 86.02118228],\n",
       "        [-19.75228945],\n",
       "        [ 71.55915509],\n",
       "        [186.74815034],\n",
       "        [ 33.29030257],\n",
       "        [ 92.25903002],\n",
       "        [107.22814985],\n",
       "        [ 41.02189445],\n",
       "        [151.48795195],\n",
       "        [-32.70031781],\n",
       "        [ 61.69850178],\n",
       "        [ 11.15696123],\n",
       "        [ 77.78758981],\n",
       "        [ 14.30387795],\n",
       "        [107.92004227],\n",
       "        [ 47.87603244],\n",
       "        [ 47.85240422],\n",
       "        [161.03907113],\n",
       "        [143.13946351],\n",
       "        [116.43656315],\n",
       "        [140.59425416],\n",
       "        [ -9.44214387],\n",
       "        [ 81.63344261],\n",
       "        [ 21.56775019],\n",
       "        [114.87749501],\n",
       "        [ 88.40771102],\n",
       "        [178.55210137],\n",
       "        [ 57.9388812 ],\n",
       "        [ 52.20563955],\n",
       "        [116.71860595],\n",
       "        [ 95.13283155],\n",
       "        [ 15.74895057],\n",
       "        [104.97667731],\n",
       "        [ 43.58013222],\n",
       "        [ -4.24647421],\n",
       "        [ 53.8481029 ],\n",
       "        [ 53.1269615 ],\n",
       "        [ 22.63368664],\n",
       "        [ -0.49885497],\n",
       "        [ 74.46960911],\n",
       "        [114.08499176],\n",
       "        [139.46928454],\n",
       "        [ 99.08229815],\n",
       "        [ 63.51456276],\n",
       "        [111.67736471],\n",
       "        [106.19817391],\n",
       "        [ 91.37550831],\n",
       "        [ 62.83556959],\n",
       "        [ -7.46249389],\n",
       "        [154.61388109],\n",
       "        [164.88747296],\n",
       "        [137.36573647],\n",
       "        [130.26708125],\n",
       "        [ 55.76379191],\n",
       "        [ 69.89387901],\n",
       "        [115.19450889],\n",
       "        [-30.41623029],\n",
       "        [ 69.39494812],\n",
       "        [144.62383605],\n",
       "        [103.64562752],\n",
       "        [106.51208689],\n",
       "        [ 44.06864655],\n",
       "        [ 57.86500396],\n",
       "        [ 66.36495813],\n",
       "        [ 59.47818676],\n",
       "        [146.14109845],\n",
       "        [-18.80689553],\n",
       "        [123.33094908],\n",
       "        [ 38.17414313],\n",
       "        [181.11156004],\n",
       "        [186.87304549],\n",
       "        [ 98.6266927 ],\n",
       "        [108.81820623],\n",
       "        [110.74161058],\n",
       "        [ 62.57816505],\n",
       "        [ 34.32077922],\n",
       "        [ 77.15529687],\n",
       "        [134.41805391],\n",
       "        [ 73.28438265],\n",
       "        [179.51080793],\n",
       "        [ 39.050727  ],\n",
       "        [ 33.90515577],\n",
       "        [ 39.66439186]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_knn = knn_model(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([5.03367061]), array([41.02189445])), (array([1.7779262]), array([-9.44214387])), (array([4.61384276]), array([63.51456276])), (array([1.17374868]), array([-18.80689553])), (array([9.86276829]), array([197.87290847]))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54.832065257690374"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| probs: [1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用icecream进行调试可以打印出变量名\n",
    "from icecream import ic  \n",
    "\n",
    "# 定义信息熵\n",
    "def entropy(elements):\n",
    "    '''群体的混乱程度'''\n",
    "    counter = Counter(elements)\n",
    "    probs = [counter[c] / len(elements) for c in set(elements)]\n",
    "    ic(probs)\n",
    "    return - sum(p * np.log(p) for p in probs)\n",
    "\n",
    "\n",
    "entropy([1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_optimal_spilter(training_data: pd.DataFrame, target: str) -> str:\n",
    "    x_fields = set(training_data.columns.tolist()) - {target}\n",
    "    ic(x_fields)\n",
    "    spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    \n",
    "    for f in x_fields:\n",
    "        ic(f)\n",
    "        values = set(training_data[f])\n",
    "        ic(values)\n",
    "        for v in values:\n",
    "            sub_spliter_1 = training_data[training_data[f] == v][target].tolist()\n",
    "            ic(sub_spliter_1)\n",
    "            # split by the current feature and one value\n",
    "            \n",
    "            entropy_1 = entropy(sub_spliter_1)\n",
    "            ic(entropy_1)\n",
    "            \n",
    "            sub_spliter_2 = training_data[training_data[f] != v][target].tolist()\n",
    "            ic(sub_spliter_2)\n",
    "            \n",
    "            entropy_2 = entropy(sub_spliter_2)\n",
    "            ic(entropy_2)\n",
    "            \n",
    "            entropy_v = entropy_1 + entropy_2\n",
    "            ic(entropy_v)\n",
    "            \n",
    "            if entropy_v <= min_entropy:\n",
    "                min_entropy = entropy_v\n",
    "                spliter = (f, v)\n",
    "    \n",
    "    print('spliter is: {}'.format(spliter))\n",
    "    print('the min entropy is: {}'.format(min_entropy))\n",
    "    \n",
    "    return spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "   # 'pet': [1, 1, 1, 0, 0, 0, 1],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(mock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_fields: {'gender', 'income', 'family_number'}\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_spliter_1: [0, 0, 1]\n",
      "ic| probs: [0.6666666666666666, 0.3333333333333333]\n",
      "ic| entropy_1: 0.6365141682948128\n",
      "ic| sub_spliter_2: [1, 1, 1, 0]\n",
      "ic| probs: [0.25, 0.75]\n",
      "ic| entropy_2: 0.5623351446188083\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| sub_spliter_1: [1, 1, 1, 0]\n",
      "ic| probs: [0.25, 0.75]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_spliter_2: [0, 0, 1]\n",
      "ic| probs: [0.6666666666666666, 0.3333333333333333]\n",
      "ic| entropy_2: 0.6365141682948128\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| f: 'income'\n",
      "ic| values: {'-10', '+10'}\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('family_number', 2)\n",
      "the min entropy is: 0.6730116670092565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('family_number', 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_optimal_spilter(training_data=dataset, target='bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "2      F    +10              2       1\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['family_number']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['family_number']!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_fields: {'gender', 'income', 'family_number'}\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_spliter_1: [0, 0]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 1, 0]\n",
      "ic| probs: [0.3333333333333333, 0.6666666666666666]\n",
      "ic| entropy_2: 0.6365141682948128\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| sub_spliter_1: [1, 1, 0]\n",
      "ic| probs: [0.3333333333333333, 0.6666666666666666]\n",
      "ic| entropy_1: 0.6365141682948128\n",
      "ic| sub_spliter_2: [0, 0]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| f: 'income'\n",
      "ic| values: {'-10', '+10'}\n",
      "ic| sub_spliter_1: [1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 0, 0, 0]\n",
      "ic| probs: [0.75, 0.25]\n",
      "ic| entropy_2: 0.5623351446188083\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| sub_spliter_1: [1, 0, 0, 0]\n",
      "ic| probs: [0.75, 0.25]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_spliter_2: [1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: []\n",
      "ic| probs: []\n",
      "ic| entropy_2: 0\n",
      "ic| entropy_v: 0.6730116670092565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('income', '+10')\n",
      "the min entropy is: 0.5623351446188083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('income', '+10')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_optimal_spilter(dataset[dataset['family_number'] != 2], 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['family_number'] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_n_1 = dataset[dataset['family_number'] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_n_1[fm_n_1['income']=='+10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "1      F    -10              1       1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_n_1[fm_n_1['income'] != '+10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_fields: {'gender', 'income', 'family_number'}\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_spliter_1: [0, 0]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 0]\n",
      "ic| probs: [0.5, 0.5]\n",
      "ic| entropy_2: 0.6931471805599453\n",
      "ic| entropy_v: 0.6931471805599453\n",
      "ic| sub_spliter_1: [1, 0]\n",
      "ic| probs: [0.5, 0.5]\n",
      "ic| entropy_1: 0.6931471805599453\n",
      "ic| sub_spliter_2: [0, 0]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6931471805599453\n",
      "ic| f: 'income'\n",
      "ic| values: {'+10'}\n",
      "ic| sub_spliter_1: [1, 0, 0, 0]\n",
      "ic| probs: [0.75, 0.25]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_spliter_2: []\n",
      "ic| probs: []\n",
      "ic| entropy_2: 0\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1}\n",
      "ic| sub_spliter_1: [1, 0, 0, 0]\n",
      "ic| probs: [0.75, 0.25]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_spliter_2: []\n",
      "ic| probs: []\n",
      "ic| entropy_2: 0\n",
      "ic| entropy_v: 0.5623351446188083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('family_number', 1)\n",
      "the min entropy is: 0.5623351446188083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('family_number', 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_optimal_spilter(fm_n_1[fm_n_1['income'] == '+10'], 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_n_1[(fm_n_1['income'] == '+10') & (fm_n_1['family_number'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  bought\n",
       "0      F       1\n",
       "3      F       0\n",
       "4      M       0\n",
       "5      M       0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_n_1.loc[fm_n_1['income'] =='+10',['gender','bought']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子，产生数据\n",
    "np.random.seed(42)\n",
    "X = np.random.randint(1,100,200).reshape(100,2)\n",
    "X1 = X[:,0]\n",
    "X2 = X[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2281111d400>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD6CAYAAAC1W2xyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGNBJREFUeJzt3W+MXFd5x/Hvg+uoS5JiR1kZ2Y1jW4qCWqLEsCJx10GOS4iiCJoG2kD584JKFhWCF0hBG9UItWrLKkGAimSEkYOQ0iKZNqxADk3amjTBslOtZaFIkAikxESLLBkc24W6UmSevpjZeL2Z2Z1759x7z5/f541nr3dnzr135sxzn/Occ83dERGRfLyh6waIiEhY6thFRDKjjl1EJDPq2EVEMqOOXUQkM+rYRUQyo45dRCQz6thFRDKjjl1EJDO/08WLXnvttb5ly5YuXlpEJFnHjx//pbtPrvZ7I3XsZrYB+Bd3v93M1gKPAdcAB9z9kUHbVnq+LVu2MD8/P8pLi4hIn5mdHOX3Vk3FmNl64JvAlf1NnwSOu/s08H4zu3rINhER6cAoOfaLwP3A+f7Pu4CD/cdPA1NDtomISAdW7djd/by7n1uy6Upgof/4DLBhyLbLmNkeM5s3s/nTp0+P12oRERmqTlXMr4GJ/uOr+s8xaNtl3H2/u0+5+9Tk5Kq5fxERqalOx34c2Nl/fDPw0pBtIiLSgTrljt8EHjez24E/AJ6ll4ZZvk1ExjR3YoGHn3iBX5y9wMZ1Ezxw143cu31T182SyI0csbv7rv6/J4E7gSPAu9z94qBtDbRVpChzJxZ48LHnWDh7AQcWzl7gwceeY+7Ewqp/K2WzLm6NNzU15apjl1R0FTVPzx5m4eyFgf+3SdF7kczsuLuvWnXYycxTkVQsRs0XXu1dhC5GzUDjneovhnTqbbdD0qO1YgKaO7HA9Oxhts4cYnr2sC6ZM/DwEy+81qkvuvDqRR5+4oXGX3vjuokV/7+tdkh61LEHonxonoZFzStF06E8cNeNTKxds+LvtNEOSY869kC6jOykOcOi5tWi6RDu3b6Jz993E5tWeK022iHpya5j7yod0mVkJ80ZFDVPrF3DA3fd2Mrr37t9E0dmdvPl+2/ptB2SlqwGT7sc6Nq4bmJgBYMiqrQtvm+6riWPpR2Shqw69pXSIU1/AB6468bLvlRAEZWEde/2TerIZSRZdexdpkMUUeWpy6tAkbqy6ti7TocoospPl1eB8npaYmE0WQ2edj3QJfnRoHg8VFI8uqwidqVDpIpRor+urwLlktVKivW5vySrjh2UDpHRjJo716B4PIZdJS2eO42DXJJdxz6OtvN3yhd2Z9Tcua4C4zHs6mmNmcZBllHH3td29YOqLbpVJXeuq8A4DLt6Wt6pLyp5HCS7jr1uFNx29UPd16u6f7oqGEy58/QMu3p6+IkXgp7LHD4zWXXs40TBbVc/1Hm9qvunq4LhlDtP07Crp1DnMpfPTFbljuMsxNX2Yk91Xq/q/mlhsuGWLrBl9G5c8fn7bkrqwys9Ic9lLp+ZrCL2caLuO94yyaPHfj5wexPqRIxV90812CtT7jwfoc5lLp+ZrCL2caLuHzx/utL2cdWJMqruX5dLzoqE0uaKrbl8ZrLq2MeZedrFN/Xikqwvzt7DkZndq0YcVfdPM3EldW3PNs3lM5NVKmacmuMUqiSq7p9qsOOVQ+VFG9quVsvlM2Pu3vqLTk1N+fz8fOuvu5Llo+HQ+6bWgJqEpvfa6LbOHGJQD2XAi7P3tN2czpnZcXefWu33skrFjENVEt0r5WbguVRetCGXnHfbskrFjEtVEt3JpX54FLlUXrRB8w3qUcQuUSgpilUUOjpdSdejiF2iUFIUqyi0Gl1JV6eIXaJQUhSrKFSapohdolBaFKsoVJqkjl2ikEv9sFSjev5mqGOXaCiKLUtJlVBtU45dRDpRUiVU2xSxj0CXiyL1Dfv8lFQJ1TZ17KvQ5aJIfSt9flJYnylVlVMxZrbezB43s3kz+1p/2wEzO2pme8M3sVu6XBSpb6XPTy4rKcaoTo79I8A/9ReiudrMPgOscfcdwDYzuyFoCzumy0WR+lb6/Kievzl1UjG/At5qZuuA64BzwMH+/z0J7AR+GqZ53dPlokh9q31+VAnVjDoR+w+B64FPAT8BrgAWl+E7A2wY9Edmtqefvpk/fbqZuxI1QZeLIvXp89ONOhH754CPu/t5M/s08PfA1/v/dxVDvizcfT+wH3rrsdd43U5o4oyUKkQ1mD4/3ajTsa8HbjKzY8CtwCy99Msx4GYgu1FFXS5KaUJWg+nz0746HfvngW/QS8ccBb4EPGNmG4G7gdvCNU+kbF3NoWj7lnQSVuWO3d3/G/jDpdvMbBdwJ/CQu58L0zSRsnU5h0LVYGkLsqSAu7/i7gfd/VSI5xORbudQlLSMco60VoxIpLqMmlXNkjYtKSASqS7nUIxazaJ1lOKkjl0kUne8ZZJHj/184PY2rFbNonWU4qVUjEikfvD84Il8w7a3TesoxUsdu0ikYq9Mib19JVMqJnLKYZYr9nWKYm9fyRSxR2wxh7lw9gLOpRzm3ImFVf9W0hd7ZUrs7SuZIvaGhIi0NfuvbDGts7LS+zmG9snl1LE3IFS1gHKYEsM6K6u9n7tun7yeOvYGhIq0Y8thKt/fnJiPbU5XjjEf55CUY29AqEg7phym8v3Nif3Y5nLlGPtxDimLjn3uxALTs4fZOnOI6dnDnZ+oUOtsxHTrMNUsNyf2Y5vLujGxH+eQkk/FxDj77YG7brysTVA/0o4lh5lL1Baj2I9tyPdzl2I/ziElH7HH+C0cU6QdSi5RW4xiP7a5vJ9jP84hJR+xx/otHEukHUouUVuMUji2ObyfUzjOoSTfscdWOZIr1Sw3R8e2HSUdZ3Nv/77SU1NTPj8/H+S5lufYofctnOKloojISszsuLtPrfZ7yUfsJX0Li4iMIvmOHfLI/4mIhJJ8VYyIiFwui4hdqitlarVIidSxFyjGSV0iEo469gI1uahTl1cCugoR6VHHXqCmJnV1eSWgqxCRSzR4WqCmplZ3ubxDjEtLiHRFHXuBmloOuMvlHWJdWkKkC0rFFKipSV1dLu+gpSVkkHHHXVIdt1HHXqgmJnV1uchSSQs8yWjGHXdJedxGHfsSqX07x9beLpd3WOm1YztOVcTY9hjbtNRi+wZdwVWp/kr5loDq2PtS+3aOtb1dLu8w6LVjPU6jiLHtMbZpqUGLAi436rhLyuM2GjztS62qIrX2diXl4xRj22Ns01KD2rfcqOMuKd+YQx17X2rfzqm1tyspH6cY2x5jm5ZarR1Vxl1iupl8VUrF9KVWVZFae7uS8nFaqe1d5bljP57D2ge9W/qNcpyWHts3Tazld9e+gbP/+2qU4wnD1I7YzWyfmb2n//iAmR01s73hmtau1L6dU2tvV1I+TsPafsdbJnnwsedYOHsB51Kee+7EQmdtiuV4Dmvfl++/hSMzu0fq1Jce27MXXuX/Xv0tXxrx72NRq2M3s9uBN7v798zsPmCNu+8AtpnZDUFb2JLUbtibWnu7kvJxGtb2Hzx/urM8d+zHc9z2xT6GMKrKt8Yzs7XAc8DjwH8Bfwz8m7s/bmYfACbc/RsD/m4PsAdg8+bNbz958uS4bRcp0taZQwz61Brw4uw9bTcnK7Ef21FvjVcnYv8o8GPgIeAdwCeAxWvAM8CGQX/k7vvdfcrdpyYnJ2u8rIhAvNUacycWmJ49zNaZQ0zPHm4lNRRarMe2qjod+3Zgv7ufAh4FngYW9/qqms8pIiOKMc+9PDfdZt4/pBiPbR11qmJ+BmzrP54CtgA7gWPAzUBaySiRiK1U/RLT7M+UZ2kuFeOxraNOx34AeKSfT18L7AK+a2YbgbuB28I1T6Rcq83yjKmzib2+vYrYjm0dldMm7v4/7v5n7v5Od9/h7ifpde7HgDvc/VzoRoqUKKUKjVxy07kIkg9391fc/WA/7y4iAaQUBeeSm86FBjpFIpVSFBx7fXtptKSAyAAxLE2b2hrzOeSmc6GOXWSZWJamzaVCQ9qXXcc+LNKKIQKTNMRUuqcoWOrIqmMfFmnNnzzDvx5f6DwCkzSkNGgpMkhWg6fDIq1vPftyMmVj0r2UBi1FBsmqYx8WUV0cstCZIjAZRKV7krqsOvZhEdUas0q/L2VT6Z6kLqsc+7DysPe9fdNlOfbF7YrAZBgNWkrKsurYVyoPm7r+GlXFiLREVWjdqnyjjRCmpqZ8fn6+9dcVkeYtr06D3hWy0lnjG/VGG1lF7BIXRW1limkeQKnUsUsjYpm9Ke3TPIDuZVUVI/FIaclZCUvzALqnjl0aoaitXJoH0D2lYqSW1fLnG9dNsDCgE1fUlj8tXtY9dexS2Sj589SWnJWwNA+gW+rYpbJRqh4UtYlcrs0qMXXsUtmo+XNFbSI9bVeJafBUKlPVg0g1bVeJqWOXylT1IFJN21ViSsVIZV3kzzWLVVLWdpWYOnappc38uWaxSurarhJTKkaip1mskrq21/hXxF6olFIbmsUqUo069gKlltrQLFZJncodpXGppTZUhSOpa/szp4i9QKmlNjSLVVKnckdpXIqpjdxmsaY0xpGrNs9B2585pWIKpNRGtxbzrQtnL+BcyrfOnVjoumnFaPsctP2Zy/qep4qKhtOx6c707OGB0dsaM37rrvPRgmHnYNO6CY7M7G7kNUN85oq/52lqlR9tyy21kZJhedWL/SBL79XmdTHO1OZnrnYqxsw2mNmJ/uMDZnbUzPaGa9p4Uqv8kHKMklfVe7VZuS9kN06O/QvAhJndB6xx9x3ANjO7IUzTxpNa5YeUY1C+dRC9V5uT+zhTrY7dzHYDvwFOAbuAg/3/ehLYGaRlY8r9G1nStXx6+Rqzgb+n92pz2p7i37bKOXYzuwL4LPCnwBxwJbA4lHwGeNuQv9sD7AHYvHlznbZWoluzScyW5luXjweB3qttyHmcqc7g6Qywz93PWi/S+DWwGFpcxZCrAHffD+yHXlVMjdetRJNaJBV6r0polcsdzexp4Lf9H2+hF6Xvc/cvmNnfAC+4+z+v9BxtlTuKiOSksXJHd3/nkhd5Cngv8IyZbQTuBm6r+pwiIk0odb7GWDNP3X2Xu5+nN4B6DLjD3c+FaJiIyDhKnuEbZEkBd3/F3Q+6+6kQzyciMq6S57JorRgRyVLJc1myXVJgVKXm4Eqic1ymFFcxDaXoiL3kHFwpdI7Llfvs0pUUHbGvlIMbFtEp+ovLauejzjmWPJQ8P6Dojr1qDk4rRsZllPNRcp5V8p5dupKiUzFV15PpYpR97sQC07OH2TpziOnZw9GnENps7yjnQ2sGSYmK7tir5uDajv5Syw+33d5RzkfJeVYpV9Ede9UV3tqO/lKrw227vaOcj9xX8RMZpOgcO1TLwbW9YmRq+eG22zvq+Sg1zyrlKjpir6rt6C+1/HDb7VU0LjJY8RF7VW1Gf6mtKd9FexWNi7yeOvaIpVaHm1p7RXJVeT32ELQeu4jUUXWCYG4TChtbj11EpAtVJwiWPKFQg6eSnNQmbUkYVctpUysXDkkRuySl5CisdFXLaVMrFw6pmIhdUV4eSo7CSle1nDa1cuGQiujYU5uaL8OVHIWVruryECUvJ5FMKmac0W0t3ZqPkm+e0LbYKkqqltOWXH6bRMc+bl5VUV4+Upu0lapYxzKqTkgrdQJbEqmYcfOqJefacqNlBNqhsYy0JRGxjxtxK8rLS6lRWJt0lZu2JCL2cSNuRXki1egqN21JROwhIm5FeSKj01Vu2pLo2Ese3Za4xFYp0pRcPnOlnK/ltAiYyIiWV4pAL4pVWi9OOZ6vURcBSyLHLhIDVYqkpeTzpY5dZESqFElLyecriRz7akrNo0m7NOs1LSWfr+Qjdq0DI20pee2RFJV8vpKP2LUOjLSlrUoRXYGGkUtlTx3Jd+wl59GkfU3Ph4h1jZZUlTp/JflUjGbISU5KruSQcCp37Gb2JjP7vpk9aWbfMbMrzOyAmR01s71NNHIlJefRJD+6ApUQ6kTsHwK+6O7vBk4BHwDWuPsOYJuZ3RCygavROjCSE12BSgiVc+zuvm/Jj5PAh4Ev939+EtgJ/HT535nZHmAPwObNmys3dCWl5tEkP1qjRUKonWM3sx3AeuBlYLG28AywYdDvu/t+d59y96nJycm6LyuSNV2BSgi1qmLM7BrgK8D7gE8Di9eJV5HBgKy0R6V9r6crUBlXncHTK4BvAw+6+0ngOL30C8DNwEvBWidZ0+QykWbUia7/Engb8Ndm9hRgwEfM7IvAnwOHwjVPcqbSPpFm1Bk8/Srw1aXbzOy7wJ3AQ+5+LlDbJHMq7RNpRpB8uLu/4u4H3f1UiOeTMqi0T6QZGuiUzmhymUgzkl8rpmShKkq6qkzpYpEmVeF0b+k5WPfGtbjDuQuv6nwEpFvjJSrUbb9yvH3YMCXta6wGnYOldD5WplvjZS5URUlJlSkx7+vciQWmZw+zdeYQ07OHsy35HHQOlmrzfOR8zJWKSVSoipKSKlNi3deSluod5Vi3cT5yP+aK2BMVqqKkycqU2CKiWKtwYr6SCG2UY93G+cj9mKtjT1SoipKmKlNinFUaaxVOrFcSTRh0DpZq63zkfszVsScq1GJRTS06FWNEFOsCW7FeSTRh+TlY/8a1rJtY2/r5yP2YqypGGrF15hCD3lkGvDh7T9vNiZqqddqX6jEftSpGg6fSiI3rJlgYcFmbS0QUUsk3Xe5K7sdcEbs0ItWISCRmitilU7lHRCIxU8cujdENI0S6oY69IFonRaQM6tgLkftMOxG5JJmOXdHmeFaqK0/5OOp9IfJ6SXTsijbHl+NMO70vRAZLYuZpjLMYUzOsftwhinVc6mjyfRHbOjciVSTRsecYbbZtpTU6YljHpY6m3hcxrnMjUkUSHXvu6zq0YekaHYOkeAXU1PtCV4iSuiQ69lhX5UvNvds3cWRmNzbk/1O7AmrqfaErREldEh17rKvypSqXK6Cm3he5HB8pVxJVMRLWA3fdOHAdlxSvgJqY3ZrT8ZEyJdGxq6wtLK3jsjIdH0ldEqs7Ts8eHrgE7KZ1ExyZ2R2yaSIi0Rp1dcckcuwazBIRGV0SqRjdtEHkciktpZBSW3ORRMSuckeRS1KaQJVSW3OSRMReZTBL0UH8Uj5He+ee41vPvsxFd9aY8cFbr+Pv7r2p1TaktKBbSm3NSRIdO4xW1qbqmfilfI72zj3Ho8d+/trPF91f+7nNzj2lMaeU2pqTJFIxo9JU8PilfI6+9ezLlbY3JaUJVCm1NSdZdeyKDuKX8jm6OKQ0eNj2pqQ05pRSW3MSrGM3swNmdtTM9oZ6zqoUHcQv5XO0xgavsjNse1NSWmIjpbbmJEiO3czuA9a4+w4ze8TMbnD3n4Z47io0FTx+KZ+jD9563WU59qXb25bSjcJTamsuQg2e7gIO9h8/CewEWu/YNRU8fimfo8UB0q6rYkRWE2RJATM7APyju//IzN4NvM3dZ5f9zh5gD8DmzZvffvLkybFfV0SkJG0vKfBrYDFJetWg53X3/e4+5e5Tk5OTgV5WRESWC9WxH6eXfgG4GXgp0POKiEhFoXLsc8AzZrYRuBu4LdDziohIRUEidnc/T28A9Rhwh7ufC/G8IiJSXbAlBdz9FS5VxoiISEc6udGGmZ0GqpTFXAv8sqHmxEz7XZ5S9137PZrr3X3V6pNOOvaqzGx+lBKf3Gi/y1Pqvmu/w8pqrRgREVHHLiKSnVQ69v1dN6Aj2u/ylLrv2u+Aksixi4jI6FKJ2EVEZETq2EVEMhN9xx7DDTzaYmZvMrPvm9mTZvYdM7uisP3fYGYn+o9L2u99Zvae/uPs99vM1pvZ42Y2b2Zf62/Ler/77+1n+o/Xmtn3zOyImX1s2LZxRN2xL72BB7DNzG7ouk0N+xDwRXd/N3AK+ABl7f8XgImSzruZ3Q682d2/V9B+fwT4p3799tVm9hky3m8zWw98E7iyv+mTwHF3nwbeb2ZXD9lWW9QdO4Nv4JEtd9/n7v/e/3ES+DCF7L+Z7QZ+Q+8LbRcF7LeZrQW+DrxkZn9CIfsN/Ap4q5mtA64DtpL3fl8E7gfO93/exaX9fRqYGrKtttg79iuBhf7jM8CGDtvSGjPbAawHXqaA/TezK4DPAjP9TaWc948CPwYeAt4BfIIy9vuHwPXAp4CfAFeQ8X67+/llCyMOen8Hfc/H3rGvegOP3JjZNcBXgI9Rzv7PAPvc/Wz/51L2ezuw391PAY/Si9RK2O/PAR93978Fngf+gjL2e9Gg93fQ93zsB7CoG3j0I9dvAw+6+0nK2f93AZ8ws6eAW4D3UMZ+/wzY1n88BWyhjP1eD9xkZmuAW4FZytjvRYM+10E/61FPUDKz3wOeAf6T/g08cl7r3cz+CvgH4Ef9Td8APk0h+w/Q79zfSwHnvT9A9gi9y+619AbLv0v++/0Oeu/t64GjwPso43w/5e67zOx64HHgP4A/ondjot9fvs3dL9Z+rZg7dnhtRPlO4On+JWtRSt1/7bf2O2f9u83tBJ5Y/BIbtK3288fesYuISDWx59hFRKQidewiIplRxy4ikhl17CIimVHHLiKSmf8Hpka9ObqS11IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X1, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=500,\n",
       "    n_clusters=6, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = KMeans(n_clusters=6, max_iter=500)\n",
    "cluster.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.68421053, 77.57894737],\n",
       "       [44.46666667, 47.        ],\n",
       "       [72.92592593, 77.40740741],\n",
       "       [ 7.72727273, 22.72727273],\n",
       "       [45.6       , 10.66666667],\n",
       "       [85.53846154, 29.15384615]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 聚类中心\n",
    "cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 4, 2, 2, 5, 3, 4, 5, 4, 2, 3, 2, 0, 2, 1, 2, 5, 2, 1, 1, 4,\n",
       "       4, 0, 4, 0, 4, 0, 4, 2, 2, 4, 1, 2, 1, 3, 0, 0, 2, 0, 1, 2, 0, 4,\n",
       "       2, 2, 0, 2, 4, 0, 1, 4, 1, 2, 0, 3, 2, 0, 3, 1, 3, 4, 2, 3, 1, 0,\n",
       "       5, 5, 2, 1, 5, 5, 3, 5, 4, 5, 5, 5, 2, 2, 0, 3, 1, 4, 0, 2, 0, 1,\n",
       "       1, 3, 1, 5, 2, 2, 2, 3, 2, 0, 5, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 聚类labels\n",
    "cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "centers = defaultdict(list)\n",
    "\n",
    "for label, location in zip(cluster.labels_, training_data):\n",
    "    centers[label].append(location)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "color = ['red', 'green', 'grey', 'black', 'blue', 'orange']\n",
    "\n",
    "for i, c in enumerate(centers):\n",
    "    for location in centers[c]:\n",
    "        plt.scatter(*location, c=color[i],marker='*')\n",
    "        \n",
    "for center in cluster.cluster_centers_:\n",
    "    plt.scatter(*center, s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "\n",
    "#### 模型是什么？\n",
    "\n",
    "所谓统计模型，顾名思义就是用以刻画、反映现象发展变化趋势、或测度不同现象之间内在联系关系、或据以推断总体特征的数学方程，所以模型构建的过程，就是把上述趋势、关系和特征进行量化的过程，因而统计模型的关键词就是变量、参数和方程形式。\n",
    "\n",
    "#### 为什么所有的模型都是错的，但有些是有用的？\n",
    "\n",
    "##### 前半句理解：\n",
    "\n",
    "1、首先，统计模型毕竟只是用一定的方程给出一个模拟的型态，由于不知道所研究问题的实际情况，所以模拟的过程没有真实的参照物，只能根据已经掌握了的有关信息去勾勒其可能的状态，因而它不是真实的，与实际情况不可能完全一致。\n",
    "\n",
    "2、统计模型的构建有很多假设条件或限制条件，这些假设通常由一组概率分布来描述，其中一些概率分布被假定为充分近似于对特定总体进行抽样的分布，因此，统计模型只是以相当理想化的形式来表示所研究问题的数据生成过程，是统计推断理论的形式化表示，而这些假设条件或者分布往往是不严格成立的，有时甚至差距很大。\n",
    "\n",
    "3、统计模型由与一个或多个变量相关的数学方程来确定，确定的依据是已掌握的样本数据或历史数据，是样本数据（历史数据）与方程形式固化的理想结果，因此在据以进行放大推断或外推预测时，实际情况不可能与模型保持一致。\n",
    "\n",
    "4、在统计模型中，我们通常都把涉及到的变量当成随机变量来处理，而事实上很多变量、特别是社会经济统计变量并不是完全的随机变量而是半随机变量，因此，基于随机变量假设的统计模型不能很好地刻画半随机变量的统计数据特征。\n",
    "\n",
    "##### 后半句理解：\n",
    "\n",
    "统计研究作为量化认识事物本质特征的方式，就是要用统计数据去描述事物特征，以统计规律去逼近事物的本质规律，而统计模型则是发现和描述统计规律的有效工具之一。当基于统计模型的统计规律能够比较好地解释客观事物的问题、本质和发展趋势时，它就是有用的。也就是说，统计模型是否有用，取决于它对现实问题的定量解释能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "#### 什么是欠拟合和过拟合？\n",
    "\n",
    "过拟合(overfitting)：“过拟合”常常在模型学习能力过强的情况中出现，此时的模型学习能力太强，以至于将训练集单个样本自身的特点都能捕捉到，并将其认为是“一般规律”，同样这种情况也会导致模型泛化能力下降。\n",
    "\n",
    "欠拟合(underfitting)：“欠拟合”常常在模型学习能力较弱，而数据复杂度较高的情况出现，此时模型由于学习能力不足，无法学习到数据集中的“一般规律”，因而导致泛化能力弱。\n",
    "\n",
    "两者的区别：欠拟合在训练集和测试集上的性能都较差，而过拟合往往能较好地学习训练集数据的性质，而在测试集上的性能较差。在神经网络训练的过程中，欠拟合主要表现为输出结果的高偏差，而过拟合主要表现为输出结果的高方差。\n",
    "\n",
    "#### 列出可能导致模型过拟合或欠拟合的原因\n",
    "\n",
    "欠拟合原因：1、模型复杂度过低；2、特征量过少\n",
    "\n",
    "欠拟合解决方法：1、增加新特征；2、增加模型复杂度\n",
    "\n",
    "过拟合原因：1、训练集和测试集特征分布不一致；2、数据噪声太大；3、数据量太小；4、特征量太多；5、模型太过复杂\n",
    "\n",
    "过拟合解决方法：1、减少特征数量；2、正则化；3、增大样本训练规模；4、简化模型；5、交叉验证；6、使用dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "#### 什么是precision, recall, AUC, F1, F2score？\n",
    "\n",
    "精确率（Precision）：查准率。即正确预测为正的占全部预测为正的比例。个人理解：真正正确的占所有预测为正的比例。\n",
    "\n",
    "召回率（Recall）：查全率。即正确预测为正的占全部实际为正的比例。个人理解：真正正确的占所有实际为正的比例。\n",
    "\n",
    "AUC值：AUC (Area Under Curve) 被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围一般在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。\n",
    "\n",
    "综合评价指标（F-score）：\n",
    "\n",
    "$$F_\\beta=(1+\\beta^2)\\cdot \\frac{precision \\cdot recall}{(\\beta^2 \\cdot precision)+recall}$$\n",
    "\n",
    "F-score中β值为1时，称为F1-score,是一个兼顾考虑了Precision 和 Recall 的评估指标,就是模型的Precision 和 Recall的调和平均数，这时，Precision 和 Recall都很重要，权重相同\n",
    "\n",
    "F-score中β值为2时，称为F2-score,这时，Recall的权重 > Precision的权重,认为Recall更重要\n",
    "\n",
    "#### 他们的主要目标是什么？\n",
    "\n",
    "precision, recall, AUC, F1, F2score都是分类模型常用的评价指标，当你费尽全力去建立完模型后，你会发现仅仅就是一些单个的数值或单个的曲线去告诉你你的模型到底是否能够派上用场。在实际情况中，我们会用不同的度量去评估我们的模型，而度量的选择，完全取决于模型的类型和模型以后要做的事。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "#### 什么是机器学习？\n",
    "\n",
    "机器学习就是对计算机一部分数据进行学习，然后对另外一些数据进行预测与判断。\n",
    "\n",
    "机器学习的核心是“使用算法解析数据，从中学习，然后对新数据做出决定或预测”。也就是说计算机利用以获取的数据得出某一模型，然后利用此模型进行预测的一种方法，这个过程跟人的学习过程有些类似，比如人获取一定的经验，可以对新问题进行预测。\n",
    "\n",
    "机器学习是一门多领域交叉学科，涉及概率论、统计学、计算机科学等多门学科。机器学习的概念就是通过输入海量训练数据对模型进行训练，使模型掌握数据所蕴含的潜在规律，进而对新输入的数据进行准确的分类或预测。\n",
    "\n",
    "#### 机器学习这种思维方式和传统的分析式编程的区别？\n",
    "\n",
    "在传统编程中，你需要对程序的行为进行硬编码,针对解决已知的问题，称为IT。\n",
    "\n",
    "在机器学习中，你将大量内容留给机器去学习数据，针对解决未知的问题，称为DT。ML 被用在传统编程策略无法满足的场景，而且它不足以独立完全完成某项任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans：\n",
    "\n",
    "我觉得这句话是正确的；\n",
    "\n",
    "因为分类模型、回归模型、聚类模型都有各自的评价标准，而模型评价对于一个模型的泛化能力或者模型好坏，都需要用各自的评价标准来衡量的，所以评价指标对于机器学习是非常重要的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始训练数据\n",
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "   # 'pet': [1, 1, 1, 0, 0, 0, 1],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "training_data = pd.DataFrame(mock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_optimal_spilter(training_data: pd.DataFrame, target: str) -> str:\n",
    "    x_fields = set(training_data.columns.tolist()) - {target}\n",
    "    #ic(x_fields)\n",
    "    spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    columns = training_data.columns.tolist()\n",
    "    for f in x_fields:\n",
    "        # ic(f)\n",
    "        values = set(training_data[f])\n",
    "        # ic(values)\n",
    "        for v in values:\n",
    "            sub_spliter_1 = training_data[training_data[f] == v][target].tolist()\n",
    "            #ic(sub_spliter_1)\n",
    "            # split by the current feature and one value\n",
    "            \n",
    "            entropy_1 = entropy(sub_spliter_1)\n",
    "            #ic(entropy_1)\n",
    "            \n",
    "            sub_spliter_2 = training_data[training_data[f] != v][target].tolist()\n",
    "            #ic(sub_spliter_2)\n",
    "            \n",
    "            entropy_2 = entropy(sub_spliter_2)\n",
    "            #ic(entropy_2)\n",
    "            \n",
    "            entropy_v = entropy_1 + entropy_2\n",
    "            #ic(entropy_v)\n",
    "            \n",
    "            if entropy_v <= min_entropy:\n",
    "                min_entropy = entropy_v\n",
    "                spliter = (f, v)\n",
    "    #axis = columns.index(spliter[0])           \n",
    "    print('spliter is: {}'.format(spliter))\n",
    "    print('the min entropy is: {}'.format(min_entropy))\n",
    "    #print('the axis is: {}'.format(axis))\n",
    "    return spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义信息熵\n",
    "def entropy(elements):\n",
    "    '''群体的混乱程度'''\n",
    "    counter = Counter(elements)\n",
    "    probs = [counter[c] / len(elements) for c in set(elements)]\n",
    "    # ic(probs)\n",
    "    return - sum(p * np.log(p) for p in probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按给定特征划分数据集:\n",
    "def data_spilt(training_data,target,feature,value):\n",
    "    get_value = (training_data[feature]==value)\n",
    "    split_data = training_data[get_value].drop(columns=feature)\n",
    "    return split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果决策树递归生成完毕，且叶子节点中样本不是属于同一类，则以少数服从多数原则确定该叶子节点类别\n",
    "def voting_classification(label_list):\n",
    "    label_set = {}\n",
    "    # 统计每个类别的样本个数\n",
    "    for label in label_list:\n",
    "        if label not in label_set.keys():\n",
    "            label_set[label] = 0\n",
    "        label_set[label] += 1\n",
    "    # iteritems：返回列表迭代器\n",
    "    # operator.itemgeter(1):获取对象第一个域的值\n",
    "    # True：降序\n",
    "    sorted_label_set = sorted(label_set.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_label_set = sorted_label_set[0][0]\n",
    "    return sorted_label_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建决策树 \n",
    "def create_decision_tree(training_data: pd.DataFrame, target: str):\n",
    "    # 数据集的分类类别\n",
    "    class_list = [dataVec[-1] for dataVec in training_data.values]\n",
    "    if len(class_list) == class_list.count(class_list[0]):\n",
    "        return class_list[0]\n",
    "    # 所有特征已经遍历完，停止划分，返回样本数最多的类别\n",
    "    if len(training_data.values[0]) == 1:\n",
    "        return voting_classification(class_list)\n",
    "    # 选择最好的特征进行划分\n",
    "    best_feat , _ = find_the_optimal_spilter(training_data,target)\n",
    "    # 以字典形式表示树\n",
    "    mytree = {best_feat:{}}\n",
    "    # 根据选择的特征，遍历该特征的所有属性值，在每个划分子集上递归调用create_decision_tree\n",
    "    for values in training_data[best_feat].unique():\n",
    "        spilt_data = data_spilt(training_data,target,best_feat,values)\n",
    "        mytree[best_feat][values] = create_decision_tree(spilt_data,target)\n",
    "    return mytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('family_number', 2)\n",
      "the min entropy is: 0.6730116670092565\n",
      "spliter is: ('income', '+10')\n",
      "the min entropy is: 0.5623351446188083\n",
      "spliter is: ('gender', 'F')\n",
      "the min entropy is: 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "tree_model = create_decision_tree(training_data, 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'family_number': {1: {'income': {'+10': {'gender': {'F': 1, 'M': 0}},\n",
       "    '-10': 1}},\n",
       "  2: 1}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成的树以字典形式显示\n",
    "tree_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测部分，对单条数据进行预测\n",
    "def predict(tree,test_data):\n",
    "    # 获取第一个节点分类特征\n",
    "    first_feat = list(tree.keys())[0]\n",
    "    # 得到firstFeat特征下的决策树（以字典方式表示）\n",
    "    second_dict = tree[first_feat]\n",
    "    feat_index = test_data[first_feat]\n",
    "    #遍历firstFeat下的每个节点\n",
    "    for key in second_dict.keys():\n",
    "        if feat_index == key:\n",
    "            #如果节点类型为字典，说明该节点下仍然是一棵树，此时递归调用predict进行预测\n",
    "            if type(second_dict[key]).__name__ == 'dict':\n",
    "                class_label = predict(second_dict[key],test_data)\n",
    "            else:\n",
    "                class_label = second_dict[key]\n",
    "    return class_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对test_data所有数据进行预测并输出dataframe\n",
    "def get_dt_predict(tree,test_data):\n",
    "    pred_result = test_data.copy()\n",
    "    pred = []\n",
    "    for index_i,_ in pred_result.iterrows():\n",
    "        pred_i = predict(tree,pred_result.loc[index_i,:])\n",
    "        pred.append(pred_i)        \n",
    "    pred_result['bought'] = pred\n",
    "    pred_result['bought'] = pred_result['bought'].astype(int)\n",
    "    return pred_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number\n",
       "0      M    +10              1\n",
       "1      F    -10              2\n",
       "2      F    +10              2\n",
       "3      M    -10              1\n",
       "4      M    +10              1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建测试数据\n",
    "test_data = pd.DataFrame({\"gender\":['M','F','F','M','M'],\n",
    "                               \"income\":['+10','-10','+10','-10','+10'],\n",
    "                               \"family_number\":[1,2,2,1,1]}) \n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      M    +10              1       0\n",
       "1      F    -10              2       1\n",
       "2      F    +10              2       1\n",
       "3      M    -10              1       1\n",
       "4      M    +10              1       0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 运行get_dt_predict对测试数据进行预测\n",
    "get_dt_predict(tree_model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaddata\n",
    "from sklearn.datasets import load_boston\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston()\n",
    "x,y=dataset['data'],dataset['target']\n",
    "X_rm = x[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2281172cc18>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_rm,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "### Assume that the target funciton is a linear function\n",
    "$$ y = k*rm + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(rm, k, b):\n",
    "    return k * rm + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define mean absolute loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MSE = \\sum_{i = 1}^n (y_i - \\hat{y_i})^2 \\tag1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MAE = \\sum_{i = 1}^n |y_i - \\hat{y_i}| \\tag2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function \n",
    "def loss(y,y_hat):\n",
    "    return sum(np.abs(y_i - y_hat_i) for y_i, y_hat_i in zip(list(y),list(y_hat)))/len(list(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define partial derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{loss}}{\\partial{k}} = -\\frac{1}{n}\\sum_{i = 1}^n x_i \\tag1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{loss}}{\\partial{b}} = -1 \\tag2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_derivative_k(x): \n",
    "    gradient = -np.mean(np.abs(list(x)))\n",
    "    return gradient\n",
    "\n",
    "def partial_derivative_b():\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.284634387351779"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_derivative_k(X_rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用MAE并不总能收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 309.92587303827577, parameters k is -32.40195224525473 and b is -83.75864341630694\n",
      "Iteration 1, the loss is 309.88537640889336, parameters k is -32.395667610867385 and b is -83.75764341630693\n",
      "Iteration 2, the loss is 309.84487977951034, parameters k is -32.389382976480036 and b is -83.75664341630693\n",
      "Iteration 3, the loss is 309.8043831501276, parameters k is -32.38309834209269 and b is -83.75564341630692\n",
      "Iteration 4, the loss is 309.7638865207453, parameters k is -32.37681370770534 and b is -83.75464341630692\n",
      "Iteration 5, the loss is 309.7233898913625, parameters k is -32.37052907331799 and b is -83.75364341630691\n",
      "Iteration 6, the loss is 309.68289326197953, parameters k is -32.36424443893064 and b is -83.75264341630691\n",
      "Iteration 7, the loss is 309.642396632597, parameters k is -32.35795980454329 and b is -83.7516434163069\n",
      "Iteration 8, the loss is 309.6019000032143, parameters k is -32.35167517015594 and b is -83.7506434163069\n",
      "Iteration 9, the loss is 309.56140337383187, parameters k is -32.345390535768594 and b is -83.7496434163069\n",
      "Iteration 10, the loss is 309.520906744449, parameters k is -32.339105901381245 and b is -83.74864341630689\n",
      "Iteration 11, the loss is 309.4804101150661, parameters k is -32.332821266993896 and b is -83.74764341630689\n",
      "Iteration 12, the loss is 309.4399134856836, parameters k is -32.32653663260655 and b is -83.74664341630688\n",
      "Iteration 13, the loss is 309.39941685630083, parameters k is -32.3202519982192 and b is -83.74564341630688\n",
      "Iteration 14, the loss is 309.35892022691826, parameters k is -32.31396736383185 and b is -83.74464341630687\n",
      "Iteration 15, the loss is 309.3184235975356, parameters k is -32.3076827294445 and b is -83.74364341630687\n",
      "Iteration 16, the loss is 309.2779269681531, parameters k is -32.30139809505715 and b is -83.74264341630686\n",
      "Iteration 17, the loss is 309.23743033877037, parameters k is -32.2951134606698 and b is -83.74164341630686\n",
      "Iteration 18, the loss is 309.19693370938757, parameters k is -32.288828826282455 and b is -83.74064341630685\n",
      "Iteration 19, the loss is 309.1564370800049, parameters k is -32.282544191895106 and b is -83.73964341630685\n",
      "Iteration 20, the loss is 309.1159404506219, parameters k is -32.27625955750776 and b is -83.73864341630684\n",
      "Iteration 21, the loss is 309.07544382123973, parameters k is -32.26997492312041 and b is -83.73764341630684\n",
      "Iteration 22, the loss is 309.03494719185693, parameters k is -32.26369028873306 and b is -83.73664341630683\n",
      "Iteration 23, the loss is 308.9944505624741, parameters k is -32.25740565434571 and b is -83.73564341630683\n",
      "Iteration 24, the loss is 308.9539539330917, parameters k is -32.25112101995836 and b is -83.73464341630682\n",
      "Iteration 25, the loss is 308.91345730370887, parameters k is -32.24483638557101 and b is -83.73364341630682\n",
      "Iteration 26, the loss is 308.8729606743262, parameters k is -32.238551751183664 and b is -83.73264341630681\n",
      "Iteration 27, the loss is 308.8324640449431, parameters k is -32.232267116796315 and b is -83.73164341630681\n",
      "Iteration 28, the loss is 308.7919674155613, parameters k is -32.225982482408966 and b is -83.7306434163068\n",
      "Iteration 29, the loss is 308.7514707861782, parameters k is -32.21969784802162 and b is -83.7296434163068\n",
      "Iteration 30, the loss is 308.71097415679543, parameters k is -32.21341321363427 and b is -83.7286434163068\n",
      "Iteration 31, the loss is 308.670477527413, parameters k is -32.20712857924692 and b is -83.72764341630679\n",
      "Iteration 32, the loss is 308.62998089803034, parameters k is -32.20084394485957 and b is -83.72664341630679\n",
      "Iteration 33, the loss is 308.5894842686478, parameters k is -32.19455931047222 and b is -83.72564341630678\n",
      "Iteration 34, the loss is 308.5489876392644, parameters k is -32.18827467608487 and b is -83.72464341630678\n",
      "Iteration 35, the loss is 308.5084910098822, parameters k is -32.181990041697524 and b is -83.72364341630677\n",
      "Iteration 36, the loss is 308.4679943804992, parameters k is -32.175705407310176 and b is -83.72264341630677\n",
      "Iteration 37, the loss is 308.42749775111685, parameters k is -32.16942077292283 and b is -83.72164341630676\n",
      "Iteration 38, the loss is 308.38700112173433, parameters k is -32.16313613853548 and b is -83.72064341630676\n",
      "Iteration 39, the loss is 308.3465044923513, parameters k is -32.15685150414813 and b is -83.71964341630675\n",
      "Iteration 40, the loss is 308.30600786296867, parameters k is -32.15056686976078 and b is -83.71864341630675\n",
      "Iteration 41, the loss is 308.2655112335862, parameters k is -32.14428223537343 and b is -83.71764341630674\n",
      "Iteration 42, the loss is 308.2250146042037, parameters k is -32.13799760098608 and b is -83.71664341630674\n",
      "Iteration 43, the loss is 308.18451797482106, parameters k is -32.131712966598734 and b is -83.71564341630673\n",
      "Iteration 44, the loss is 308.1440213454384, parameters k is -32.125428332211385 and b is -83.71464341630673\n",
      "Iteration 45, the loss is 308.1035247160556, parameters k is -32.119143697824036 and b is -83.71364341630672\n",
      "Iteration 46, the loss is 308.0630280866728, parameters k is -32.11285906343669 and b is -83.71264341630672\n",
      "Iteration 47, the loss is 308.02253145729, parameters k is -32.10657442904934 and b is -83.71164341630671\n",
      "Iteration 48, the loss is 307.98203482790717, parameters k is -32.10028979466199 and b is -83.71064341630671\n",
      "Iteration 49, the loss is 307.9415381985248, parameters k is -32.09400516027464 and b is -83.7096434163067\n",
      "Iteration 50, the loss is 307.901041569142, parameters k is -32.08772052588729 and b is -83.7086434163067\n",
      "Iteration 51, the loss is 307.8605449397597, parameters k is -32.08143589149994 and b is -83.7076434163067\n",
      "Iteration 52, the loss is 307.82004831037654, parameters k is -32.075151257112594 and b is -83.70664341630669\n",
      "Iteration 53, the loss is 307.7795516809944, parameters k is -32.068866622725245 and b is -83.70564341630669\n",
      "Iteration 54, the loss is 307.73905505161133, parameters k is -32.0625819883379 and b is -83.70464341630668\n",
      "Iteration 55, the loss is 307.69855842222876, parameters k is -32.05629735395055 and b is -83.70364341630668\n",
      "Iteration 56, the loss is 307.6580617928461, parameters k is -32.0500127195632 and b is -83.70264341630667\n",
      "Iteration 57, the loss is 307.6175651634636, parameters k is -32.04372808517585 and b is -83.70164341630667\n",
      "Iteration 58, the loss is 307.577068534081, parameters k is -32.0374434507885 and b is -83.70064341630666\n",
      "Iteration 59, the loss is 307.53657190469846, parameters k is -32.03115881640115 and b is -83.69964341630666\n",
      "Iteration 60, the loss is 307.49607527531543, parameters k is -32.024874182013804 and b is -83.69864341630665\n",
      "Iteration 61, the loss is 307.4555786459328, parameters k is -32.018589547626455 and b is -83.69764341630665\n",
      "Iteration 62, the loss is 307.4150820165506, parameters k is -32.012304913239106 and b is -83.69664341630664\n",
      "Iteration 63, the loss is 307.3745853871673, parameters k is -32.00602027885176 and b is -83.69564341630664\n",
      "Iteration 64, the loss is 307.3340887577849, parameters k is -31.999735644464405 and b is -83.69464341630663\n",
      "Iteration 65, the loss is 307.2935921284016, parameters k is -31.993451010077052 and b is -83.69364341630663\n",
      "Iteration 66, the loss is 307.25309549901937, parameters k is -31.9871663756897 and b is -83.69264341630662\n",
      "Iteration 67, the loss is 307.21259886963645, parameters k is -31.980881741302348 and b is -83.69164341630662\n",
      "Iteration 68, the loss is 307.1721022402537, parameters k is -31.974597106914995 and b is -83.69064341630661\n",
      "Iteration 69, the loss is 307.1316056108712, parameters k is -31.968312472527643 and b is -83.68964341630661\n",
      "Iteration 70, the loss is 307.0911089814887, parameters k is -31.96202783814029 and b is -83.6886434163066\n",
      "Iteration 71, the loss is 307.05061235210593, parameters k is -31.955743203752938 and b is -83.6876434163066\n",
      "Iteration 72, the loss is 307.0101157227233, parameters k is -31.949458569365586 and b is -83.6866434163066\n",
      "Iteration 73, the loss is 306.96961909334027, parameters k is -31.943173934978233 and b is -83.68564341630659\n",
      "Iteration 74, the loss is 306.92912246395747, parameters k is -31.93688930059088 and b is -83.68464341630659\n",
      "Iteration 75, the loss is 306.8886258345749, parameters k is -31.93060466620353 and b is -83.68364341630658\n",
      "Iteration 76, the loss is 306.8481292051925, parameters k is -31.924320031816176 and b is -83.68264341630658\n",
      "Iteration 77, the loss is 306.80763257581, parameters k is -31.918035397428824 and b is -83.68164341630657\n",
      "Iteration 78, the loss is 306.7671359464272, parameters k is -31.91175076304147 and b is -83.68064341630657\n",
      "Iteration 79, the loss is 306.7266393170446, parameters k is -31.90546612865412 and b is -83.67964341630656\n",
      "Iteration 80, the loss is 306.6861426876617, parameters k is -31.899181494266767 and b is -83.67864341630656\n",
      "Iteration 81, the loss is 306.6456460582786, parameters k is -31.892896859879414 and b is -83.67764341630655\n",
      "Iteration 82, the loss is 306.6051494288966, parameters k is -31.886612225492062 and b is -83.67664341630655\n",
      "Iteration 83, the loss is 306.56465279951357, parameters k is -31.88032759110471 and b is -83.67564341630654\n",
      "Iteration 84, the loss is 306.524156170131, parameters k is -31.874042956717357 and b is -83.67464341630654\n",
      "Iteration 85, the loss is 306.48365954074814, parameters k is -31.867758322330005 and b is -83.67364341630653\n",
      "Iteration 86, the loss is 306.44316291136545, parameters k is -31.861473687942652 and b is -83.67264341630653\n",
      "Iteration 87, the loss is 306.40266628198265, parameters k is -31.8551890535553 and b is -83.67164341630652\n",
      "Iteration 88, the loss is 306.3621696525998, parameters k is -31.848904419167948 and b is -83.67064341630652\n",
      "Iteration 89, the loss is 306.3216730232176, parameters k is -31.842619784780595 and b is -83.66964341630651\n",
      "Iteration 90, the loss is 306.2811763938349, parameters k is -31.836335150393243 and b is -83.66864341630651\n",
      "Iteration 91, the loss is 306.240679764452, parameters k is -31.83005051600589 and b is -83.6676434163065\n",
      "Iteration 92, the loss is 306.2001831350694, parameters k is -31.823765881618538 and b is -83.6666434163065\n",
      "Iteration 93, the loss is 306.15968650568686, parameters k is -31.817481247231186 and b is -83.6656434163065\n",
      "Iteration 94, the loss is 306.119189876304, parameters k is -31.811196612843833 and b is -83.66464341630649\n",
      "Iteration 95, the loss is 306.07869324692126, parameters k is -31.80491197845648 and b is -83.66364341630648\n",
      "Iteration 96, the loss is 306.03819661753863, parameters k is -31.79862734406913 and b is -83.66264341630648\n",
      "Iteration 97, the loss is 305.9976999881559, parameters k is -31.792342709681776 and b is -83.66164341630648\n",
      "Iteration 98, the loss is 305.9572033587734, parameters k is -31.786058075294424 and b is -83.66064341630647\n",
      "Iteration 99, the loss is 305.9167067293905, parameters k is -31.77977344090707 and b is -83.65964341630647\n",
      "Iteration 100, the loss is 305.8762101000077, parameters k is -31.77348880651972 and b is -83.65864341630646\n",
      "Iteration 101, the loss is 305.83571347062536, parameters k is -31.767204172132367 and b is -83.65764341630646\n",
      "Iteration 102, the loss is 305.79521684124234, parameters k is -31.760919537745014 and b is -83.65664341630645\n",
      "Iteration 103, the loss is 305.7547202118596, parameters k is -31.754634903357662 and b is -83.65564341630645\n",
      "Iteration 104, the loss is 305.7142235824773, parameters k is -31.74835026897031 and b is -83.65464341630644\n",
      "Iteration 105, the loss is 305.6737269530946, parameters k is -31.742065634582957 and b is -83.65364341630644\n",
      "Iteration 106, the loss is 305.633230323712, parameters k is -31.735781000195605 and b is -83.65264341630643\n",
      "Iteration 107, the loss is 305.5927336943288, parameters k is -31.729496365808252 and b is -83.65164341630643\n",
      "Iteration 108, the loss is 305.5522370649466, parameters k is -31.7232117314209 and b is -83.65064341630642\n",
      "Iteration 109, the loss is 305.5117404355633, parameters k is -31.716927097033548 and b is -83.64964341630642\n",
      "Iteration 110, the loss is 305.47124380618123, parameters k is -31.710642462646195 and b is -83.64864341630641\n",
      "Iteration 111, the loss is 305.4307471767982, parameters k is -31.704357828258843 and b is -83.64764341630641\n",
      "Iteration 112, the loss is 305.39025054741535, parameters k is -31.69807319387149 and b is -83.6466434163064\n",
      "Iteration 113, the loss is 305.34975391803266, parameters k is -31.691788559484138 and b is -83.6456434163064\n",
      "Iteration 114, the loss is 305.3092572886504, parameters k is -31.685503925096786 and b is -83.6446434163064\n",
      "Iteration 115, the loss is 305.2687606592675, parameters k is -31.679219290709433 and b is -83.64364341630639\n",
      "Iteration 116, the loss is 305.2282640298848, parameters k is -31.67293465632208 and b is -83.64264341630638\n",
      "Iteration 117, the loss is 305.18776740050197, parameters k is -31.66665002193473 and b is -83.64164341630638\n",
      "Iteration 118, the loss is 305.14727077111917, parameters k is -31.660365387547376 and b is -83.64064341630638\n",
      "Iteration 119, the loss is 305.1067741417366, parameters k is -31.654080753160024 and b is -83.63964341630637\n",
      "Iteration 120, the loss is 305.0662775123539, parameters k is -31.64779611877267 and b is -83.63864341630637\n",
      "Iteration 121, the loss is 305.0257808829714, parameters k is -31.64151148438532 and b is -83.63764341630636\n",
      "Iteration 122, the loss is 304.9852842535883, parameters k is -31.635226849997967 and b is -83.63664341630636\n",
      "Iteration 123, the loss is 304.94478762420596, parameters k is -31.628942215610614 and b is -83.63564341630635\n",
      "Iteration 124, the loss is 304.90429099482344, parameters k is -31.62265758122326 and b is -83.63464341630635\n",
      "Iteration 125, the loss is 304.8637943654406, parameters k is -31.61637294683591 and b is -83.63364341630634\n",
      "Iteration 126, the loss is 304.82329773605795, parameters k is -31.610088312448557 and b is -83.63264341630634\n",
      "Iteration 127, the loss is 304.782801106675, parameters k is -31.603803678061205 and b is -83.63164341630633\n",
      "Iteration 128, the loss is 304.74230447729263, parameters k is -31.597519043673852 and b is -83.63064341630633\n",
      "Iteration 129, the loss is 304.70180784790966, parameters k is -31.5912344092865 and b is -83.62964341630632\n",
      "Iteration 130, the loss is 304.66131121852646, parameters k is -31.584949774899147 and b is -83.62864341630632\n",
      "Iteration 131, the loss is 304.62081458914435, parameters k is -31.578665140511795 and b is -83.62764341630631\n",
      "Iteration 132, the loss is 304.58031795976194, parameters k is -31.572380506124443 and b is -83.62664341630631\n",
      "Iteration 133, the loss is 304.53982133037897, parameters k is -31.56609587173709 and b is -83.6256434163063\n",
      "Iteration 134, the loss is 304.4993247009961, parameters k is -31.559811237349738 and b is -83.6246434163063\n",
      "Iteration 135, the loss is 304.45882807161325, parameters k is -31.553526602962386 and b is -83.6236434163063\n",
      "Iteration 136, the loss is 304.4183314422309, parameters k is -31.547241968575033 and b is -83.62264341630629\n",
      "Iteration 137, the loss is 304.3778348128483, parameters k is -31.54095733418768 and b is -83.62164341630628\n",
      "Iteration 138, the loss is 304.33733818346576, parameters k is -31.53467269980033 and b is -83.62064341630628\n",
      "Iteration 139, the loss is 304.29684155408256, parameters k is -31.528388065412976 and b is -83.61964341630627\n",
      "Iteration 140, the loss is 304.2563449246999, parameters k is -31.522103431025624 and b is -83.61864341630627\n",
      "Iteration 141, the loss is 304.2158482953173, parameters k is -31.51581879663827 and b is -83.61764341630627\n",
      "Iteration 142, the loss is 304.1753516659345, parameters k is -31.50953416225092 and b is -83.61664341630626\n",
      "Iteration 143, the loss is 304.1348550365515, parameters k is -31.503249527863566 and b is -83.61564341630626\n",
      "Iteration 144, the loss is 304.0943584071691, parameters k is -31.496964893476214 and b is -83.61464341630625\n",
      "Iteration 145, the loss is 304.0538617777867, parameters k is -31.49068025908886 and b is -83.61364341630625\n",
      "Iteration 146, the loss is 304.0133651484036, parameters k is -31.48439562470151 and b is -83.61264341630624\n",
      "Iteration 147, the loss is 303.9728685190213, parameters k is -31.478110990314157 and b is -83.61164341630624\n",
      "Iteration 148, the loss is 303.93237188963843, parameters k is -31.471826355926805 and b is -83.61064341630623\n",
      "Iteration 149, the loss is 303.891875260256, parameters k is -31.465541721539452 and b is -83.60964341630623\n",
      "Iteration 150, the loss is 303.8513786308733, parameters k is -31.4592570871521 and b is -83.60864341630622\n",
      "Iteration 151, the loss is 303.8108820014903, parameters k is -31.452972452764747 and b is -83.60764341630622\n",
      "Iteration 152, the loss is 303.77038537210774, parameters k is -31.446687818377395 and b is -83.60664341630621\n",
      "Iteration 153, the loss is 303.72988874272517, parameters k is -31.440403183990043 and b is -83.60564341630621\n",
      "Iteration 154, the loss is 303.68939211334197, parameters k is -31.43411854960269 and b is -83.6046434163062\n",
      "Iteration 155, the loss is 303.6488954839598, parameters k is -31.427833915215338 and b is -83.6036434163062\n",
      "Iteration 156, the loss is 303.608398854577, parameters k is -31.421549280827985 and b is -83.6026434163062\n",
      "Iteration 157, the loss is 303.56790222519436, parameters k is -31.415264646440633 and b is -83.60164341630619\n",
      "Iteration 158, the loss is 303.5274055958118, parameters k is -31.40898001205328 and b is -83.60064341630618\n",
      "Iteration 159, the loss is 303.48690896642887, parameters k is -31.40269537766593 and b is -83.59964341630618\n",
      "Iteration 160, the loss is 303.4464123370464, parameters k is -31.396410743278576 and b is -83.59864341630617\n",
      "Iteration 161, the loss is 303.4059157076633, parameters k is -31.390126108891224 and b is -83.59764341630617\n",
      "Iteration 162, the loss is 303.36541907828064, parameters k is -31.38384147450387 and b is -83.59664341630616\n",
      "Iteration 163, the loss is 303.3249224488981, parameters k is -31.37755684011652 and b is -83.59564341630616\n",
      "Iteration 164, the loss is 303.28442581951543, parameters k is -31.371272205729166 and b is -83.59464341630616\n",
      "Iteration 165, the loss is 303.2439291901326, parameters k is -31.364987571341814 and b is -83.59364341630615\n",
      "Iteration 166, the loss is 303.2034325607499, parameters k is -31.35870293695446 and b is -83.59264341630615\n",
      "Iteration 167, the loss is 303.16293593136743, parameters k is -31.35241830256711 and b is -83.59164341630614\n",
      "Iteration 168, the loss is 303.12243930198497, parameters k is -31.346133668179757 and b is -83.59064341630614\n",
      "Iteration 169, the loss is 303.08194267260194, parameters k is -31.339849033792404 and b is -83.58964341630613\n",
      "Iteration 170, the loss is 303.0414460432188, parameters k is -31.333564399405052 and b is -83.58864341630613\n",
      "Iteration 171, the loss is 303.00094941383674, parameters k is -31.3272797650177 and b is -83.58764341630612\n",
      "Iteration 172, the loss is 302.9604527844542, parameters k is -31.320995130630347 and b is -83.58664341630612\n",
      "Iteration 173, the loss is 302.9199561550712, parameters k is -31.314710496242995 and b is -83.58564341630611\n",
      "Iteration 174, the loss is 302.87945952568845, parameters k is -31.308425861855643 and b is -83.58464341630611\n",
      "Iteration 175, the loss is 302.8389628963056, parameters k is -31.30214122746829 and b is -83.5836434163061\n",
      "Iteration 176, the loss is 302.79846626692284, parameters k is -31.295856593080938 and b is -83.5826434163061\n",
      "Iteration 177, the loss is 302.75796963754044, parameters k is -31.289571958693585 and b is -83.5816434163061\n",
      "Iteration 178, the loss is 302.7174730081574, parameters k is -31.283287324306233 and b is -83.58064341630609\n",
      "Iteration 179, the loss is 302.67697637877495, parameters k is -31.27700268991888 and b is -83.57964341630608\n",
      "Iteration 180, the loss is 302.63647974939227, parameters k is -31.27071805553153 and b is -83.57864341630608\n",
      "Iteration 181, the loss is 302.5959831200095, parameters k is -31.264433421144176 and b is -83.57764341630607\n",
      "Iteration 182, the loss is 302.555486490627, parameters k is -31.258148786756824 and b is -83.57664341630607\n",
      "Iteration 183, the loss is 302.5149898612438, parameters k is -31.25186415236947 and b is -83.57564341630606\n",
      "Iteration 184, the loss is 302.47449323186174, parameters k is -31.24557951798212 and b is -83.57464341630606\n",
      "Iteration 185, the loss is 302.43399660247894, parameters k is -31.239294883594766 and b is -83.57364341630606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 186, the loss is 302.39349997309637, parameters k is -31.233010249207414 and b is -83.57264341630605\n",
      "Iteration 187, the loss is 302.3530033437136, parameters k is -31.22672561482006 and b is -83.57164341630605\n",
      "Iteration 188, the loss is 302.31250671433105, parameters k is -31.22044098043271 and b is -83.57064341630604\n",
      "Iteration 189, the loss is 302.27201008494825, parameters k is -31.214156346045357 and b is -83.56964341630604\n",
      "Iteration 190, the loss is 302.2315134555654, parameters k is -31.207871711658004 and b is -83.56864341630603\n",
      "Iteration 191, the loss is 302.1910168261824, parameters k is -31.201587077270652 and b is -83.56764341630603\n",
      "Iteration 192, the loss is 302.1505201968001, parameters k is -31.1953024428833 and b is -83.56664341630602\n",
      "Iteration 193, the loss is 302.11002356741744, parameters k is -31.189017808495947 and b is -83.56564341630602\n",
      "Iteration 194, the loss is 302.06952693803464, parameters k is -31.182733174108595 and b is -83.56464341630601\n",
      "Iteration 195, the loss is 302.0290303086523, parameters k is -31.176448539721243 and b is -83.56364341630601\n",
      "Iteration 196, the loss is 301.9885336792692, parameters k is -31.17016390533389 and b is -83.562643416306\n",
      "Iteration 197, the loss is 301.9480370498863, parameters k is -31.163879270946538 and b is -83.561643416306\n",
      "Iteration 198, the loss is 301.90754042050395, parameters k is -31.157594636559185 and b is -83.560643416306\n",
      "Iteration 199, the loss is 301.8670437911212, parameters k is -31.151310002171833 and b is -83.55964341630599\n",
      "Iteration 200, the loss is 301.8265471617387, parameters k is -31.14502536778448 and b is -83.55864341630598\n",
      "Iteration 201, the loss is 301.7860505323557, parameters k is -31.13874073339713 and b is -83.55764341630598\n",
      "Iteration 202, the loss is 301.7455539029729, parameters k is -31.132456099009776 and b is -83.55664341630597\n",
      "Iteration 203, the loss is 301.7050572735903, parameters k is -31.126171464622423 and b is -83.55564341630597\n",
      "Iteration 204, the loss is 301.66456064420754, parameters k is -31.11988683023507 and b is -83.55464341630596\n",
      "Iteration 205, the loss is 301.6240640148251, parameters k is -31.11360219584772 and b is -83.55364341630596\n",
      "Iteration 206, the loss is 301.5835673854422, parameters k is -31.107317561460366 and b is -83.55264341630595\n",
      "Iteration 207, the loss is 301.5430707560599, parameters k is -31.101032927073014 and b is -83.55164341630595\n",
      "Iteration 208, the loss is 301.50257412667673, parameters k is -31.09474829268566 and b is -83.55064341630595\n",
      "Iteration 209, the loss is 301.4620774972941, parameters k is -31.08846365829831 and b is -83.54964341630594\n",
      "Iteration 210, the loss is 301.4215808679113, parameters k is -31.082179023910957 and b is -83.54864341630594\n",
      "Iteration 211, the loss is 301.3810842385286, parameters k is -31.075894389523604 and b is -83.54764341630593\n",
      "Iteration 212, the loss is 301.3405876091459, parameters k is -31.069609755136252 and b is -83.54664341630593\n",
      "Iteration 213, the loss is 301.3000909797635, parameters k is -31.0633251207489 and b is -83.54564341630592\n",
      "Iteration 214, the loss is 301.25959435038067, parameters k is -31.057040486361547 and b is -83.54464341630592\n",
      "Iteration 215, the loss is 301.21909772099804, parameters k is -31.050755851974195 and b is -83.54364341630591\n",
      "Iteration 216, the loss is 301.1786010916155, parameters k is -31.044471217586842 and b is -83.5426434163059\n",
      "Iteration 217, the loss is 301.1381044622327, parameters k is -31.03818658319949 and b is -83.5416434163059\n",
      "Iteration 218, the loss is 301.09760783285, parameters k is -31.031901948812138 and b is -83.5406434163059\n",
      "Iteration 219, the loss is 301.05711120346734, parameters k is -31.025617314424785 and b is -83.53964341630589\n",
      "Iteration 220, the loss is 301.0166145740846, parameters k is -31.019332680037433 and b is -83.53864341630589\n",
      "Iteration 221, the loss is 300.976117944702, parameters k is -31.01304804565008 and b is -83.53764341630588\n",
      "Iteration 222, the loss is 300.9356213153193, parameters k is -31.006763411262728 and b is -83.53664341630588\n",
      "Iteration 223, the loss is 300.8951246859363, parameters k is -31.000478776875376 and b is -83.53564341630587\n",
      "Iteration 224, the loss is 300.85462805655357, parameters k is -30.994194142488023 and b is -83.53464341630587\n",
      "Iteration 225, the loss is 300.81413142717145, parameters k is -30.98790950810067 and b is -83.53364341630586\n",
      "Iteration 226, the loss is 300.7736347977886, parameters k is -30.98162487371332 and b is -83.53264341630586\n",
      "Iteration 227, the loss is 300.7331381684057, parameters k is -30.975340239325966 and b is -83.53164341630585\n",
      "Iteration 228, the loss is 300.6926415390235, parameters k is -30.969055604938614 and b is -83.53064341630585\n",
      "Iteration 229, the loss is 300.6521449096403, parameters k is -30.96277097055126 and b is -83.52964341630584\n",
      "Iteration 230, the loss is 300.611648280258, parameters k is -30.95648633616391 and b is -83.52864341630584\n",
      "Iteration 231, the loss is 300.5711516508753, parameters k is -30.950201701776557 and b is -83.52764341630584\n",
      "Iteration 232, the loss is 300.5306550214922, parameters k is -30.943917067389204 and b is -83.52664341630583\n",
      "Iteration 233, the loss is 300.49015839210983, parameters k is -30.937632433001852 and b is -83.52564341630583\n",
      "Iteration 234, the loss is 300.44966176272686, parameters k is -30.9313477986145 and b is -83.52464341630582\n",
      "Iteration 235, the loss is 300.4091651333441, parameters k is -30.925063164227147 and b is -83.52364341630582\n",
      "Iteration 236, the loss is 300.36866850396154, parameters k is -30.918778529839795 and b is -83.52264341630581\n",
      "Iteration 237, the loss is 300.32817187457897, parameters k is -30.912493895452442 and b is -83.5216434163058\n",
      "Iteration 238, the loss is 300.2876752451963, parameters k is -30.90620926106509 and b is -83.5206434163058\n",
      "Iteration 239, the loss is 300.24717861581314, parameters k is -30.899924626677738 and b is -83.5196434163058\n",
      "Iteration 240, the loss is 300.20668198643085, parameters k is -30.893639992290385 and b is -83.51864341630579\n",
      "Iteration 241, the loss is 300.1661853570481, parameters k is -30.887355357903033 and b is -83.51764341630579\n",
      "Iteration 242, the loss is 300.1256887276651, parameters k is -30.88107072351568 and b is -83.51664341630578\n",
      "Iteration 243, the loss is 300.08519209828285, parameters k is -30.874786089128328 and b is -83.51564341630578\n",
      "Iteration 244, the loss is 300.0446954689001, parameters k is -30.868501454740976 and b is -83.51464341630577\n",
      "Iteration 245, the loss is 300.00419883951747, parameters k is -30.862216820353623 and b is -83.51364341630577\n",
      "Iteration 246, the loss is 299.96370221013444, parameters k is -30.85593218596627 and b is -83.51264341630576\n",
      "Iteration 247, the loss is 299.9232055807518, parameters k is -30.84964755157892 and b is -83.51164341630576\n",
      "Iteration 248, the loss is 299.8827089513692, parameters k is -30.843362917191566 and b is -83.51064341630575\n",
      "Iteration 249, the loss is 299.8422123219866, parameters k is -30.837078282804214 and b is -83.50964341630575\n",
      "Iteration 250, the loss is 299.8017156926042, parameters k is -30.83079364841686 and b is -83.50864341630574\n",
      "Iteration 251, the loss is 299.7612190632209, parameters k is -30.82450901402951 and b is -83.50764341630574\n",
      "Iteration 252, the loss is 299.72072243383843, parameters k is -30.818224379642157 and b is -83.50664341630574\n",
      "Iteration 253, the loss is 299.6802258044557, parameters k is -30.811939745254804 and b is -83.50564341630573\n",
      "Iteration 254, the loss is 299.63972917507294, parameters k is -30.805655110867452 and b is -83.50464341630573\n",
      "Iteration 255, the loss is 299.5992325456903, parameters k is -30.7993704764801 and b is -83.50364341630572\n",
      "Iteration 256, the loss is 299.5587359163076, parameters k is -30.793085842092747 and b is -83.50264341630572\n",
      "Iteration 257, the loss is 299.51823928692517, parameters k is -30.786801207705395 and b is -83.50164341630571\n",
      "Iteration 258, the loss is 299.4777426575425, parameters k is -30.780516573318042 and b is -83.5006434163057\n",
      "Iteration 259, the loss is 299.43724602815934, parameters k is -30.77423193893069 and b is -83.4996434163057\n",
      "Iteration 260, the loss is 299.39674939877716, parameters k is -30.767947304543338 and b is -83.4986434163057\n",
      "Iteration 261, the loss is 299.35625276939436, parameters k is -30.761662670155985 and b is -83.49764341630569\n",
      "Iteration 262, the loss is 299.3157561400117, parameters k is -30.755378035768633 and b is -83.49664341630569\n",
      "Iteration 263, the loss is 299.2752595106287, parameters k is -30.74909340138128 and b is -83.49564341630568\n",
      "Iteration 264, the loss is 299.2347628812459, parameters k is -30.742808766993928 and b is -83.49464341630568\n",
      "Iteration 265, the loss is 299.1942662518636, parameters k is -30.736524132606576 and b is -83.49364341630567\n",
      "Iteration 266, the loss is 299.1537696224807, parameters k is -30.730239498219223 and b is -83.49264341630567\n",
      "Iteration 267, the loss is 299.1132729930981, parameters k is -30.72395486383187 and b is -83.49164341630566\n",
      "Iteration 268, the loss is 299.0727763637154, parameters k is -30.71767022944452 and b is -83.49064341630566\n",
      "Iteration 269, the loss is 299.0322797343328, parameters k is -30.711385595057166 and b is -83.48964341630565\n",
      "Iteration 270, the loss is 298.9917831049499, parameters k is -30.705100960669814 and b is -83.48864341630565\n",
      "Iteration 271, the loss is 298.951286475567, parameters k is -30.69881632628246 and b is -83.48764341630564\n",
      "Iteration 272, the loss is 298.91078984618485, parameters k is -30.69253169189511 and b is -83.48664341630564\n",
      "Iteration 273, the loss is 298.8702932168015, parameters k is -30.686247057507757 and b is -83.48564341630563\n",
      "Iteration 274, the loss is 298.8297965874189, parameters k is -30.679962423120404 and b is -83.48464341630563\n",
      "Iteration 275, the loss is 298.78929995803634, parameters k is -30.673677788733052 and b is -83.48364341630563\n",
      "Iteration 276, the loss is 298.74880332865376, parameters k is -30.6673931543457 and b is -83.48264341630562\n",
      "Iteration 277, the loss is 298.7083066992713, parameters k is -30.661108519958347 and b is -83.48164341630562\n",
      "Iteration 278, the loss is 298.6678100698881, parameters k is -30.654823885570995 and b is -83.48064341630561\n",
      "Iteration 279, the loss is 298.62731344050576, parameters k is -30.648539251183642 and b is -83.4796434163056\n",
      "Iteration 280, the loss is 298.5868168111232, parameters k is -30.64225461679629 and b is -83.4786434163056\n",
      "Iteration 281, the loss is 298.54632018174044, parameters k is -30.635969982408938 and b is -83.4776434163056\n",
      "Iteration 282, the loss is 298.50582355235815, parameters k is -30.629685348021585 and b is -83.47664341630559\n",
      "Iteration 283, the loss is 298.4653269229747, parameters k is -30.623400713634233 and b is -83.47564341630559\n",
      "Iteration 284, the loss is 298.42483029359204, parameters k is -30.61711607924688 and b is -83.47464341630558\n",
      "Iteration 285, the loss is 298.3843336642091, parameters k is -30.610831444859528 and b is -83.47364341630558\n",
      "Iteration 286, the loss is 298.34383703482655, parameters k is -30.604546810472176 and b is -83.47264341630557\n",
      "Iteration 287, the loss is 298.30334040544426, parameters k is -30.598262176084823 and b is -83.47164341630557\n",
      "Iteration 288, the loss is 298.26284377606163, parameters k is -30.59197754169747 and b is -83.47064341630556\n",
      "Iteration 289, the loss is 298.22234714667877, parameters k is -30.58569290731012 and b is -83.46964341630556\n",
      "Iteration 290, the loss is 298.1818505172962, parameters k is -30.579408272922766 and b is -83.46864341630555\n",
      "Iteration 291, the loss is 298.1413538879133, parameters k is -30.573123638535414 and b is -83.46764341630555\n",
      "Iteration 292, the loss is 298.1008572585309, parameters k is -30.56683900414806 and b is -83.46664341630554\n",
      "Iteration 293, the loss is 298.06036062914785, parameters k is -30.56055436976071 and b is -83.46564341630554\n",
      "Iteration 294, the loss is 298.0198639997654, parameters k is -30.554269735373357 and b is -83.46464341630553\n",
      "Iteration 295, the loss is 297.9793673703825, parameters k is -30.547985100986004 and b is -83.46364341630553\n",
      "Iteration 296, the loss is 297.9388707409999, parameters k is -30.541700466598652 and b is -83.46264341630553\n",
      "Iteration 297, the loss is 297.8983741116172, parameters k is -30.5354158322113 and b is -83.46164341630552\n",
      "Iteration 298, the loss is 297.8578774822344, parameters k is -30.529131197823947 and b is -83.46064341630552\n",
      "Iteration 299, the loss is 297.8173808528518, parameters k is -30.522846563436595 and b is -83.45964341630551\n",
      "Iteration 300, the loss is 297.77688422346904, parameters k is -30.516561929049242 and b is -83.4586434163055\n",
      "Iteration 301, the loss is 297.73638759408647, parameters k is -30.51027729466189 and b is -83.4576434163055\n",
      "Iteration 302, the loss is 297.6958909647037, parameters k is -30.503992660274537 and b is -83.4566434163055\n",
      "Iteration 303, the loss is 297.6553943353212, parameters k is -30.497708025887185 and b is -83.45564341630549\n",
      "Iteration 304, the loss is 297.61489770593806, parameters k is -30.491423391499833 and b is -83.45464341630549\n",
      "Iteration 305, the loss is 297.57440107655515, parameters k is -30.48513875711248 and b is -83.45364341630548\n",
      "Iteration 306, the loss is 297.5339044471727, parameters k is -30.478854122725128 and b is -83.45264341630548\n",
      "Iteration 307, the loss is 297.4934078177903, parameters k is -30.472569488337776 and b is -83.45164341630547\n",
      "Iteration 308, the loss is 297.45291118840777, parameters k is -30.466284853950423 and b is -83.45064341630547\n",
      "Iteration 309, the loss is 297.412414559025, parameters k is -30.46000021956307 and b is -83.44964341630546\n",
      "Iteration 310, the loss is 297.3719179296424, parameters k is -30.45371558517572 and b is -83.44864341630546\n",
      "Iteration 311, the loss is 297.3314213002598, parameters k is -30.447430950788366 and b is -83.44764341630545\n",
      "Iteration 312, the loss is 297.2909246708768, parameters k is -30.441146316401014 and b is -83.44664341630545\n",
      "Iteration 313, the loss is 297.2504280414942, parameters k is -30.43486168201366 and b is -83.44564341630544\n",
      "Iteration 314, the loss is 297.2099314121115, parameters k is -30.42857704762631 and b is -83.44464341630544\n",
      "Iteration 315, the loss is 297.1694347827285, parameters k is -30.422292413238956 and b is -83.44364341630543\n",
      "Iteration 316, the loss is 297.12893815334587, parameters k is -30.416007778851604 and b is -83.44264341630543\n",
      "Iteration 317, the loss is 297.0884415239632, parameters k is -30.40972314446425 and b is -83.44164341630542\n",
      "Iteration 318, the loss is 297.04794489458084, parameters k is -30.4034385100769 and b is -83.44064341630542\n",
      "Iteration 319, the loss is 297.00744826519815, parameters k is -30.397153875689547 and b is -83.43964341630542\n",
      "Iteration 320, the loss is 296.9669516358151, parameters k is -30.390869241302195 and b is -83.43864341630541\n",
      "Iteration 321, the loss is 296.9264550064328, parameters k is -30.384584606914842 and b is -83.4376434163054\n",
      "Iteration 322, the loss is 296.88595837705003, parameters k is -30.37829997252749 and b is -83.4366434163054\n",
      "Iteration 323, the loss is 296.8454617476669, parameters k is -30.372015338140137 and b is -83.4356434163054\n",
      "Iteration 324, the loss is 296.8049651182847, parameters k is -30.365730703752785 and b is -83.43464341630539\n",
      "Iteration 325, the loss is 296.7644684889018, parameters k is -30.359446069365433 and b is -83.43364341630539\n",
      "Iteration 326, the loss is 296.7239718595189, parameters k is -30.35316143497808 and b is -83.43264341630538\n",
      "Iteration 327, the loss is 296.6834752301363, parameters k is -30.346876800590728 and b is -83.43164341630538\n",
      "Iteration 328, the loss is 296.6429786007535, parameters k is -30.340592166203376 and b is -83.43064341630537\n",
      "Iteration 329, the loss is 296.6024819713712, parameters k is -30.334307531816023 and b is -83.42964341630537\n",
      "Iteration 330, the loss is 296.5619853419885, parameters k is -30.32802289742867 and b is -83.42864341630536\n",
      "Iteration 331, the loss is 296.52148871260596, parameters k is -30.32173826304132 and b is -83.42764341630536\n",
      "Iteration 332, the loss is 296.4809920832231, parameters k is -30.315453628653966 and b is -83.42664341630535\n",
      "Iteration 333, the loss is 296.4404954538401, parameters k is -30.309168994266614 and b is -83.42564341630535\n",
      "Iteration 334, the loss is 296.39999882445755, parameters k is -30.30288435987926 and b is -83.42464341630534\n",
      "Iteration 335, the loss is 296.3595021950754, parameters k is -30.29659972549191 and b is -83.42364341630534\n",
      "Iteration 336, the loss is 296.3190055656925, parameters k is -30.290315091104556 and b is -83.42264341630533\n",
      "Iteration 337, the loss is 296.2785089363098, parameters k is -30.284030456717204 and b is -83.42164341630533\n",
      "Iteration 338, the loss is 296.23801230692675, parameters k is -30.27774582232985 and b is -83.42064341630532\n",
      "Iteration 339, the loss is 296.1975156775443, parameters k is -30.2714611879425 and b is -83.41964341630532\n",
      "Iteration 340, the loss is 296.1570190481614, parameters k is -30.265176553555147 and b is -83.41864341630531\n",
      "Iteration 341, the loss is 296.11652241877846, parameters k is -30.258891919167795 and b is -83.41764341630531\n",
      "Iteration 342, the loss is 296.07602578939594, parameters k is -30.252607284780442 and b is -83.4166434163053\n",
      "Iteration 343, the loss is 296.03552916001314, parameters k is -30.24632265039309 and b is -83.4156434163053\n",
      "Iteration 344, the loss is 295.9950325306308, parameters k is -30.240038016005737 and b is -83.4146434163053\n",
      "Iteration 345, the loss is 295.95453590124794, parameters k is -30.233753381618385 and b is -83.41364341630529\n",
      "Iteration 346, the loss is 295.9140392718653, parameters k is -30.227468747231033 and b is -83.41264341630529\n",
      "Iteration 347, the loss is 295.8735426424828, parameters k is -30.22118411284368 and b is -83.41164341630528\n",
      "Iteration 348, the loss is 295.8330460131002, parameters k is -30.214899478456328 and b is -83.41064341630528\n",
      "Iteration 349, the loss is 295.79254938371736, parameters k is -30.208614844068975 and b is -83.40964341630527\n",
      "Iteration 350, the loss is 295.7520527543349, parameters k is -30.202330209681623 and b is -83.40864341630527\n",
      "Iteration 351, the loss is 295.71155612495176, parameters k is -30.19604557529427 and b is -83.40764341630526\n",
      "Iteration 352, the loss is 295.67105949556924, parameters k is -30.18976094090692 and b is -83.40664341630526\n",
      "Iteration 353, the loss is 295.6305628661866, parameters k is -30.183476306519566 and b is -83.40564341630525\n",
      "Iteration 354, the loss is 295.5900662368039, parameters k is -30.177191672132214 and b is -83.40464341630525\n",
      "Iteration 355, the loss is 295.5495696074208, parameters k is -30.17090703774486 and b is -83.40364341630524\n",
      "Iteration 356, the loss is 295.5090729780381, parameters k is -30.16462240335751 and b is -83.40264341630524\n",
      "Iteration 357, the loss is 295.4685763486555, parameters k is -30.158337768970156 and b is -83.40164341630523\n",
      "Iteration 358, the loss is 295.428079719273, parameters k is -30.152053134582804 and b is -83.40064341630523\n",
      "Iteration 359, the loss is 295.3875830898902, parameters k is -30.14576850019545 and b is -83.39964341630522\n",
      "Iteration 360, the loss is 295.34708646050757, parameters k is -30.1394838658081 and b is -83.39864341630522\n",
      "Iteration 361, the loss is 295.3065898311248, parameters k is -30.133199231420747 and b is -83.39764341630521\n",
      "Iteration 362, the loss is 295.2660932017423, parameters k is -30.126914597033394 and b is -83.39664341630521\n",
      "Iteration 363, the loss is 295.22559657235956, parameters k is -30.120629962646042 and b is -83.3956434163052\n",
      "Iteration 364, the loss is 295.18509994297676, parameters k is -30.11434532825869 and b is -83.3946434163052\n",
      "Iteration 365, the loss is 295.1446033135942, parameters k is -30.108060693871337 and b is -83.3936434163052\n",
      "Iteration 366, the loss is 295.1041066842113, parameters k is -30.101776059483985 and b is -83.39264341630519\n",
      "Iteration 367, the loss is 295.0636100548288, parameters k is -30.095491425096633 and b is -83.39164341630519\n",
      "Iteration 368, the loss is 295.0231134254458, parameters k is -30.08920679070928 and b is -83.39064341630518\n",
      "Iteration 369, the loss is 294.9826167960636, parameters k is -30.082922156321928 and b is -83.38964341630518\n",
      "Iteration 370, the loss is 294.9421201666804, parameters k is -30.076637521934575 and b is -83.38864341630517\n",
      "Iteration 371, the loss is 294.9016235372977, parameters k is -30.070352887547223 and b is -83.38764341630517\n",
      "Iteration 372, the loss is 294.86112690791555, parameters k is -30.06406825315987 and b is -83.38664341630516\n",
      "Iteration 373, the loss is 294.82063027853235, parameters k is -30.05778361877252 and b is -83.38564341630516\n",
      "Iteration 374, the loss is 294.78013364915006, parameters k is -30.051498984385166 and b is -83.38464341630515\n",
      "Iteration 375, the loss is 294.73963701976726, parameters k is -30.045214349997813 and b is -83.38364341630515\n",
      "Iteration 376, the loss is 294.69914039038474, parameters k is -30.03892971561046 and b is -83.38264341630514\n",
      "Iteration 377, the loss is 294.65864376100205, parameters k is -30.03264508122311 and b is -83.38164341630514\n",
      "Iteration 378, the loss is 294.6181471316188, parameters k is -30.026360446835756 and b is -83.38064341630513\n",
      "Iteration 379, the loss is 294.5776505022367, parameters k is -30.020075812448404 and b is -83.37964341630513\n",
      "Iteration 380, the loss is 294.5371538728537, parameters k is -30.01379117806105 and b is -83.37864341630512\n",
      "Iteration 381, the loss is 294.4966572434711, parameters k is -30.0075065436737 and b is -83.37764341630512\n",
      "Iteration 382, the loss is 294.45616061408873, parameters k is -30.001221909286347 and b is -83.37664341630511\n",
      "Iteration 383, the loss is 294.4156639847054, parameters k is -29.994937274898994 and b is -83.37564341630511\n",
      "Iteration 384, the loss is 294.37516735532313, parameters k is -29.988652640511642 and b is -83.3746434163051\n",
      "Iteration 385, the loss is 294.3346707259401, parameters k is -29.98236800612429 and b is -83.3736434163051\n",
      "Iteration 386, the loss is 294.29417409655747, parameters k is -29.976083371736937 and b is -83.3726434163051\n",
      "Iteration 387, the loss is 294.25367746717455, parameters k is -29.969798737349585 and b is -83.37164341630509\n",
      "Iteration 388, the loss is 294.2131808377924, parameters k is -29.963514102962232 and b is -83.37064341630509\n",
      "Iteration 389, the loss is 294.1726842084094, parameters k is -29.95722946857488 and b is -83.36964341630508\n",
      "Iteration 390, the loss is 294.1321875790265, parameters k is -29.950944834187528 and b is -83.36864341630508\n",
      "Iteration 391, the loss is 294.09169094964403, parameters k is -29.944660199800175 and b is -83.36764341630507\n",
      "Iteration 392, the loss is 294.0511943202615, parameters k is -29.938375565412823 and b is -83.36664341630507\n",
      "Iteration 393, the loss is 294.0106976908786, parameters k is -29.93209093102547 and b is -83.36564341630506\n",
      "Iteration 394, the loss is 293.9702010614959, parameters k is -29.925806296638118 and b is -83.36464341630506\n",
      "Iteration 395, the loss is 293.9297044321133, parameters k is -29.919521662250766 and b is -83.36364341630505\n",
      "Iteration 396, the loss is 293.88920780273025, parameters k is -29.913237027863413 and b is -83.36264341630505\n",
      "Iteration 397, the loss is 293.84871117334785, parameters k is -29.90695239347606 and b is -83.36164341630504\n",
      "Iteration 398, the loss is 293.8082145439654, parameters k is -29.90066775908871 and b is -83.36064341630504\n",
      "Iteration 399, the loss is 293.7677179145826, parameters k is -29.894383124701356 and b is -83.35964341630503\n",
      "Iteration 400, the loss is 293.7272212851999, parameters k is -29.888098490314004 and b is -83.35864341630503\n",
      "Iteration 401, the loss is 293.6867246558173, parameters k is -29.88181385592665 and b is -83.35764341630502\n",
      "Iteration 402, the loss is 293.64622802643447, parameters k is -29.8755292215393 and b is -83.35664341630502\n",
      "Iteration 403, the loss is 293.605731397052, parameters k is -29.869244587151947 and b is -83.35564341630501\n",
      "Iteration 404, the loss is 293.56523476766955, parameters k is -29.862959952764594 and b is -83.35464341630501\n",
      "Iteration 405, the loss is 293.5247381382864, parameters k is -29.856675318377242 and b is -83.353643416305\n",
      "Iteration 406, the loss is 293.48424150890366, parameters k is -29.85039068398989 and b is -83.352643416305\n",
      "Iteration 407, the loss is 293.44374487952075, parameters k is -29.844106049602537 and b is -83.351643416305\n",
      "Iteration 408, the loss is 293.4032482501388, parameters k is -29.837821415215185 and b is -83.35064341630499\n",
      "Iteration 409, the loss is 293.3627516207556, parameters k is -29.831536780827832 and b is -83.34964341630499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 410, the loss is 293.3222549913725, parameters k is -29.82525214644048 and b is -83.34864341630498\n",
      "Iteration 411, the loss is 293.2817583619904, parameters k is -29.818967512053128 and b is -83.34764341630498\n",
      "Iteration 412, the loss is 293.2412617326077, parameters k is -29.812682877665775 and b is -83.34664341630497\n",
      "Iteration 413, the loss is 293.2007651032246, parameters k is -29.806398243278423 and b is -83.34564341630497\n",
      "Iteration 414, the loss is 293.1602684738421, parameters k is -29.80011360889107 and b is -83.34464341630496\n",
      "Iteration 415, the loss is 293.11977184445976, parameters k is -29.793828974503718 and b is -83.34364341630496\n",
      "Iteration 416, the loss is 293.0792752150769, parameters k is -29.787544340116366 and b is -83.34264341630495\n",
      "Iteration 417, the loss is 293.03877858569365, parameters k is -29.781259705729013 and b is -83.34164341630495\n",
      "Iteration 418, the loss is 292.9982819563114, parameters k is -29.77497507134166 and b is -83.34064341630494\n",
      "Iteration 419, the loss is 292.9577853269288, parameters k is -29.76869043695431 and b is -83.33964341630494\n",
      "Iteration 420, the loss is 292.917288697546, parameters k is -29.762405802566956 and b is -83.33864341630493\n",
      "Iteration 421, the loss is 292.8767920681634, parameters k is -29.756121168179604 and b is -83.33764341630493\n",
      "Iteration 422, the loss is 292.83629543878044, parameters k is -29.74983653379225 and b is -83.33664341630492\n",
      "Iteration 423, the loss is 292.7957988093981, parameters k is -29.7435518994049 and b is -83.33564341630492\n",
      "Iteration 424, the loss is 292.7553021800151, parameters k is -29.737267265017547 and b is -83.33464341630491\n",
      "Iteration 425, the loss is 292.71480555063243, parameters k is -29.730982630630194 and b is -83.33364341630491\n",
      "Iteration 426, the loss is 292.6743089212499, parameters k is -29.724697996242842 and b is -83.3326434163049\n",
      "Iteration 427, the loss is 292.633812291867, parameters k is -29.71841336185549 and b is -83.3316434163049\n",
      "Iteration 428, the loss is 292.5933156624843, parameters k is -29.712128727468137 and b is -83.3306434163049\n",
      "Iteration 429, the loss is 292.5528190331014, parameters k is -29.705844093080785 and b is -83.32964341630489\n",
      "Iteration 430, the loss is 292.5123224037191, parameters k is -29.699559458693432 and b is -83.32864341630489\n",
      "Iteration 431, the loss is 292.4718257743362, parameters k is -29.69327482430608 and b is -83.32764341630488\n",
      "Iteration 432, the loss is 292.4313291449537, parameters k is -29.686990189918728 and b is -83.32664341630488\n",
      "Iteration 433, the loss is 292.39083251557093, parameters k is -29.680705555531375 and b is -83.32564341630487\n",
      "Iteration 434, the loss is 292.3503358861884, parameters k is -29.674420921144023 and b is -83.32464341630487\n",
      "Iteration 435, the loss is 292.30983925680556, parameters k is -29.66813628675667 and b is -83.32364341630486\n",
      "Iteration 436, the loss is 292.26934262742293, parameters k is -29.661851652369318 and b is -83.32264341630486\n",
      "Iteration 437, the loss is 292.2288459980403, parameters k is -29.655567017981966 and b is -83.32164341630485\n",
      "Iteration 438, the loss is 292.1883493686574, parameters k is -29.649282383594613 and b is -83.32064341630485\n",
      "Iteration 439, the loss is 292.14785273927475, parameters k is -29.64299774920726 and b is -83.31964341630484\n",
      "Iteration 440, the loss is 292.1073561098923, parameters k is -29.63671311481991 and b is -83.31864341630484\n",
      "Iteration 441, the loss is 292.0668594805092, parameters k is -29.630428480432556 and b is -83.31764341630483\n",
      "Iteration 442, the loss is 292.0263628511271, parameters k is -29.624143846045204 and b is -83.31664341630483\n",
      "Iteration 443, the loss is 291.98586622174366, parameters k is -29.61785921165785 and b is -83.31564341630482\n",
      "Iteration 444, the loss is 291.94536959236126, parameters k is -29.6115745772705 and b is -83.31464341630482\n",
      "Iteration 445, the loss is 291.9048729629784, parameters k is -29.605289942883147 and b is -83.31364341630481\n",
      "Iteration 446, the loss is 291.86437633359634, parameters k is -29.599005308495794 and b is -83.31264341630481\n",
      "Iteration 447, the loss is 291.823879704213, parameters k is -29.592720674108442 and b is -83.3116434163048\n",
      "Iteration 448, the loss is 291.7833830748307, parameters k is -29.58643603972109 and b is -83.3106434163048\n",
      "Iteration 449, the loss is 291.74288644544805, parameters k is -29.580151405333737 and b is -83.3096434163048\n",
      "Iteration 450, the loss is 291.70238981606497, parameters k is -29.573866770946385 and b is -83.30864341630479\n",
      "Iteration 451, the loss is 291.66189318668245, parameters k is -29.567582136559032 and b is -83.30764341630478\n",
      "Iteration 452, the loss is 291.62139655729993, parameters k is -29.56129750217168 and b is -83.30664341630478\n",
      "Iteration 453, the loss is 291.5808999279171, parameters k is -29.555012867784328 and b is -83.30564341630478\n",
      "Iteration 454, the loss is 291.5404032985343, parameters k is -29.548728233396975 and b is -83.30464341630477\n",
      "Iteration 455, the loss is 291.4999066691516, parameters k is -29.542443599009623 and b is -83.30364341630477\n",
      "Iteration 456, the loss is 291.4594100397693, parameters k is -29.53615896462227 and b is -83.30264341630476\n",
      "Iteration 457, the loss is 291.4189134103863, parameters k is -29.529874330234918 and b is -83.30164341630476\n",
      "Iteration 458, the loss is 291.3784167810037, parameters k is -29.523589695847566 and b is -83.30064341630475\n",
      "Iteration 459, the loss is 291.3379201516211, parameters k is -29.517305061460213 and b is -83.29964341630475\n",
      "Iteration 460, the loss is 291.2974235222387, parameters k is -29.51102042707286 and b is -83.29864341630474\n",
      "Iteration 461, the loss is 291.25692689285546, parameters k is -29.50473579268551 and b is -83.29764341630474\n",
      "Iteration 462, the loss is 291.21643026347294, parameters k is -29.498451158298156 and b is -83.29664341630473\n",
      "Iteration 463, the loss is 291.1759336340902, parameters k is -29.492166523910804 and b is -83.29564341630473\n",
      "Iteration 464, the loss is 291.13543700470734, parameters k is -29.48588188952345 and b is -83.29464341630472\n",
      "Iteration 465, the loss is 291.09494037532454, parameters k is -29.4795972551361 and b is -83.29364341630472\n",
      "Iteration 466, the loss is 291.0544437459416, parameters k is -29.473312620748747 and b is -83.29264341630471\n",
      "Iteration 467, the loss is 291.01394711655917, parameters k is -29.467027986361394 and b is -83.29164341630471\n",
      "Iteration 468, the loss is 290.9734504871767, parameters k is -29.460743351974042 and b is -83.2906434163047\n",
      "Iteration 469, the loss is 290.932953857794, parameters k is -29.45445871758669 and b is -83.2896434163047\n",
      "Iteration 470, the loss is 290.8924572284112, parameters k is -29.448174083199337 and b is -83.2886434163047\n",
      "Iteration 471, the loss is 290.85196059902887, parameters k is -29.441889448811985 and b is -83.28764341630469\n",
      "Iteration 472, the loss is 290.81146396964596, parameters k is -29.435604814424632 and b is -83.28664341630468\n",
      "Iteration 473, the loss is 290.77096734026344, parameters k is -29.42932018003728 and b is -83.28564341630468\n",
      "Iteration 474, the loss is 290.7304707108806, parameters k is -29.423035545649928 and b is -83.28464341630468\n",
      "Iteration 475, the loss is 290.689974081498, parameters k is -29.416750911262575 and b is -83.28364341630467\n",
      "Iteration 476, the loss is 290.64947745211543, parameters k is -29.410466276875223 and b is -83.28264341630467\n",
      "Iteration 477, the loss is 290.6089808227326, parameters k is -29.40418164248787 and b is -83.28164341630466\n",
      "Iteration 478, the loss is 290.56848419335, parameters k is -29.397897008100518 and b is -83.28064341630466\n",
      "Iteration 479, the loss is 290.52798756396714, parameters k is -29.391612373713166 and b is -83.27964341630465\n",
      "Iteration 480, the loss is 290.48749093458395, parameters k is -29.385327739325813 and b is -83.27864341630465\n",
      "Iteration 481, the loss is 290.44699430520194, parameters k is -29.37904310493846 and b is -83.27764341630464\n",
      "Iteration 482, the loss is 290.406497675819, parameters k is -29.37275847055111 and b is -83.27664341630464\n",
      "Iteration 483, the loss is 290.36600104643634, parameters k is -29.366473836163756 and b is -83.27564341630463\n",
      "Iteration 484, the loss is 290.32550441705393, parameters k is -29.360189201776404 and b is -83.27464341630463\n",
      "Iteration 485, the loss is 290.28500778767074, parameters k is -29.35390456738905 and b is -83.27364341630462\n",
      "Iteration 486, the loss is 290.24451115828816, parameters k is -29.3476199330017 and b is -83.27264341630462\n",
      "Iteration 487, the loss is 290.20401452890593, parameters k is -29.341335298614347 and b is -83.27164341630461\n",
      "Iteration 488, the loss is 290.16351789952296, parameters k is -29.335050664226994 and b is -83.27064341630461\n",
      "Iteration 489, the loss is 290.1230212701401, parameters k is -29.32876602983964 and b is -83.2696434163046\n",
      "Iteration 490, the loss is 290.08252464075764, parameters k is -29.32248139545229 and b is -83.2686434163046\n",
      "Iteration 491, the loss is 290.0420280113749, parameters k is -29.316196761064937 and b is -83.2676434163046\n",
      "Iteration 492, the loss is 290.00153138199204, parameters k is -29.309912126677585 and b is -83.26664341630459\n",
      "Iteration 493, the loss is 289.9610347526095, parameters k is -29.303627492290232 and b is -83.26564341630458\n",
      "Iteration 494, the loss is 289.92053812322644, parameters k is -29.29734285790288 and b is -83.26464341630458\n",
      "Iteration 495, the loss is 289.88004149384415, parameters k is -29.291058223515527 and b is -83.26364341630457\n",
      "Iteration 496, the loss is 289.8395448644616, parameters k is -29.284773589128175 and b is -83.26264341630457\n",
      "Iteration 497, the loss is 289.7990482350786, parameters k is -29.278488954740823 and b is -83.26164341630457\n",
      "Iteration 498, the loss is 289.7585516056961, parameters k is -29.27220432035347 and b is -83.26064341630456\n",
      "Iteration 499, the loss is 289.7180549763133, parameters k is -29.265919685966118 and b is -83.25964341630456\n",
      "Iteration 500, the loss is 289.6775583469303, parameters k is -29.259635051578766 and b is -83.25864341630455\n",
      "Iteration 501, the loss is 289.6370617175481, parameters k is -29.253350417191413 and b is -83.25764341630455\n",
      "Iteration 502, the loss is 289.596565088165, parameters k is -29.24706578280406 and b is -83.25664341630454\n",
      "Iteration 503, the loss is 289.5560684587825, parameters k is -29.24078114841671 and b is -83.25564341630454\n",
      "Iteration 504, the loss is 289.51557182939985, parameters k is -29.234496514029356 and b is -83.25464341630453\n",
      "Iteration 505, the loss is 289.47507520001693, parameters k is -29.228211879642004 and b is -83.25364341630453\n",
      "Iteration 506, the loss is 289.43457857063464, parameters k is -29.22192724525465 and b is -83.25264341630452\n",
      "Iteration 507, the loss is 289.39408194125156, parameters k is -29.2156426108673 and b is -83.25164341630452\n",
      "Iteration 508, the loss is 289.3535853118693, parameters k is -29.209357976479946 and b is -83.25064341630451\n",
      "Iteration 509, the loss is 289.3130886824865, parameters k is -29.203073342092594 and b is -83.24964341630451\n",
      "Iteration 510, the loss is 289.2725920531037, parameters k is -29.19678870770524 and b is -83.2486434163045\n",
      "Iteration 511, the loss is 289.23209542372126, parameters k is -29.19050407331789 and b is -83.2476434163045\n",
      "Iteration 512, the loss is 289.19159879433863, parameters k is -29.184219438930537 and b is -83.2466434163045\n",
      "Iteration 513, the loss is 289.1511021649557, parameters k is -29.177934804543185 and b is -83.24564341630449\n",
      "Iteration 514, the loss is 289.1106055355728, parameters k is -29.171650170155832 and b is -83.24464341630448\n",
      "Iteration 515, the loss is 289.0701089061904, parameters k is -29.16536553576848 and b is -83.24364341630448\n",
      "Iteration 516, the loss is 289.02961227680737, parameters k is -29.159080901381127 and b is -83.24264341630447\n",
      "Iteration 517, the loss is 288.98911564742485, parameters k is -29.152796266993775 and b is -83.24164341630447\n",
      "Iteration 518, the loss is 288.9486190180417, parameters k is -29.146511632606423 and b is -83.24064341630447\n",
      "Iteration 519, the loss is 288.9081223886593, parameters k is -29.14022699821907 and b is -83.23964341630446\n",
      "Iteration 520, the loss is 288.8676257592771, parameters k is -29.133942363831718 and b is -83.23864341630446\n",
      "Iteration 521, the loss is 288.82712912989376, parameters k is -29.127657729444365 and b is -83.23764341630445\n",
      "Iteration 522, the loss is 288.7866325005113, parameters k is -29.121373095057013 and b is -83.23664341630445\n",
      "Iteration 523, the loss is 288.74613587112873, parameters k is -29.11508846066966 and b is -83.23564341630444\n",
      "Iteration 524, the loss is 288.7056392417461, parameters k is -29.10880382628231 and b is -83.23464341630444\n",
      "Iteration 525, the loss is 288.6651426123634, parameters k is -29.102519191894956 and b is -83.23364341630443\n",
      "Iteration 526, the loss is 288.62464598298044, parameters k is -29.096234557507604 and b is -83.23264341630443\n",
      "Iteration 527, the loss is 288.58414935359775, parameters k is -29.08994992312025 and b is -83.23164341630442\n",
      "Iteration 528, the loss is 288.5436527242154, parameters k is -29.0836652887329 and b is -83.23064341630442\n",
      "Iteration 529, the loss is 288.50315609483243, parameters k is -29.077380654345546 and b is -83.22964341630441\n",
      "Iteration 530, the loss is 288.46265946544975, parameters k is -29.071096019958194 and b is -83.22864341630441\n",
      "Iteration 531, the loss is 288.4221628360672, parameters k is -29.06481138557084 and b is -83.2276434163044\n",
      "Iteration 532, the loss is 288.38166620668426, parameters k is -29.05852675118349 and b is -83.2266434163044\n",
      "Iteration 533, the loss is 288.3411695773013, parameters k is -29.052242116796137 and b is -83.2256434163044\n",
      "Iteration 534, the loss is 288.30067294791894, parameters k is -29.045957482408785 and b is -83.22464341630439\n",
      "Iteration 535, the loss is 288.2601763185364, parameters k is -29.039672848021432 and b is -83.22364341630438\n",
      "Iteration 536, the loss is 288.21967968915345, parameters k is -29.03338821363408 and b is -83.22264341630438\n",
      "Iteration 537, the loss is 288.1791830597708, parameters k is -29.027103579246727 and b is -83.22164341630437\n",
      "Iteration 538, the loss is 288.138686430388, parameters k is -29.020818944859375 and b is -83.22064341630437\n",
      "Iteration 539, the loss is 288.0981898010056, parameters k is -29.014534310472023 and b is -83.21964341630436\n",
      "Iteration 540, the loss is 288.05769317162265, parameters k is -29.00824967608467 and b is -83.21864341630436\n",
      "Iteration 541, the loss is 288.01719654224024, parameters k is -29.001965041697318 and b is -83.21764341630436\n",
      "Iteration 542, the loss is 287.9766999128576, parameters k is -28.995680407309965 and b is -83.21664341630435\n",
      "Iteration 543, the loss is 287.9362032834748, parameters k is -28.989395772922613 and b is -83.21564341630435\n",
      "Iteration 544, the loss is 287.8957066540923, parameters k is -28.98311113853526 and b is -83.21464341630434\n",
      "Iteration 545, the loss is 287.85521002470927, parameters k is -28.97682650414791 and b is -83.21364341630434\n",
      "Iteration 546, the loss is 287.81471339532686, parameters k is -28.970541869760556 and b is -83.21264341630433\n",
      "Iteration 547, the loss is 287.77421676594395, parameters k is -28.964257235373204 and b is -83.21164341630433\n",
      "Iteration 548, the loss is 287.73372013656126, parameters k is -28.95797260098585 and b is -83.21064341630432\n",
      "Iteration 549, the loss is 287.6932235071786, parameters k is -28.9516879665985 and b is -83.20964341630432\n",
      "Iteration 550, the loss is 287.6527268777963, parameters k is -28.945403332211146 and b is -83.20864341630431\n",
      "Iteration 551, the loss is 287.6122302484136, parameters k is -28.939118697823794 and b is -83.20764341630431\n",
      "Iteration 552, the loss is 287.5717336190308, parameters k is -28.93283406343644 and b is -83.2066434163043\n",
      "Iteration 553, the loss is 287.5312369896478, parameters k is -28.92654942904909 and b is -83.2056434163043\n",
      "Iteration 554, the loss is 287.49074036026514, parameters k is -28.920264794661737 and b is -83.2046434163043\n",
      "Iteration 555, the loss is 287.45024373088245, parameters k is -28.913980160274384 and b is -83.20364341630429\n",
      "Iteration 556, the loss is 287.40974710150005, parameters k is -28.907695525887032 and b is -83.20264341630428\n",
      "Iteration 557, the loss is 287.36925047211673, parameters k is -28.90141089149968 and b is -83.20164341630428\n",
      "Iteration 558, the loss is 287.3287538427343, parameters k is -28.895126257112327 and b is -83.20064341630427\n",
      "Iteration 559, the loss is 287.28825721335187, parameters k is -28.888841622724975 and b is -83.19964341630427\n",
      "Iteration 560, the loss is 287.2477605839686, parameters k is -28.882556988337623 and b is -83.19864341630426\n",
      "Iteration 561, the loss is 287.20726395458627, parameters k is -28.87627235395027 and b is -83.19764341630426\n",
      "Iteration 562, the loss is 287.16676732520364, parameters k is -28.869987719562918 and b is -83.19664341630425\n",
      "Iteration 563, the loss is 287.1262706958212, parameters k is -28.863703085175565 and b is -83.19564341630425\n",
      "Iteration 564, the loss is 287.08577406643855, parameters k is -28.857418450788213 and b is -83.19464341630425\n",
      "Iteration 565, the loss is 287.04527743705574, parameters k is -28.85113381640086 and b is -83.19364341630424\n",
      "Iteration 566, the loss is 287.00478080767294, parameters k is -28.84484918201351 and b is -83.19264341630424\n",
      "Iteration 567, the loss is 286.96428417829, parameters k is -28.838564547626156 and b is -83.19164341630423\n",
      "Iteration 568, the loss is 286.92378754890734, parameters k is -28.832279913238803 and b is -83.19064341630423\n",
      "Iteration 569, the loss is 286.8832909195248, parameters k is -28.82599527885145 and b is -83.18964341630422\n",
      "Iteration 570, the loss is 286.84279429014197, parameters k is -28.8197106444641 and b is -83.18864341630422\n",
      "Iteration 571, the loss is 286.8022976607594, parameters k is -28.813426010076746 and b is -83.18764341630421\n",
      "Iteration 572, the loss is 286.76180103137665, parameters k is -28.807141375689394 and b is -83.18664341630421\n",
      "Iteration 573, the loss is 286.721304401994, parameters k is -28.80085674130204 and b is -83.1856434163042\n",
      "Iteration 574, the loss is 286.6808077726114, parameters k is -28.79457210691469 and b is -83.1846434163042\n",
      "Iteration 575, the loss is 286.6403111432283, parameters k is -28.788287472527337 and b is -83.1836434163042\n",
      "Iteration 576, the loss is 286.5998145138456, parameters k is -28.782002838139984 and b is -83.18264341630419\n",
      "Iteration 577, the loss is 286.5593178844633, parameters k is -28.775718203752632 and b is -83.18164341630418\n",
      "Iteration 578, the loss is 286.5188212550806, parameters k is -28.76943356936528 and b is -83.18064341630418\n",
      "Iteration 579, the loss is 286.47832462569755, parameters k is -28.763148934977927 and b is -83.17964341630417\n",
      "Iteration 580, the loss is 286.43782799631515, parameters k is -28.756864300590575 and b is -83.17864341630417\n",
      "Iteration 581, the loss is 286.3973313669324, parameters k is -28.750579666203222 and b is -83.17764341630416\n",
      "Iteration 582, the loss is 286.35683473754966, parameters k is -28.74429503181587 and b is -83.17664341630416\n",
      "Iteration 583, the loss is 286.31633810816703, parameters k is -28.738010397428518 and b is -83.17564341630415\n",
      "Iteration 584, the loss is 286.2758414787841, parameters k is -28.731725763041165 and b is -83.17464341630415\n",
      "Iteration 585, the loss is 286.23534484940166, parameters k is -28.725441128653813 and b is -83.17364341630415\n",
      "Iteration 586, the loss is 286.19484822001914, parameters k is -28.71915649426646 and b is -83.17264341630414\n",
      "Iteration 587, the loss is 286.15435159063645, parameters k is -28.712871859879108 and b is -83.17164341630414\n",
      "Iteration 588, the loss is 286.1138549612535, parameters k is -28.706587225491756 and b is -83.17064341630413\n",
      "Iteration 589, the loss is 286.0733583318708, parameters k is -28.700302591104403 and b is -83.16964341630413\n",
      "Iteration 590, the loss is 286.0328617024879, parameters k is -28.69401795671705 and b is -83.16864341630412\n",
      "Iteration 591, the loss is 285.99236507310536, parameters k is -28.6877333223297 and b is -83.16764341630412\n",
      "Iteration 592, the loss is 285.951868443723, parameters k is -28.681448687942346 and b is -83.16664341630411\n",
      "Iteration 593, the loss is 285.91137181434016, parameters k is -28.675164053554994 and b is -83.1656434163041\n",
      "Iteration 594, the loss is 285.87087518495787, parameters k is -28.66887941916764 and b is -83.1646434163041\n",
      "Iteration 595, the loss is 285.83037855557444, parameters k is -28.66259478478029 and b is -83.1636434163041\n",
      "Iteration 596, the loss is 285.78988192619215, parameters k is -28.656310150392937 and b is -83.16264341630409\n",
      "Iteration 597, the loss is 285.7493852968092, parameters k is -28.650025516005584 and b is -83.16164341630409\n",
      "Iteration 598, the loss is 285.7088886674265, parameters k is -28.643740881618232 and b is -83.16064341630408\n",
      "Iteration 599, the loss is 285.66839203804375, parameters k is -28.63745624723088 and b is -83.15964341630408\n",
      "Iteration 600, the loss is 285.62789540866106, parameters k is -28.631171612843527 and b is -83.15864341630407\n",
      "Iteration 601, the loss is 285.5873987792786, parameters k is -28.624886978456175 and b is -83.15764341630407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 602, the loss is 285.54690214989574, parameters k is -28.618602344068822 and b is -83.15664341630406\n",
      "Iteration 603, the loss is 285.5064055205133, parameters k is -28.61231770968147 and b is -83.15564341630406\n",
      "Iteration 604, the loss is 285.46590889113037, parameters k is -28.606033075294118 and b is -83.15464341630405\n",
      "Iteration 605, the loss is 285.4254122617475, parameters k is -28.599748440906765 and b is -83.15364341630405\n",
      "Iteration 606, the loss is 285.3849156323651, parameters k is -28.593463806519413 and b is -83.15264341630404\n",
      "Iteration 607, the loss is 285.34441900298236, parameters k is -28.58717917213206 and b is -83.15164341630404\n",
      "Iteration 608, the loss is 285.30392237359973, parameters k is -28.580894537744708 and b is -83.15064341630404\n",
      "Iteration 609, the loss is 285.26342574421693, parameters k is -28.574609903357356 and b is -83.14964341630403\n",
      "Iteration 610, the loss is 285.22292911483424, parameters k is -28.568325268970003 and b is -83.14864341630403\n",
      "Iteration 611, the loss is 285.18243248545167, parameters k is -28.56204063458265 and b is -83.14764341630402\n",
      "Iteration 612, the loss is 285.14193585606887, parameters k is -28.5557560001953 and b is -83.14664341630402\n",
      "Iteration 613, the loss is 285.101439226686, parameters k is -28.549471365807946 and b is -83.14564341630401\n",
      "Iteration 614, the loss is 285.0609425973033, parameters k is -28.543186731420594 and b is -83.144643416304\n",
      "Iteration 615, the loss is 285.02044596792115, parameters k is -28.53690209703324 and b is -83.143643416304\n",
      "Iteration 616, the loss is 284.9799493385382, parameters k is -28.53061746264589 and b is -83.142643416304\n",
      "Iteration 617, the loss is 284.9394527091554, parameters k is -28.524332828258537 and b is -83.14164341630399\n",
      "Iteration 618, the loss is 284.898956079773, parameters k is -28.518048193871184 and b is -83.14064341630399\n",
      "Iteration 619, the loss is 284.85845945039006, parameters k is -28.511763559483832 and b is -83.13964341630398\n",
      "Iteration 620, the loss is 284.81796282100777, parameters k is -28.50547892509648 and b is -83.13864341630398\n",
      "Iteration 621, the loss is 284.7774661916246, parameters k is -28.499194290709127 and b is -83.13764341630397\n",
      "Iteration 622, the loss is 284.7369695622422, parameters k is -28.492909656321775 and b is -83.13664341630397\n",
      "Iteration 623, the loss is 284.69647293285965, parameters k is -28.486625021934422 and b is -83.13564341630396\n",
      "Iteration 624, the loss is 284.65597630347673, parameters k is -28.48034038754707 and b is -83.13464341630396\n",
      "Iteration 625, the loss is 284.61547967409416, parameters k is -28.474055753159718 and b is -83.13364341630395\n",
      "Iteration 626, the loss is 284.57498304471125, parameters k is -28.467771118772365 and b is -83.13264341630395\n",
      "Iteration 627, the loss is 284.5344864153283, parameters k is -28.461486484385013 and b is -83.13164341630394\n",
      "Iteration 628, the loss is 284.4939897859459, parameters k is -28.45520184999766 and b is -83.13064341630394\n",
      "Iteration 629, the loss is 284.45349315656307, parameters k is -28.448917215610308 and b is -83.12964341630394\n",
      "Iteration 630, the loss is 284.4129965271805, parameters k is -28.442632581222956 and b is -83.12864341630393\n",
      "Iteration 631, the loss is 284.372499897798, parameters k is -28.436347946835603 and b is -83.12764341630393\n",
      "Iteration 632, the loss is 284.3320032684152, parameters k is -28.43006331244825 and b is -83.12664341630392\n",
      "Iteration 633, the loss is 284.2915066390325, parameters k is -28.4237786780609 and b is -83.12564341630392\n",
      "Iteration 634, the loss is 284.25101000964963, parameters k is -28.417494043673546 and b is -83.12464341630391\n",
      "Iteration 635, the loss is 284.2105133802669, parameters k is -28.411209409286194 and b is -83.1236434163039\n",
      "Iteration 636, the loss is 284.17001675088443, parameters k is -28.40492477489884 and b is -83.1226434163039\n",
      "Iteration 637, the loss is 284.1295201215018, parameters k is -28.39864014051149 and b is -83.1216434163039\n",
      "Iteration 638, the loss is 284.0890234921188, parameters k is -28.392355506124137 and b is -83.12064341630389\n",
      "Iteration 639, the loss is 284.0485268627363, parameters k is -28.386070871736784 and b is -83.11964341630389\n",
      "Iteration 640, the loss is 284.00803023335385, parameters k is -28.379786237349432 and b is -83.11864341630388\n",
      "Iteration 641, the loss is 283.967533603971, parameters k is -28.37350160296208 and b is -83.11764341630388\n",
      "Iteration 642, the loss is 283.9270369745884, parameters k is -28.367216968574727 and b is -83.11664341630387\n",
      "Iteration 643, the loss is 283.88654034520545, parameters k is -28.360932334187375 and b is -83.11564341630387\n",
      "Iteration 644, the loss is 283.84604371582265, parameters k is -28.354647699800022 and b is -83.11464341630386\n",
      "Iteration 645, the loss is 283.80554708644024, parameters k is -28.34836306541267 and b is -83.11364341630386\n",
      "Iteration 646, the loss is 283.76505045705727, parameters k is -28.342078431025318 and b is -83.11264341630385\n",
      "Iteration 647, the loss is 283.724553827675, parameters k is -28.335793796637965 and b is -83.11164341630385\n",
      "Iteration 648, the loss is 283.6840571982919, parameters k is -28.329509162250613 and b is -83.11064341630384\n",
      "Iteration 649, the loss is 283.6435605689092, parameters k is -28.32322452786326 and b is -83.10964341630384\n",
      "Iteration 650, the loss is 283.6030639395268, parameters k is -28.316939893475908 and b is -83.10864341630383\n",
      "Iteration 651, the loss is 283.56256731014395, parameters k is -28.310655259088556 and b is -83.10764341630383\n",
      "Iteration 652, the loss is 283.5220706807613, parameters k is -28.304370624701203 and b is -83.10664341630383\n",
      "Iteration 653, the loss is 283.4815740513787, parameters k is -28.29808599031385 and b is -83.10564341630382\n",
      "Iteration 654, the loss is 283.44107742199566, parameters k is -28.2918013559265 and b is -83.10464341630382\n",
      "Iteration 655, the loss is 283.40058079261325, parameters k is -28.285516721539146 and b is -83.10364341630381\n",
      "Iteration 656, the loss is 283.36008416323045, parameters k is -28.279232087151794 and b is -83.1026434163038\n",
      "Iteration 657, the loss is 283.3195875338477, parameters k is -28.27294745276444 and b is -83.1016434163038\n",
      "Iteration 658, the loss is 283.279090904465, parameters k is -28.26666281837709 and b is -83.1006434163038\n",
      "Iteration 659, the loss is 283.2385942750828, parameters k is -28.260378183989737 and b is -83.09964341630379\n",
      "Iteration 660, the loss is 283.1980976456996, parameters k is -28.254093549602384 and b is -83.09864341630379\n",
      "Iteration 661, the loss is 283.157601016317, parameters k is -28.247808915215032 and b is -83.09764341630378\n",
      "Iteration 662, the loss is 283.1171043869341, parameters k is -28.24152428082768 and b is -83.09664341630378\n",
      "Iteration 663, the loss is 283.0766077575516, parameters k is -28.235239646440327 and b is -83.09564341630377\n",
      "Iteration 664, the loss is 283.0361111281689, parameters k is -28.228955012052975 and b is -83.09464341630377\n",
      "Iteration 665, the loss is 282.9956144987862, parameters k is -28.222670377665622 and b is -83.09364341630376\n",
      "Iteration 666, the loss is 282.95511786940335, parameters k is -28.21638574327827 and b is -83.09264341630376\n",
      "Iteration 667, the loss is 282.914621240021, parameters k is -28.210101108890917 and b is -83.09164341630375\n",
      "Iteration 668, the loss is 282.87412461063855, parameters k is -28.203816474503565 and b is -83.09064341630375\n",
      "Iteration 669, the loss is 282.83362798125546, parameters k is -28.197531840116213 and b is -83.08964341630374\n",
      "Iteration 670, the loss is 282.7931313518726, parameters k is -28.19124720572886 and b is -83.08864341630374\n",
      "Iteration 671, the loss is 282.75263472249003, parameters k is -28.184962571341508 and b is -83.08764341630373\n",
      "Iteration 672, the loss is 282.71213809310746, parameters k is -28.178677936954156 and b is -83.08664341630373\n",
      "Iteration 673, the loss is 282.67164146372477, parameters k is -28.172393302566803 and b is -83.08564341630372\n",
      "Iteration 674, the loss is 282.6311448343421, parameters k is -28.16610866817945 and b is -83.08464341630372\n",
      "Iteration 675, the loss is 282.5906482049595, parameters k is -28.1598240337921 and b is -83.08364341630372\n",
      "Iteration 676, the loss is 282.5501515755765, parameters k is -28.153539399404746 and b is -83.08264341630371\n",
      "Iteration 677, the loss is 282.5096549461938, parameters k is -28.147254765017394 and b is -83.0816434163037\n",
      "Iteration 678, the loss is 282.46915831681116, parameters k is -28.14097013063004 and b is -83.0806434163037\n",
      "Iteration 679, the loss is 282.4286616874285, parameters k is -28.13468549624269 and b is -83.0796434163037\n",
      "Iteration 680, the loss is 282.38816505804596, parameters k is -28.128400861855337 and b is -83.07864341630369\n",
      "Iteration 681, the loss is 282.3476684286632, parameters k is -28.122116227467984 and b is -83.07764341630369\n",
      "Iteration 682, the loss is 282.3071717992803, parameters k is -28.11583159308063 and b is -83.07664341630368\n",
      "Iteration 683, the loss is 282.2666751698978, parameters k is -28.10954695869328 and b is -83.07564341630368\n",
      "Iteration 684, the loss is 282.22617854051487, parameters k is -28.103262324305927 and b is -83.07464341630367\n",
      "Iteration 685, the loss is 282.1856819111321, parameters k is -28.096977689918575 and b is -83.07364341630367\n",
      "Iteration 686, the loss is 282.1451852817498, parameters k is -28.090693055531222 and b is -83.07264341630366\n",
      "Iteration 687, the loss is 282.104688652367, parameters k is -28.08440842114387 and b is -83.07164341630366\n",
      "Iteration 688, the loss is 282.06419202298406, parameters k is -28.078123786756517 and b is -83.07064341630365\n",
      "Iteration 689, the loss is 282.02369539360143, parameters k is -28.071839152369165 and b is -83.06964341630365\n",
      "Iteration 690, the loss is 281.98319876421886, parameters k is -28.065554517981813 and b is -83.06864341630364\n",
      "Iteration 691, the loss is 281.94270213483634, parameters k is -28.05926988359446 and b is -83.06764341630364\n",
      "Iteration 692, the loss is 281.90220550545354, parameters k is -28.052985249207108 and b is -83.06664341630363\n",
      "Iteration 693, the loss is 281.8617088760706, parameters k is -28.046700614819756 and b is -83.06564341630363\n",
      "Iteration 694, the loss is 281.8212122466884, parameters k is -28.040415980432403 and b is -83.06464341630362\n",
      "Iteration 695, the loss is 281.7807156173055, parameters k is -28.03413134604505 and b is -83.06364341630362\n",
      "Iteration 696, the loss is 281.7402189879226, parameters k is -28.0278467116577 and b is -83.06264341630362\n",
      "Iteration 697, the loss is 281.6997223585402, parameters k is -28.021562077270346 and b is -83.06164341630361\n",
      "Iteration 698, the loss is 281.65922572915736, parameters k is -28.015277442882994 and b is -83.0606434163036\n",
      "Iteration 699, the loss is 281.61872909977467, parameters k is -28.00899280849564 and b is -83.0596434163036\n",
      "Iteration 700, the loss is 281.578232470392, parameters k is -28.00270817410829 and b is -83.0586434163036\n",
      "Iteration 701, the loss is 281.5377358410091, parameters k is -27.996423539720936 and b is -83.05764341630359\n",
      "Iteration 702, the loss is 281.4972392116266, parameters k is -27.990138905333584 and b is -83.05664341630359\n",
      "Iteration 703, the loss is 281.45674258224375, parameters k is -27.98385427094623 and b is -83.05564341630358\n",
      "Iteration 704, the loss is 281.4162459528609, parameters k is -27.97756963655888 and b is -83.05464341630358\n",
      "Iteration 705, the loss is 281.3757493234786, parameters k is -27.971285002171527 and b is -83.05364341630357\n",
      "Iteration 706, the loss is 281.3352526940958, parameters k is -27.965000367784175 and b is -83.05264341630357\n",
      "Iteration 707, the loss is 281.29475606471317, parameters k is -27.958715733396822 and b is -83.05164341630356\n",
      "Iteration 708, the loss is 281.25425943533054, parameters k is -27.95243109900947 and b is -83.05064341630356\n",
      "Iteration 709, the loss is 281.2137628059474, parameters k is -27.946146464622117 and b is -83.04964341630355\n",
      "Iteration 710, the loss is 281.17326617656545, parameters k is -27.939861830234765 and b is -83.04864341630355\n",
      "Iteration 711, the loss is 281.1327695471822, parameters k is -27.933577195847413 and b is -83.04764341630354\n",
      "Iteration 712, the loss is 281.0922729177995, parameters k is -27.92729256146006 and b is -83.04664341630354\n",
      "Iteration 713, the loss is 281.0517762884168, parameters k is -27.921007927072708 and b is -83.04564341630353\n",
      "Iteration 714, the loss is 281.01127965903424, parameters k is -27.914723292685355 and b is -83.04464341630353\n",
      "Iteration 715, the loss is 280.9707830296515, parameters k is -27.908438658298003 and b is -83.04364341630352\n",
      "Iteration 716, the loss is 280.93028640026927, parameters k is -27.90215402391065 and b is -83.04264341630352\n",
      "Iteration 717, the loss is 280.8897897708862, parameters k is -27.8958693895233 and b is -83.04164341630351\n",
      "Iteration 718, the loss is 280.84929314150327, parameters k is -27.889584755135946 and b is -83.04064341630351\n",
      "Iteration 719, the loss is 280.8087965121207, parameters k is -27.883300120748594 and b is -83.0396434163035\n",
      "Iteration 720, the loss is 280.76829988273835, parameters k is -27.87701548636124 and b is -83.0386434163035\n",
      "Iteration 721, the loss is 280.72780325335526, parameters k is -27.87073085197389 and b is -83.0376434163035\n",
      "Iteration 722, the loss is 280.68730662397286, parameters k is -27.864446217586536 and b is -83.03664341630349\n",
      "Iteration 723, the loss is 280.6468099945901, parameters k is -27.858161583199184 and b is -83.03564341630349\n",
      "Iteration 724, the loss is 280.6063133652076, parameters k is -27.85187694881183 and b is -83.03464341630348\n",
      "Iteration 725, the loss is 280.5658167358248, parameters k is -27.84559231442448 and b is -83.03364341630348\n",
      "Iteration 726, the loss is 280.5253201064421, parameters k is -27.839307680037127 and b is -83.03264341630347\n",
      "Iteration 727, the loss is 280.4848234770592, parameters k is -27.833023045649774 and b is -83.03164341630347\n",
      "Iteration 728, the loss is 280.4443268476764, parameters k is -27.826738411262422 and b is -83.03064341630346\n",
      "Iteration 729, the loss is 280.4038302182937, parameters k is -27.82045377687507 and b is -83.02964341630346\n",
      "Iteration 730, the loss is 280.363333588911, parameters k is -27.814169142487717 and b is -83.02864341630345\n",
      "Iteration 731, the loss is 280.3228369595282, parameters k is -27.807884508100365 and b is -83.02764341630345\n",
      "Iteration 732, the loss is 280.28234033014564, parameters k is -27.801599873713013 and b is -83.02664341630344\n",
      "Iteration 733, the loss is 280.2418437007632, parameters k is -27.79531523932566 and b is -83.02564341630344\n",
      "Iteration 734, the loss is 280.2013470713806, parameters k is -27.789030604938308 and b is -83.02464341630343\n",
      "Iteration 735, the loss is 280.1608504419978, parameters k is -27.782745970550955 and b is -83.02364341630343\n",
      "Iteration 736, the loss is 280.12035381261524, parameters k is -27.776461336163603 and b is -83.02264341630342\n",
      "Iteration 737, the loss is 280.07985718323226, parameters k is -27.77017670177625 and b is -83.02164341630342\n",
      "Iteration 738, the loss is 280.0393605538496, parameters k is -27.7638920673889 and b is -83.02064341630341\n",
      "Iteration 739, the loss is 279.99886392446695, parameters k is -27.757607433001546 and b is -83.01964341630341\n",
      "Iteration 740, the loss is 279.95836729508414, parameters k is -27.751322798614193 and b is -83.0186434163034\n",
      "Iteration 741, the loss is 279.9178706657017, parameters k is -27.74503816422684 and b is -83.0176434163034\n",
      "Iteration 742, the loss is 279.8773740363188, parameters k is -27.73875352983949 and b is -83.0166434163034\n",
      "Iteration 743, the loss is 279.8368774069357, parameters k is -27.732468895452136 and b is -83.01564341630339\n",
      "Iteration 744, the loss is 279.7963807775534, parameters k is -27.726184261064784 and b is -83.01464341630339\n",
      "Iteration 745, the loss is 279.75588414817076, parameters k is -27.71989962667743 and b is -83.01364341630338\n",
      "Iteration 746, the loss is 279.715387518788, parameters k is -27.71361499229008 and b is -83.01264341630338\n",
      "Iteration 747, the loss is 279.67489088940533, parameters k is -27.707330357902727 and b is -83.01164341630337\n",
      "Iteration 748, the loss is 279.63439426002276, parameters k is -27.701045723515374 and b is -83.01064341630337\n",
      "Iteration 749, the loss is 279.59389763063984, parameters k is -27.694761089128022 and b is -83.00964341630336\n",
      "Iteration 750, the loss is 279.5534010012572, parameters k is -27.68847645474067 and b is -83.00864341630336\n",
      "Iteration 751, the loss is 279.5129043718747, parameters k is -27.682191820353317 and b is -83.00764341630335\n",
      "Iteration 752, the loss is 279.47240774249184, parameters k is -27.675907185965965 and b is -83.00664341630335\n",
      "Iteration 753, the loss is 279.4319111131094, parameters k is -27.669622551578613 and b is -83.00564341630334\n",
      "Iteration 754, the loss is 279.3914144837266, parameters k is -27.66333791719126 and b is -83.00464341630334\n",
      "Iteration 755, the loss is 279.3509178543437, parameters k is -27.657053282803908 and b is -83.00364341630333\n",
      "Iteration 756, the loss is 279.3104212249612, parameters k is -27.650768648416555 and b is -83.00264341630333\n",
      "Iteration 757, the loss is 279.26992459557863, parameters k is -27.644484014029203 and b is -83.00164341630332\n",
      "Iteration 758, the loss is 279.2294279661958, parameters k is -27.63819937964185 and b is -83.00064341630332\n",
      "Iteration 759, the loss is 279.18893133681286, parameters k is -27.631914745254498 and b is -82.99964341630331\n",
      "Iteration 760, the loss is 279.14843470742994, parameters k is -27.625630110867146 and b is -82.99864341630331\n",
      "Iteration 761, the loss is 279.10793807804754, parameters k is -27.619345476479793 and b is -82.9976434163033\n",
      "Iteration 762, the loss is 279.06744144866525, parameters k is -27.61306084209244 and b is -82.9966434163033\n",
      "Iteration 763, the loss is 279.02694481928233, parameters k is -27.60677620770509 and b is -82.9956434163033\n",
      "Iteration 764, the loss is 278.9864481898995, parameters k is -27.600491573317736 and b is -82.99464341630329\n",
      "Iteration 765, the loss is 278.9459515605167, parameters k is -27.594206938930384 and b is -82.99364341630329\n",
      "Iteration 766, the loss is 278.9054549311343, parameters k is -27.58792230454303 and b is -82.99264341630328\n",
      "Iteration 767, the loss is 278.86495830175147, parameters k is -27.58163767015568 and b is -82.99164341630328\n",
      "Iteration 768, the loss is 278.824461672369, parameters k is -27.575353035768327 and b is -82.99064341630327\n",
      "Iteration 769, the loss is 278.78396504298615, parameters k is -27.569068401380974 and b is -82.98964341630327\n",
      "Iteration 770, the loss is 278.7434684136038, parameters k is -27.562783766993622 and b is -82.98864341630326\n",
      "Iteration 771, the loss is 278.7029717842204, parameters k is -27.55649913260627 and b is -82.98764341630326\n",
      "Iteration 772, the loss is 278.6624751548384, parameters k is -27.550214498218917 and b is -82.98664341630325\n",
      "Iteration 773, the loss is 278.62197852545535, parameters k is -27.543929863831565 and b is -82.98564341630325\n",
      "Iteration 774, the loss is 278.58148189607226, parameters k is -27.537645229444212 and b is -82.98464341630324\n",
      "Iteration 775, the loss is 278.5409852666902, parameters k is -27.53136059505686 and b is -82.98364341630324\n",
      "Iteration 776, the loss is 278.50048863730746, parameters k is -27.525075960669508 and b is -82.98264341630323\n",
      "Iteration 777, the loss is 278.45999200792465, parameters k is -27.518791326282155 and b is -82.98164341630323\n",
      "Iteration 778, the loss is 278.4194953785419, parameters k is -27.512506691894803 and b is -82.98064341630322\n",
      "Iteration 779, the loss is 278.378998749159, parameters k is -27.50622205750745 and b is -82.97964341630322\n",
      "Iteration 780, the loss is 278.33850211977676, parameters k is -27.499937423120098 and b is -82.97864341630321\n",
      "Iteration 781, the loss is 278.29800549039413, parameters k is -27.493652788732746 and b is -82.97764341630321\n",
      "Iteration 782, the loss is 278.2575088610114, parameters k is -27.487368154345393 and b is -82.9766434163032\n",
      "Iteration 783, the loss is 278.21701223162876, parameters k is -27.48108351995804 and b is -82.9756434163032\n",
      "Iteration 784, the loss is 278.176515602246, parameters k is -27.47479888557069 and b is -82.9746434163032\n",
      "Iteration 785, the loss is 278.136018972863, parameters k is -27.468514251183336 and b is -82.97364341630319\n",
      "Iteration 786, the loss is 278.0955223434805, parameters k is -27.462229616795984 and b is -82.97264341630319\n",
      "Iteration 787, the loss is 278.05502571409744, parameters k is -27.45594498240863 and b is -82.97164341630318\n",
      "Iteration 788, the loss is 278.01452908471475, parameters k is -27.44966034802128 and b is -82.97064341630318\n",
      "Iteration 789, the loss is 277.9740324553324, parameters k is -27.443375713633927 and b is -82.96964341630317\n",
      "Iteration 790, the loss is 277.93353582594966, parameters k is -27.437091079246574 and b is -82.96864341630317\n",
      "Iteration 791, the loss is 277.8930391965672, parameters k is -27.430806444859222 and b is -82.96764341630316\n",
      "Iteration 792, the loss is 277.8525425671841, parameters k is -27.42452181047187 and b is -82.96664341630316\n",
      "Iteration 793, the loss is 277.81204593780126, parameters k is -27.418237176084517 and b is -82.96564341630315\n",
      "Iteration 794, the loss is 277.77154930841886, parameters k is -27.411952541697165 and b is -82.96464341630315\n",
      "Iteration 795, the loss is 277.7310526790359, parameters k is -27.405667907309812 and b is -82.96364341630314\n",
      "Iteration 796, the loss is 277.69055604965354, parameters k is -27.39938327292246 and b is -82.96264341630314\n",
      "Iteration 797, the loss is 277.65005942027096, parameters k is -27.393098638535108 and b is -82.96164341630313\n",
      "Iteration 798, the loss is 277.6095627908877, parameters k is -27.386814004147755 and b is -82.96064341630313\n",
      "Iteration 799, the loss is 277.56906616150553, parameters k is -27.380529369760403 and b is -82.95964341630312\n",
      "Iteration 800, the loss is 277.52856953212284, parameters k is -27.37424473537305 and b is -82.95864341630312\n",
      "Iteration 801, the loss is 277.48807290273993, parameters k is -27.367960100985698 and b is -82.95764341630311\n",
      "Iteration 802, the loss is 277.4475762733574, parameters k is -27.361675466598346 and b is -82.95664341630311\n",
      "Iteration 803, the loss is 277.40707964397467, parameters k is -27.355390832210993 and b is -82.9556434163031\n",
      "Iteration 804, the loss is 277.3665830145919, parameters k is -27.34910619782364 and b is -82.9546434163031\n",
      "Iteration 805, the loss is 277.32608638520924, parameters k is -27.34282156343629 and b is -82.9536434163031\n",
      "Iteration 806, the loss is 277.28558975582655, parameters k is -27.336536929048936 and b is -82.95264341630309\n",
      "Iteration 807, the loss is 277.2450931264438, parameters k is -27.330252294661584 and b is -82.95164341630309\n",
      "Iteration 808, the loss is 277.204596497061, parameters k is -27.32396766027423 and b is -82.95064341630308\n",
      "Iteration 809, the loss is 277.1640998676783, parameters k is -27.31768302588688 and b is -82.94964341630308\n",
      "Iteration 810, the loss is 277.123603238296, parameters k is -27.311398391499527 and b is -82.94864341630307\n",
      "Iteration 811, the loss is 277.08310660891306, parameters k is -27.305113757112174 and b is -82.94764341630307\n",
      "Iteration 812, the loss is 277.0426099795301, parameters k is -27.298829122724822 and b is -82.94664341630306\n",
      "Iteration 813, the loss is 277.00211335014774, parameters k is -27.29254448833747 and b is -82.94564341630306\n",
      "Iteration 814, the loss is 276.9616167207649, parameters k is -27.286259853950117 and b is -82.94464341630305\n",
      "Iteration 815, the loss is 276.92112009138225, parameters k is -27.279975219562765 and b is -82.94364341630305\n",
      "Iteration 816, the loss is 276.88062346199956, parameters k is -27.273690585175412 and b is -82.94264341630304\n",
      "Iteration 817, the loss is 276.8401268326171, parameters k is -27.26740595078806 and b is -82.94164341630304\n",
      "Iteration 818, the loss is 276.7996302032341, parameters k is -27.261121316400708 and b is -82.94064341630303\n",
      "Iteration 819, the loss is 276.75913357385167, parameters k is -27.254836682013355 and b is -82.93964341630303\n",
      "Iteration 820, the loss is 276.7186369444689, parameters k is -27.248552047626003 and b is -82.93864341630302\n",
      "Iteration 821, the loss is 276.678140315086, parameters k is -27.24226741323865 and b is -82.93764341630302\n",
      "Iteration 822, the loss is 276.6376436857038, parameters k is -27.235982778851298 and b is -82.93664341630301\n",
      "Iteration 823, the loss is 276.59714705632075, parameters k is -27.229698144463946 and b is -82.93564341630301\n",
      "Iteration 824, the loss is 276.5566504269379, parameters k is -27.223413510076593 and b is -82.934643416303\n",
      "Iteration 825, the loss is 276.5161537975553, parameters k is -27.21712887568924 and b is -82.933643416303\n",
      "Iteration 826, the loss is 276.47565716817235, parameters k is -27.21084424130189 and b is -82.932643416303\n",
      "Iteration 827, the loss is 276.4351605387897, parameters k is -27.204559606914536 and b is -82.93164341630299\n",
      "Iteration 828, the loss is 276.394663909407, parameters k is -27.198274972527184 and b is -82.93064341630298\n",
      "Iteration 829, the loss is 276.3541672800247, parameters k is -27.19199033813983 and b is -82.92964341630298\n",
      "Iteration 830, the loss is 276.313670650642, parameters k is -27.18570570375248 and b is -82.92864341630298\n",
      "Iteration 831, the loss is 276.2731740212589, parameters k is -27.179421069365127 and b is -82.92764341630297\n",
      "Iteration 832, the loss is 276.2326773918765, parameters k is -27.173136434977774 and b is -82.92664341630297\n",
      "Iteration 833, the loss is 276.19218076249405, parameters k is -27.166851800590422 and b is -82.92564341630296\n",
      "Iteration 834, the loss is 276.1516841331109, parameters k is -27.16056716620307 and b is -82.92464341630296\n",
      "Iteration 835, the loss is 276.11118750372816, parameters k is -27.154282531815717 and b is -82.92364341630295\n",
      "Iteration 836, the loss is 276.0706908743456, parameters k is -27.147997897428365 and b is -82.92264341630295\n",
      "Iteration 837, the loss is 276.03019424496307, parameters k is -27.141713263041012 and b is -82.92164341630294\n",
      "Iteration 838, the loss is 275.9896976155805, parameters k is -27.13542862865366 and b is -82.92064341630294\n",
      "Iteration 839, the loss is 275.9492009861973, parameters k is -27.129143994266308 and b is -82.91964341630293\n",
      "Iteration 840, the loss is 275.9087043568147, parameters k is -27.122859359878955 and b is -82.91864341630293\n",
      "Iteration 841, the loss is 275.8682077274325, parameters k is -27.116574725491603 and b is -82.91764341630292\n",
      "Iteration 842, the loss is 275.8277110980495, parameters k is -27.11029009110425 and b is -82.91664341630292\n",
      "Iteration 843, the loss is 275.78721446866655, parameters k is -27.104005456716898 and b is -82.91564341630291\n",
      "Iteration 844, the loss is 275.74671783928414, parameters k is -27.097720822329546 and b is -82.91464341630291\n",
      "Iteration 845, the loss is 275.7062212099017, parameters k is -27.091436187942193 and b is -82.9136434163029\n",
      "Iteration 846, the loss is 275.66572458051866, parameters k is -27.08515155355484 and b is -82.9126434163029\n",
      "Iteration 847, the loss is 275.62522795113637, parameters k is -27.07886691916749 and b is -82.9116434163029\n",
      "Iteration 848, the loss is 275.5847313217532, parameters k is -27.072582284780136 and b is -82.91064341630289\n",
      "Iteration 849, the loss is 275.54423469237065, parameters k is -27.066297650392784 and b is -82.90964341630288\n",
      "Iteration 850, the loss is 275.50373806298796, parameters k is -27.06001301600543 and b is -82.90864341630288\n",
      "Iteration 851, the loss is 275.4632414336051, parameters k is -27.05372838161808 and b is -82.90764341630288\n",
      "Iteration 852, the loss is 275.4227448042227, parameters k is -27.047443747230727 and b is -82.90664341630287\n",
      "Iteration 853, the loss is 275.3822481748398, parameters k is -27.041159112843374 and b is -82.90564341630287\n",
      "Iteration 854, the loss is 275.3417515454573, parameters k is -27.03487447845602 and b is -82.90464341630286\n",
      "Iteration 855, the loss is 275.30125491607475, parameters k is -27.02858984406867 and b is -82.90364341630286\n",
      "Iteration 856, the loss is 275.2607582866917, parameters k is -27.022305209681317 and b is -82.90264341630285\n",
      "Iteration 857, the loss is 275.22026165730915, parameters k is -27.016020575293965 and b is -82.90164341630285\n",
      "Iteration 858, the loss is 275.1797650279265, parameters k is -27.009735940906612 and b is -82.90064341630284\n",
      "Iteration 859, the loss is 275.1392683985437, parameters k is -27.00345130651926 and b is -82.89964341630284\n",
      "Iteration 860, the loss is 275.09877176916115, parameters k is -26.997166672131907 and b is -82.89864341630283\n",
      "Iteration 861, the loss is 275.05827513977846, parameters k is -26.990882037744555 and b is -82.89764341630283\n",
      "Iteration 862, the loss is 275.01777851039594, parameters k is -26.984597403357203 and b is -82.89664341630282\n",
      "Iteration 863, the loss is 274.977281881013, parameters k is -26.97831276896985 and b is -82.89564341630282\n",
      "Iteration 864, the loss is 274.9367852516302, parameters k is -26.972028134582498 and b is -82.89464341630281\n",
      "Iteration 865, the loss is 274.89628862224765, parameters k is -26.965743500195146 and b is -82.89364341630281\n",
      "Iteration 866, the loss is 274.8557919928648, parameters k is -26.959458865807793 and b is -82.8926434163028\n",
      "Iteration 867, the loss is 274.815295363482, parameters k is -26.95317423142044 and b is -82.8916434163028\n",
      "Iteration 868, the loss is 274.7747987340995, parameters k is -26.94688959703309 and b is -82.8906434163028\n",
      "Iteration 869, the loss is 274.734302104717, parameters k is -26.940604962645736 and b is -82.88964341630279\n",
      "Iteration 870, the loss is 274.69380547533405, parameters k is -26.934320328258384 and b is -82.88864341630278\n",
      "Iteration 871, the loss is 274.65330884595164, parameters k is -26.92803569387103 and b is -82.88764341630278\n",
      "Iteration 872, the loss is 274.61281221656895, parameters k is -26.92175105948368 and b is -82.88664341630277\n",
      "Iteration 873, the loss is 274.572315587186, parameters k is -26.915466425096326 and b is -82.88564341630277\n",
      "Iteration 874, the loss is 274.5318189578033, parameters k is -26.909181790708974 and b is -82.88464341630277\n",
      "Iteration 875, the loss is 274.4913223284207, parameters k is -26.90289715632162 and b is -82.88364341630276\n",
      "Iteration 876, the loss is 274.450825699038, parameters k is -26.89661252193427 and b is -82.88264341630276\n",
      "Iteration 877, the loss is 274.4103290696557, parameters k is -26.890327887546917 and b is -82.88164341630275\n",
      "Iteration 878, the loss is 274.36983244027283, parameters k is -26.884043253159565 and b is -82.88064341630275\n",
      "Iteration 879, the loss is 274.3293358108901, parameters k is -26.877758618772212 and b is -82.87964341630274\n",
      "Iteration 880, the loss is 274.2888391815074, parameters k is -26.87147398438486 and b is -82.87864341630274\n",
      "Iteration 881, the loss is 274.2483425521244, parameters k is -26.865189349997507 and b is -82.87764341630273\n",
      "Iteration 882, the loss is 274.207845922742, parameters k is -26.858904715610155 and b is -82.87664341630273\n",
      "Iteration 883, the loss is 274.167349293359, parameters k is -26.852620081222803 and b is -82.87564341630272\n",
      "Iteration 884, the loss is 274.1268526639764, parameters k is -26.84633544683545 and b is -82.87464341630272\n",
      "Iteration 885, the loss is 274.08635603459385, parameters k is -26.840050812448098 and b is -82.87364341630271\n",
      "Iteration 886, the loss is 274.0458594052112, parameters k is -26.833766178060745 and b is -82.87264341630271\n",
      "Iteration 887, the loss is 274.0053627758283, parameters k is -26.827481543673393 and b is -82.8716434163027\n",
      "Iteration 888, the loss is 273.96486614644573, parameters k is -26.82119690928604 and b is -82.8706434163027\n",
      "Iteration 889, the loss is 273.92436951706287, parameters k is -26.81491227489869 and b is -82.8696434163027\n",
      "Iteration 890, the loss is 273.8838728876803, parameters k is -26.808627640511336 and b is -82.86864341630269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 891, the loss is 273.84337625829744, parameters k is -26.802343006123984 and b is -82.86764341630268\n",
      "Iteration 892, the loss is 273.80287962891487, parameters k is -26.79605837173663 and b is -82.86664341630268\n",
      "Iteration 893, the loss is 273.76238299953195, parameters k is -26.78977373734928 and b is -82.86564341630267\n",
      "Iteration 894, the loss is 273.7218863701497, parameters k is -26.783489102961926 and b is -82.86464341630267\n",
      "Iteration 895, the loss is 273.6813897407666, parameters k is -26.777204468574574 and b is -82.86364341630266\n",
      "Iteration 896, the loss is 273.6408931113842, parameters k is -26.77091983418722 and b is -82.86264341630266\n",
      "Iteration 897, the loss is 273.60039648200154, parameters k is -26.76463519979987 and b is -82.86164341630266\n",
      "Iteration 898, the loss is 273.5598998526189, parameters k is -26.758350565412517 and b is -82.86064341630265\n",
      "Iteration 899, the loss is 273.5194032232363, parameters k is -26.752065931025165 and b is -82.85964341630265\n",
      "Iteration 900, the loss is 273.4789065938534, parameters k is -26.745781296637812 and b is -82.85864341630264\n",
      "Iteration 901, the loss is 273.4384099644706, parameters k is -26.73949666225046 and b is -82.85764341630264\n",
      "Iteration 902, the loss is 273.39791333508816, parameters k is -26.733212027863107 and b is -82.85664341630263\n",
      "Iteration 903, the loss is 273.3574167057052, parameters k is -26.726927393475755 and b is -82.85564341630263\n",
      "Iteration 904, the loss is 273.3169200763222, parameters k is -26.720642759088403 and b is -82.85464341630262\n",
      "Iteration 905, the loss is 273.27642344693976, parameters k is -26.71435812470105 and b is -82.85364341630262\n",
      "Iteration 906, the loss is 273.23592681755713, parameters k is -26.708073490313698 and b is -82.85264341630261\n",
      "Iteration 907, the loss is 273.1954301881746, parameters k is -26.701788855926345 and b is -82.85164341630261\n",
      "Iteration 908, the loss is 273.1549335587918, parameters k is -26.695504221538993 and b is -82.8506434163026\n",
      "Iteration 909, the loss is 273.1144369294091, parameters k is -26.68921958715164 and b is -82.8496434163026\n",
      "Iteration 910, the loss is 273.0739403000264, parameters k is -26.68293495276429 and b is -82.8486434163026\n",
      "Iteration 911, the loss is 273.0334436706434, parameters k is -26.676650318376936 and b is -82.84764341630259\n",
      "Iteration 912, the loss is 272.99294704126106, parameters k is -26.670365683989584 and b is -82.84664341630258\n",
      "Iteration 913, the loss is 272.9524504118786, parameters k is -26.66408104960223 and b is -82.84564341630258\n",
      "Iteration 914, the loss is 272.91195378249574, parameters k is -26.65779641521488 and b is -82.84464341630257\n",
      "Iteration 915, the loss is 272.8714571531129, parameters k is -26.651511780827526 and b is -82.84364341630257\n",
      "Iteration 916, the loss is 272.83096052373037, parameters k is -26.645227146440174 and b is -82.84264341630256\n",
      "Iteration 917, the loss is 272.79046389434757, parameters k is -26.63894251205282 and b is -82.84164341630256\n",
      "Iteration 918, the loss is 272.74996726496505, parameters k is -26.63265787766547 and b is -82.84064341630256\n",
      "Iteration 919, the loss is 272.70947063558214, parameters k is -26.626373243278117 and b is -82.83964341630255\n",
      "Iteration 920, the loss is 272.6689740061992, parameters k is -26.620088608890764 and b is -82.83864341630255\n",
      "Iteration 921, the loss is 272.6284773768167, parameters k is -26.613803974503412 and b is -82.83764341630254\n",
      "Iteration 922, the loss is 272.5879807474341, parameters k is -26.60751934011606 and b is -82.83664341630254\n",
      "Iteration 923, the loss is 272.54748411805156, parameters k is -26.601234705728707 and b is -82.83564341630253\n",
      "Iteration 924, the loss is 272.5069874886687, parameters k is -26.594950071341355 and b is -82.83464341630253\n",
      "Iteration 925, the loss is 272.46649085928584, parameters k is -26.588665436954003 and b is -82.83364341630252\n",
      "Iteration 926, the loss is 272.4259942299032, parameters k is -26.58238080256665 and b is -82.83264341630252\n",
      "Iteration 927, the loss is 272.3854976005207, parameters k is -26.576096168179298 and b is -82.83164341630251\n",
      "Iteration 928, the loss is 272.3450009711379, parameters k is -26.569811533791945 and b is -82.83064341630251\n",
      "Iteration 929, the loss is 272.30450434175526, parameters k is -26.563526899404593 and b is -82.8296434163025\n",
      "Iteration 930, the loss is 272.26400771237246, parameters k is -26.55724226501724 and b is -82.8286434163025\n",
      "Iteration 931, the loss is 272.2235110829896, parameters k is -26.55095763062989 and b is -82.8276434163025\n",
      "Iteration 932, the loss is 272.1830144536073, parameters k is -26.544672996242536 and b is -82.82664341630249\n",
      "Iteration 933, the loss is 272.14251782422446, parameters k is -26.538388361855183 and b is -82.82564341630248\n",
      "Iteration 934, the loss is 272.1020211948415, parameters k is -26.53210372746783 and b is -82.82464341630248\n",
      "Iteration 935, the loss is 272.06152456545954, parameters k is -26.52581909308048 and b is -82.82364341630247\n",
      "Iteration 936, the loss is 272.0210279360764, parameters k is -26.519534458693126 and b is -82.82264341630247\n",
      "Iteration 937, the loss is 271.98053130669393, parameters k is -26.513249824305774 and b is -82.82164341630246\n",
      "Iteration 938, the loss is 271.94003467731113, parameters k is -26.50696518991842 and b is -82.82064341630246\n",
      "Iteration 939, the loss is 271.8995380479284, parameters k is -26.50068055553107 and b is -82.81964341630245\n",
      "Iteration 940, the loss is 271.8590414185456, parameters k is -26.494395921143717 and b is -82.81864341630245\n",
      "Iteration 941, the loss is 271.81854478916296, parameters k is -26.488111286756364 and b is -82.81764341630245\n",
      "Iteration 942, the loss is 271.7780481597803, parameters k is -26.481826652369012 and b is -82.81664341630244\n",
      "Iteration 943, the loss is 271.7375515303972, parameters k is -26.47554201798166 and b is -82.81564341630244\n",
      "Iteration 944, the loss is 271.697054901015, parameters k is -26.469257383594307 and b is -82.81464341630243\n",
      "Iteration 945, the loss is 271.6565582716322, parameters k is -26.462972749206955 and b is -82.81364341630243\n",
      "Iteration 946, the loss is 271.61606164224946, parameters k is -26.456688114819602 and b is -82.81264341630242\n",
      "Iteration 947, the loss is 271.57556501286666, parameters k is -26.45040348043225 and b is -82.81164341630242\n",
      "Iteration 948, the loss is 271.5350683834835, parameters k is -26.444118846044898 and b is -82.81064341630241\n",
      "Iteration 949, the loss is 271.4945717541016, parameters k is -26.437834211657545 and b is -82.8096434163024\n",
      "Iteration 950, the loss is 271.454075124719, parameters k is -26.431549577270193 and b is -82.8086434163024\n",
      "Iteration 951, the loss is 271.41357849533586, parameters k is -26.42526494288284 and b is -82.8076434163024\n",
      "Iteration 952, the loss is 271.37308186595334, parameters k is -26.418980308495488 and b is -82.80664341630239\n",
      "Iteration 953, the loss is 271.3325852365708, parameters k is -26.412695674108136 and b is -82.80564341630239\n",
      "Iteration 954, the loss is 271.292088607188, parameters k is -26.406411039720783 and b is -82.80464341630238\n",
      "Iteration 955, the loss is 271.2515919778055, parameters k is -26.40012640533343 and b is -82.80364341630238\n",
      "Iteration 956, the loss is 271.21109534842253, parameters k is -26.39384177094608 and b is -82.80264341630237\n",
      "Iteration 957, the loss is 271.17059871903984, parameters k is -26.387557136558726 and b is -82.80164341630237\n",
      "Iteration 958, the loss is 271.1301020896569, parameters k is -26.381272502171374 and b is -82.80064341630236\n",
      "Iteration 959, the loss is 271.0896054602743, parameters k is -26.37498786778402 and b is -82.79964341630236\n",
      "Iteration 960, the loss is 271.0491088308919, parameters k is -26.36870323339667 and b is -82.79864341630235\n",
      "Iteration 961, the loss is 271.0086122015092, parameters k is -26.362418599009317 and b is -82.79764341630235\n",
      "Iteration 962, the loss is 270.9681155721263, parameters k is -26.356133964621964 and b is -82.79664341630235\n",
      "Iteration 963, the loss is 270.92761894274383, parameters k is -26.349849330234612 and b is -82.79564341630234\n",
      "Iteration 964, the loss is 270.8871223133609, parameters k is -26.34356469584726 and b is -82.79464341630234\n",
      "Iteration 965, the loss is 270.8466256839781, parameters k is -26.337280061459907 and b is -82.79364341630233\n",
      "Iteration 966, the loss is 270.8061290545954, parameters k is -26.330995427072555 and b is -82.79264341630233\n",
      "Iteration 967, the loss is 270.76563242521274, parameters k is -26.324710792685202 and b is -82.79164341630232\n",
      "Iteration 968, the loss is 270.7251357958304, parameters k is -26.31842615829785 and b is -82.79064341630232\n",
      "Iteration 969, the loss is 270.68463916644765, parameters k is -26.312141523910498 and b is -82.78964341630231\n",
      "Iteration 970, the loss is 270.64414253706485, parameters k is -26.305856889523145 and b is -82.7886434163023\n",
      "Iteration 971, the loss is 270.6036459076823, parameters k is -26.299572255135793 and b is -82.7876434163023\n",
      "Iteration 972, the loss is 270.56314927829936, parameters k is -26.29328762074844 and b is -82.7866434163023\n",
      "Iteration 973, the loss is 270.522652648917, parameters k is -26.287002986361088 and b is -82.78564341630229\n",
      "Iteration 974, the loss is 270.48215601953433, parameters k is -26.280718351973736 and b is -82.78464341630229\n",
      "Iteration 975, the loss is 270.4416593901513, parameters k is -26.274433717586383 and b is -82.78364341630228\n",
      "Iteration 976, the loss is 270.40116276076867, parameters k is -26.26814908319903 and b is -82.78264341630228\n",
      "Iteration 977, the loss is 270.3606661313858, parameters k is -26.26186444881168 and b is -82.78164341630227\n",
      "Iteration 978, the loss is 270.3201695020033, parameters k is -26.255579814424326 and b is -82.78064341630227\n",
      "Iteration 979, the loss is 270.27967287262055, parameters k is -26.249295180036974 and b is -82.77964341630226\n",
      "Iteration 980, the loss is 270.2391762432379, parameters k is -26.24301054564962 and b is -82.77864341630226\n",
      "Iteration 981, the loss is 270.19867961385546, parameters k is -26.23672591126227 and b is -82.77764341630225\n",
      "Iteration 982, the loss is 270.15818298447243, parameters k is -26.230441276874917 and b is -82.77664341630225\n",
      "Iteration 983, the loss is 270.11768635509, parameters k is -26.224156642487564 and b is -82.77564341630224\n",
      "Iteration 984, the loss is 270.0771897257072, parameters k is -26.217872008100212 and b is -82.77464341630224\n",
      "Iteration 985, the loss is 270.03669309632437, parameters k is -26.21158737371286 and b is -82.77364341630224\n",
      "Iteration 986, the loss is 269.996196466942, parameters k is -26.205302739325507 and b is -82.77264341630223\n",
      "Iteration 987, the loss is 269.955699837559, parameters k is -26.199018104938155 and b is -82.77164341630223\n",
      "Iteration 988, the loss is 269.9152032081764, parameters k is -26.192733470550802 and b is -82.77064341630222\n",
      "Iteration 989, the loss is 269.8747065787937, parameters k is -26.18644883616345 and b is -82.76964341630222\n",
      "Iteration 990, the loss is 269.83420994941093, parameters k is -26.180164201776098 and b is -82.76864341630221\n",
      "Iteration 991, the loss is 269.79371332002825, parameters k is -26.173879567388745 and b is -82.7676434163022\n",
      "Iteration 992, the loss is 269.7532166906454, parameters k is -26.167594933001393 and b is -82.7666434163022\n",
      "Iteration 993, the loss is 269.7127200612624, parameters k is -26.16131029861404 and b is -82.7656434163022\n",
      "Iteration 994, the loss is 269.67222343188007, parameters k is -26.155025664226688 and b is -82.76464341630219\n",
      "Iteration 995, the loss is 269.63172680249767, parameters k is -26.148741029839336 and b is -82.76364341630219\n",
      "Iteration 996, the loss is 269.5912301731146, parameters k is -26.142456395451983 and b is -82.76264341630218\n",
      "Iteration 997, the loss is 269.5507335437326, parameters k is -26.13617176106463 and b is -82.76164341630218\n",
      "Iteration 998, the loss is 269.5102369143494, parameters k is -26.12988712667728 and b is -82.76064341630217\n",
      "Iteration 999, the loss is 269.4697402849669, parameters k is -26.123602492289926 and b is -82.75964341630217\n"
     ]
    }
   ],
   "source": [
    "#random.seed(2020)\n",
    "k = random.random() * 200 - 100  # -100 100\n",
    "b = random.random() * 200 - 100  # -100 100\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "iteration_num = 1000 \n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    \n",
    "    current_loss = loss(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = partial_derivative_k(X_rm)\n",
    "    b_gradient = partial_derivative_b()\n",
    "    \n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x228117a60b8>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(iteration_num)),losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x22811753898>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "price_use_best_parameters = [price(r, best_k, best_b) for r in X_rm]\n",
    "\n",
    "plt.scatter(X_rm,y)\n",
    "plt.scatter(X_rm,price_use_current_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
